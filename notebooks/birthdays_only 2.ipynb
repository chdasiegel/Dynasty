{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726675f6",
   "metadata": {},
   "source": [
    "### Running `birthdays_only.py`\n",
    "\n",
    "This cell runs the `birthdays_only.py` script as a subprocess.  \n",
    "It takes the input CSV (`skill_draftees_2000_2025.csv`) and generates  \n",
    "an output CSV (`skill_draftee_birthdays.csv`).  \n",
    "\n",
    "The command is built automatically, executed, and the script’s  \n",
    "stdout and stderr are printed for easy debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47390ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your project root\n",
    "ROOT = Path(\"/Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty\")\n",
    "\n",
    "# Path to the script\n",
    "SCRIPT = ROOT / \"src\" / \"scrapers\" / \"birthdays_only.py\"\n",
    "\n",
    "# Input and output file paths\n",
    "INPUT_CSV = ROOT / \"data\" / \"scraper\" / \"skill_draftees_2000_2025.csv\"\n",
    "OUTPUT_CSV = ROOT / \"data\" / \"scraper\" / \"skill_draftee_birthdays.csv\"\n",
    "\n",
    "# Build the CLI command\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    str(SCRIPT),\n",
    "    \"--in\", str(INPUT_CSV),\n",
    "    \"--out\", str(OUTPUT_CSV)\n",
    "]\n",
    "\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "print(\"\\n--- STDOUT ---\")\n",
    "print(result.stdout)\n",
    "\n",
    "print(\"\\n--- STDERR ---\")\n",
    "print(result.stderr)\n",
    "\n",
    "print(\"\\nFinished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ede5db",
   "metadata": {},
   "source": [
    "### Extracting Players With Missing Birthdays\n",
    "\n",
    "This cell loads the processed birthday file  \n",
    "(`skill_draftee_birthdays.csv`) and filters out all players who still  \n",
    "do not have a `birth_date` value.\n",
    "\n",
    "It displays the total count of missing birthdays, shows a preview, and  \n",
    "saves the filtered list to:\n",
    "\n",
    "`data/scrapers/skill_draftees_missing_birthdays.csv`\n",
    "\n",
    "This file is then used for the Wikipedia fallback scraper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your project root\n",
    "ROOT = Path(\"/Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty\")\n",
    "\n",
    "# Load the birthday output file\n",
    "birthday_path = ROOT / \"data\" / \"processed\" / \"skill_draftee_birthdays.csv\"\n",
    "df = pd.read_csv(birthday_path)\n",
    "\n",
    "# Filter for missing birthdays\n",
    "missing = df[df[\"birth_date\"].isna()].copy()\n",
    "\n",
    "print(f\"Total players missing birthdays: {len(missing)}\")\n",
    "missing.head()\n",
    "\n",
    "missing_path = ROOT / \"data\" / \"scrapers\" / \"skill_draftees_missing_birthdays.csv\"\n",
    "missing.to_csv(missing_path, index=False)\n",
    "\n",
    "print(\"Saved missing list to:\", missing_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ce1fa",
   "metadata": {},
   "source": [
    "### Scraping Missing Player Birthdays From Wikipedia\n",
    "\n",
    "This cell loads the list of players who still have missing birthdays\n",
    "after the primary scraping step. For each player, it:\n",
    "\n",
    "1. Searches Wikipedia using the API\n",
    "2. Checks the top matching pages\n",
    "3. Looks for a `<span class=\"bday\">YYYY-MM-DD</span>` field\n",
    "4. Records the birthday and the source URL if found\n",
    "\n",
    "A small delay is added between requests to avoid overloading Wikipedia.\n",
    "\n",
    "Results are saved to:\n",
    "\n",
    "`data/processed/skill_draftees_missing_birthdays_wiki.csv`\n",
    "\n",
    "The output includes:\n",
    "- wiki_birth_date (if found)\n",
    "- wiki_source_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff67ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# ====== CONFIG ======\n",
    "ROOT = Path(\"/Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty\")\n",
    "\n",
    "missing_csv = ROOT / \"data\" / \"scraper\" / \"skill_draftees_missing_birthdays.csv\"\n",
    "out_csv = ROOT / \"data\" / \"scraper\" / \"skill_draftees_missing_birthdays_wiki.csv\"\n",
    "\n",
    "WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "REQUEST_HEADERS = {\n",
    "    \"User-Agent\": \"ChaseDynastyScraper/1.0 (contact: youremail@example.com)\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "# ====== FUNCTION TO FETCH ONE PLAYER'S BIRTHDAY FROM WIKIPEDIA ======\n",
    "def fetch_wikipedia_birthday(player_name: str, max_hits: int = 5):\n",
    "    \"\"\"\n",
    "    Search Wikipedia for the player name and try to extract a YYYY-MM-DD birthday.\n",
    "    We:\n",
    "      - Ask for up to `max_hits` search results\n",
    "      - Visit each candidate page in order\n",
    "      - Return the first one that has a <span class=\"bday\">YYYY-MM-DD</span>\n",
    "    Returns (birth_date_iso, page_url) or (None, None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: search for up to max_hits pages\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"format\": \"json\",\n",
    "            \"srsearch\": player_name,\n",
    "            \"srlimit\": max_hits,\n",
    "        }\n",
    "        r = requests.get(WIKI_API, params=params, headers=REQUEST_HEADERS, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        search_results = data.get(\"query\", {}).get(\"search\", [])\n",
    "        if not search_results:\n",
    "            return None, None\n",
    "\n",
    "        # Step 2: iterate each hit and look for a .bday span\n",
    "        for hit in search_results:\n",
    "            title = hit[\"title\"]\n",
    "            page_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "\n",
    "            page = requests.get(page_url, headers=REQUEST_HEADERS, timeout=15)\n",
    "            if page.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(page.text, \"lxml\")\n",
    "\n",
    "            bday_span = soup.find(\"span\", {\"class\": \"bday\"})\n",
    "            if bday_span:\n",
    "                return bday_span.text.strip(), page_url\n",
    "\n",
    "        # No candidate pages had a .bday span\n",
    "        first_title = search_results[0][\"title\"]\n",
    "        fallback_url = f\"https://en.wikipedia.org/wiki/{first_title.replace(' ', '_')}\"\n",
    "        return None, fallback_url\n",
    "\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "# ====== LOAD MISSING LIST ======\n",
    "df = pd.read_csv(missing_csv)\n",
    "print(f\"Players with missing birthdays: {len(df)}\")\n",
    "\n",
    "# Prepare new columns (overwrite if they already exist)\n",
    "df[\"wiki_birth_date\"] = None\n",
    "df[\"wiki_source_url\"] = None\n",
    "\n",
    "# ====== SCRAPE WIKIPEDIA FOR EACH PLAYER ======\n",
    "for idx, row in df.iterrows():\n",
    "    name = row[\"player_name\"]\n",
    "    print(f\"[{idx+1}/{len(df)}] {name}...\", end=\" \")\n",
    "\n",
    "    birth_date, src_url = fetch_wikipedia_birthday(name, max_hits=5)\n",
    "    df.at[idx, \"wiki_birth_date\"] = birth_date\n",
    "    df.at[idx, \"wiki_source_url\"] = src_url\n",
    "\n",
    "    if birth_date:\n",
    "        print(f\"FOUND: {birth_date}\")\n",
    "    else:\n",
    "        print(\"not found\")\n",
    "\n",
    "    # Gentle rate limiting so we don't hammer Wikipedia\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# ====== SAVE RESULT ======\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"Saved to:\", out_csv)\n",
    "print(\"Still missing:\", df['wiki_birth_date'].isna().sum())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad2c46c",
   "metadata": {},
   "source": [
    "### Merging Primary Birthdays With Wikipedia Fallback\n",
    "\n",
    "This cell combines the original birthday results  \n",
    "(`skill_draftee_birthdays.csv`) with the Wikipedia-enriched fallback  \n",
    "(`skill_draftees_missing_birthdays_wiki.csv`).\n",
    "\n",
    "Steps performed:\n",
    "1. Load both CSV files\n",
    "2. Normalize player names\n",
    "3. Merge them on `player_name`\n",
    "4. For each player:\n",
    "   - Keep the original `birth_date` if it exists\n",
    "   - Otherwise use the `wiki_birth_date`\n",
    "5. Do the same for the source URL\n",
    "6. Drop the temporary wiki columns\n",
    "7. Save the final unified dataset to:\n",
    "\n",
    "`data/processed/skill_draftee_birthdays_master.csv`\n",
    "\n",
    "The output includes exactly one birthday and one source URL per player.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eefa77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged file to: /Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty/data/processed/skill_draftee_birthdays_master.csv\n",
      "Total rows: 2122\n",
      "Missing final birthdays: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_name</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>birthdate_source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A.J. Brown</td>\n",
       "      <td>1997-06-30</td>\n",
       "      <td>https://www.pro-football-reference.com/players...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A.J. Derby</td>\n",
       "      <td>1991-09-20</td>\n",
       "      <td>https://www.pro-football-reference.com/players...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A.J. Feeley</td>\n",
       "      <td>1977-05-16</td>\n",
       "      <td>https://www.pro-football-reference.com/players...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A.J. Green</td>\n",
       "      <td>1988-07-31</td>\n",
       "      <td>https://www.pro-football-reference.com/players...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A.J. Jenkins</td>\n",
       "      <td>1989-09-30</td>\n",
       "      <td>https://www.pro-football-reference.com/players...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    player_name  birth_date                               birthdate_source_url\n",
       "0    A.J. Brown  1997-06-30  https://www.pro-football-reference.com/players...\n",
       "1    A.J. Derby  1991-09-20  https://www.pro-football-reference.com/players...\n",
       "2   A.J. Feeley  1977-05-16  https://www.pro-football-reference.com/players...\n",
       "3    A.J. Green  1988-07-31  https://www.pro-football-reference.com/players...\n",
       "4  A.J. Jenkins  1989-09-30  https://www.pro-football-reference.com/players..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== CONFIG ======\n",
    "ROOT = Path(\"/Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty\")\n",
    "\n",
    "# Original birthdays (from SR / your birthdays_only.py)\n",
    "csv1 = ROOT / \"data\" / \"scraper\" / \"skill_draftee_birthdays.csv\"\n",
    "\n",
    "# Wikipedia-enriched missing list\n",
    "csv2 = ROOT / \"data\" / \"scraper\" / \"skill_draftees_missing_birthdays_wiki.csv\"\n",
    "\n",
    "# Output master file\n",
    "out_csv = ROOT / \"data\" / \"processed\" / \"skill_draftee_birthdays_master.csv\"\n",
    "\n",
    "# ====== LOAD FILES ======\n",
    "df1 = pd.read_csv(csv1)\n",
    "df2 = pd.read_csv(csv2)\n",
    "\n",
    "# ====== NORMALIZE PLAYER NAME ======\n",
    "def clean_name(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "df1[\"player_name\"] = df1[\"player_name\"].apply(clean_name)\n",
    "df2[\"player_name\"] = df2[\"player_name\"].apply(clean_name)\n",
    "\n",
    "# ====== MERGE ON player_name (outer to keep everything) ======\n",
    "merged = df1.merge(\n",
    "    df2[[\"player_name\", \"wiki_birth_date\", \"wiki_source_url\"]],\n",
    "    on=\"player_name\",\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "# ====== COLLAPSE INTO SINGLE COLUMNS ======\n",
    "# Use existing birth_date if present; otherwise fall back to wiki_birth_date\n",
    "if \"birth_date\" in merged.columns and \"wiki_birth_date\" in merged.columns:\n",
    "    merged[\"birth_date\"] = merged[\"birth_date\"].combine_first(merged[\"wiki_birth_date\"])\n",
    "else:\n",
    "    raise ValueError(\"Expected columns 'birth_date' and 'wiki_birth_date' not found.\")\n",
    "\n",
    "# Same for source URL: prefer existing, else Wikipedia\n",
    "if \"birthdate_source_url\" in merged.columns and \"wiki_source_url\" in merged.columns:\n",
    "    merged[\"birthdate_source_url\"] = merged[\"birthdate_source_url\"].combine_first(\n",
    "        merged[\"wiki_source_url\"]\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Expected columns 'birthdate_source_url' and 'wiki_source_url' not found.\")\n",
    "\n",
    "# (Optional) drop the wiki-specific helper columns now that we've merged them\n",
    "merged = merged.drop(columns=[\"wiki_birth_date\", \"wiki_source_url\"])\n",
    "\n",
    "# ====== SAVE RESULT ======\n",
    "merged.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Saved merged file to:\", out_csv)\n",
    "print(\"Total rows:\", len(merged))\n",
    "print(\"Missing final birthdays:\", merged[\"birth_date\"].isna().sum())\n",
    "merged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2565a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f340f68b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab973446",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff143f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== CONFIG ======\n",
    "ROOT = Path(\"/Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty\")\n",
    "\n",
    "birth_csv = ROOT / \"data\" / \"processed\" / \"skill_draftee_birthdays_master.csv\"\n",
    "dom_csv   = ROOT / \"data\" / \"processed\" / \"players_dom_numbered_with_years.csv\"\n",
    "\n",
    "out_csv   = ROOT / \"data\" / \"processed\" / \"players_dom_numbered_with_years_and_birthdays.csv\"\n",
    "\n",
    "# ====== LOAD FILES ======\n",
    "df_birth = pd.read_csv(birth_csv)\n",
    "df_dom   = pd.read_csv(dom_csv)\n",
    "\n",
    "# ====== NORMALIZE PLAYER NAME ======\n",
    "def clean_name(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "df_birth[\"player_name\"] = df_birth[\"player_name\"].apply(clean_name)\n",
    "df_dom[\"player_name\"]   = df_dom[\"player_name\"].apply(clean_name)\n",
    "\n",
    "# ====== MERGE ======\n",
    "# Use birthdays as the base; attach DOM data where available\n",
    "merged = df_birth.merge(\n",
    "    df_dom,\n",
    "    on=\"player_name\",\n",
    "    how=\"left\",   # keep all drafted players with birthdays, bring over DOM stats if they exist\n",
    ")\n",
    "\n",
    "# ====== REORDER COLUMNS ======\n",
    "front_cols = [\"player_name\", \"birth_date\", \"birthdate_source_url\"]\n",
    "rest_cols = [c for c in merged.columns if c not in front_cols]\n",
    "\n",
    "merged = merged[front_cols + rest_cols]\n",
    "\n",
    "# ====== SAVE ======\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "merged.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"Saved merged file to:\", out_csv)\n",
    "print(\"Rows:\", len(merged))\n",
    "print(\"Columns:\", len(merged.columns))\n",
    "print(\"Missing birthdays in final (sanity check):\", merged[\"birth_date\"].isna().sum())\n",
    "merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# ====== CONFIG ======\n",
    "ROOT = Path(\"/Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty\")\n",
    "\n",
    "merged_path = ROOT / \"data\" / \"processed\" / \"players_dom_numbered_with_years_and_birthdays.csv\"\n",
    "dom_path    = ROOT / \"data\" / \"processed\" / \"players_dom_numbered_with_years.csv\"\n",
    "\n",
    "# ====== LOAD FILES ======\n",
    "merged = pd.read_csv(merged_path)\n",
    "dom    = pd.read_csv(dom_path)\n",
    "\n",
    "print(\"Merged rows:\", len(merged))\n",
    "print(\"DOM rows:\", len(dom))\n",
    "\n",
    "# ====== NORMALIZE NAMES ======\n",
    "def clean_name(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "merged[\"player_name_clean\"] = merged[\"player_name\"].apply(clean_name)\n",
    "dom[\"player_name_clean\"]    = dom[\"player_name\"].apply(clean_name)\n",
    "\n",
    "# ====== IDENTIFY \"EMPTY DOM\" RECORDS IN MERGED ======\n",
    "# Heuristic: if ALL DOM-related columns are NaN for a row, treat it as an empty DOM record.\n",
    "# Adjust this list if your column names differ.\n",
    "dom_like_cols = [c for c in merged.columns if any(\n",
    "    c.startswith(prefix) for prefix in (\"Year\", \"DOM\", \"DOM+\", \"PDOM\", \"PDOM+\", \"RDOM\", \"RDOM+\")\n",
    ")]\n",
    "\n",
    "print(\"Number of DOM-related columns considered:\", len(dom_like_cols))\n",
    "\n",
    "mask_empty_dom = merged[dom_like_cols].isna().all(axis=1)\n",
    "empty_dom = merged[mask_empty_dom].copy()\n",
    "\n",
    "print(\"Players with completely empty DOM records in merged:\", len(empty_dom))\n",
    "\n",
    "# ====== BUILD CANDIDATE NAME SET FROM ORIGINAL DOM TABLE ======\n",
    "dom_names = dom[\"player_name_clean\"].dropna().unique().tolist()\n",
    "\n",
    "def best_match(name, candidates):\n",
    "    \"\"\"\n",
    "    Return (best_candidate, score) where score in [0,1].\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    best_score = 0.0\n",
    "    for cand in candidates:\n",
    "        s = SequenceMatcher(None, name, cand).ratio()\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best = cand\n",
    "    return best, best_score\n",
    "\n",
    "# ====== FUZZY MATCH EMPTY-DOM PLAYERS AGAINST DOM TABLE ======\n",
    "rows = []\n",
    "THRESHOLD = 0.50  # adjust: 0.75–0.9 depending how strict you want to be\n",
    "\n",
    "for _, row in empty_dom.iterrows():\n",
    "    name = row[\"player_name_clean\"]\n",
    "    if not name:\n",
    "        continue\n",
    "    match, score = best_match(name, dom_names)\n",
    "    if score >= THRESHOLD:\n",
    "        rows.append({\n",
    "            \"merged_player_name\": row[\"player_name\"],\n",
    "            \"merged_player_name_clean\": name,\n",
    "            \"matched_dom_name\": match,\n",
    "            \"similarity\": score\n",
    "        })\n",
    "\n",
    "matches_df = pd.DataFrame(rows).sort_values(\"similarity\", ascending=False)\n",
    "\n",
    "print(\"\\nPotential near-matches for empty DOM players (similarity >= {:.2f}):\".format(THRESHOLD))\n",
    "print(\"Total candidates:\", len(matches_df))\n",
    "\n",
    "# Show top 30 for inspection\n",
    "matches_df.head(30)\n",
    "\n",
    "\n",
    "\n",
    "# ====== SAVE RESULTS TO CSV ======\n",
    "out_csv = ROOT / \"data\" / \"processed\" / \"potential_dom_name_matches.csv\"\n",
    "matches_df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"\\nCSV saved to:\", out_csv)\n",
    "matches_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43828cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== CONFIG ======\n",
    "ROOT = Path(\"/Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty\")\n",
    "\n",
    "in_csv  = ROOT / \"data\" / \"processed\" / \"players_dom_numbered_with_years_and_birthdays.csv\"\n",
    "out_csv = ROOT / \"data\" / \"processed\" / \"players_missing_dom_metrics.csv\"\n",
    "\n",
    "# ====== LOAD FILE ======\n",
    "df = pd.read_csv(in_csv)\n",
    "print(\"Loaded:\", len(df))\n",
    "\n",
    "# ====== IDENTIFY DOM/PDOM/RDOM COLUMNS ======\n",
    "dom_cols = [\n",
    "    c for c in df.columns\n",
    "    if any(\n",
    "        c.startswith(prefix)\n",
    "        for prefix in (\"DOM\", \"DOM+\", \"PDOM\", \"PDOM+\", \"RDOM\", \"RDOM+\")\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"DOM-related columns found:\", len(dom_cols))\n",
    "\n",
    "# ====== FIND ROWS WHERE *ALL* METRIC COLUMNS ARE EMPTY ======\n",
    "mask_missing = df[dom_cols].isna().all(axis=1)\n",
    "\n",
    "missing_df = df[mask_missing].copy()\n",
    "print(\"Players missing ALL DOM/PDOM/RDOM metrics:\", len(missing_df))\n",
    "\n",
    "# ====== SAVE TO CSV ======\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "missing_df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"Saved →\", out_csv)\n",
    "\n",
    "missing_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c327c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ====== PATH CONFIG ======\n",
    "ROOT = Path(\"/Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty\")\n",
    "\n",
    "# Stage 1: run birthdays_only.py\n",
    "SCRIPT = ROOT / \"src\" / \"scrapers\" / \"birthdays_only.py\"\n",
    "INPUT_CSV = ROOT / \"data\" / \"scraper\" / \"skill_draftees_2000_2025.csv\"\n",
    "OUTPUT_CSV = ROOT / \"data\" / \"scraper\" / \"skill_draftee_birthdays.csv\"\n",
    "\n",
    "print(\"=== Stage 1: Running birthdays_only.py ===\")\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    str(SCRIPT),\n",
    "    \"--in\", str(INPUT_CSV),\n",
    "    \"--out\", str(OUTPUT_CSV),\n",
    "]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "print(\"\\n--- STDOUT ---\")\n",
    "print(result.stdout)\n",
    "\n",
    "print(\"\\n--- STDERR ---\")\n",
    "print(result.stderr)\n",
    "\n",
    "print(\"\\nFinished Stage 1.\")\n",
    "\n",
    "# ====== Stage 2: Wikipedia fallback for missing birthdays ======\n",
    "\n",
    "print(\"\\n=== Stage 2: Wikipedia fallback for missing birthdays ===\")\n",
    "\n",
    "# Load the master birthdays file produced by birthdays_only.py\n",
    "df_master = pd.read_csv(OUTPUT_CSV)\n",
    "print(f\"Loaded master birthdays file with {len(df_master)} rows from {OUTPUT_CSV}\")\n",
    "\n",
    "# Try to locate the birthday column\n",
    "birthday_col = None\n",
    "for cand in [\"birth_date\", \"birthday\", \"birthdate\", \"BirthDate\", \"Birthday\"]:\n",
    "    if cand in df_master.columns:\n",
    "        birthday_col = cand\n",
    "        break\n",
    "\n",
    "if birthday_col is None:\n",
    "    raise ValueError(\n",
    "        \"Could not find a birthday column in OUTPUT_CSV. \"\n",
    "        \"Expected one of: birth_date, birthday, birthdate, BirthDate, Birthday.\"\n",
    "    )\n",
    "\n",
    "# Ensure we have a player name column\n",
    "name_col = None\n",
    "for cand in [\"player_name\", \"Player\", \"player\", \"name\", \"Name\"]:\n",
    "    if cand in df_master.columns:\n",
    "        name_col = cand\n",
    "        break\n",
    "\n",
    "if name_col is None:\n",
    "    raise ValueError(\"Could not find a player name column (expected something like 'player_name').\")\n",
    "\n",
    "# Identify rows with missing birthdays (NaN or empty string)\n",
    "mask_missing = df_master[birthday_col].isna() | (df_master[birthday_col].astype(str).str.strip() == \"\")\n",
    "df_missing = df_master.loc[mask_missing].copy()\n",
    "\n",
    "print(f\"Players with missing birthdays after Stage 1: {len(df_missing)}\")\n",
    "\n",
    "if df_missing.empty:\n",
    "    print(\"No missing birthdays to fill from Wikipedia. Skipping Stage 2.\")\n",
    "else:\n",
    "    # ====== WIKIPEDIA CONFIG ======\n",
    "    WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "    REQUEST_HEADERS = {\n",
    "        \"User-Agent\": \"ChaseDynastyScraper/1.0 (contact: youremail@example.com)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "\n",
    "    def fetch_wikipedia_birthday(player_name: str, max_hits: int = 5):\n",
    "        \"\"\"\n",
    "        Search Wikipedia for the player name and try to extract a YYYY-MM-DD birthday.\n",
    "        We:\n",
    "          - Ask for up to `max_hits` search results\n",
    "          - Visit each candidate page in order\n",
    "          - Return the first one that has a <span class=\"bday\">YYYY-MM-DD</span>\n",
    "        Returns (birth_date_iso, page_url) or (None, None).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: search for up to max_hits pages\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"list\": \"search\",\n",
    "                \"format\": \"json\",\n",
    "                \"srsearch\": player_name,\n",
    "                \"srlimit\": max_hits,\n",
    "            }\n",
    "            r = requests.get(WIKI_API, params=params, headers=REQUEST_HEADERS, timeout=15)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            search_results = data.get(\"query\", {}).get(\"search\", [])\n",
    "            if not search_results:\n",
    "                return None, None\n",
    "\n",
    "            # Step 2: iterate each hit and look for a .bday span\n",
    "            for hit in search_results:\n",
    "                title = hit[\"title\"]\n",
    "                page_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "\n",
    "                page = requests.get(page_url, headers=REQUEST_HEADERS, timeout=15)\n",
    "                if page.status_code != 200:\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(page.text, \"lxml\")\n",
    "\n",
    "                bday_span = soup.find(\"span\", {\"class\": \"bday\"})\n",
    "                if bday_span:\n",
    "                    return bday_span.text.strip(), page_url\n",
    "\n",
    "            # No candidate pages had a .bday span\n",
    "            first_title = search_results[0][\"title\"]\n",
    "            fallback_url = f\"https://en.wikipedia.org/wiki/{first_title.replace(' ', '_')}\"\n",
    "            return None, fallback_url\n",
    "\n",
    "        except Exception:\n",
    "            return None, None\n",
    "\n",
    "    # Prepare new columns on the missing subset\n",
    "    df_missing[\"wiki_birth_date\"] = None\n",
    "    df_missing[\"wiki_source_url\"] = None\n",
    "\n",
    "    print(\"\\nScraping Wikipedia for missing players...\\n\")\n",
    "\n",
    "    for idx, row in df_missing.iterrows():\n",
    "        name = row[name_col]\n",
    "        print(f\"[{idx+1}/{len(df_master)}] {name}...\", end=\" \")\n",
    "\n",
    "        birth_date, src_url = fetch_wikipedia_birthday(str(name), max_hits=5)\n",
    "        df_missing.at[idx, \"wiki_birth_date\"] = birth_date\n",
    "        df_missing.at[idx, \"wiki_source_url\"] = src_url\n",
    "\n",
    "        if birth_date:\n",
    "            print(f\"FOUND: {birth_date}\")\n",
    "        else:\n",
    "            print(\"not found\")\n",
    "\n",
    "        # Gentle rate limiting so we don't hammer Wikipedia\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Merge Wikipedia birthdays back into the master dataframe\n",
    "    # Only fill where birthday is missing and wiki_birth_date is not null\n",
    "    for idx, row in df_missing.iterrows():\n",
    "        wiki_bd = row[\"wiki_birth_date\"]\n",
    "        if pd.notna(wiki_bd) and str(wiki_bd).strip() != \"\":\n",
    "            df_master.at[idx, birthday_col] = wiki_bd\n",
    "            # Optionally also store a source URL\n",
    "            source_col = \"birthday_source_url\"\n",
    "            if source_col not in df_master.columns:\n",
    "                df_master[source_col] = None\n",
    "            df_master.at[idx, source_col] = row[\"wiki_source_url\"]\n",
    "\n",
    "    # Save debug CSV of missing list + wiki results\n",
    "    wiki_debug_csv = ROOT / \"data\" / \"processed\" / \"skill_draftees_missing_birthdays_wiki.csv\"\n",
    "    wiki_debug_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_missing.to_csv(wiki_debug_csv, index=False)\n",
    "    print(\"\\nSaved Wikipedia debug results to:\", wiki_debug_csv)\n",
    "\n",
    "    # Show how many are still missing after Wikipedia\n",
    "    mask_still_missing = df_master[birthday_col].isna() | (df_master[birthday_col].astype(str).str.strip() == \"\")\n",
    "    print(\"Still missing birthdays after Wikipedia fallback:\", mask_still_missing.sum())\n",
    "\n",
    "# Save final enriched birthdays CSV (overwriting OUTPUT_CSV)\n",
    "df_master.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"\\n=== All done ===\")\n",
    "print(\"Final combined birthdays (with Wikipedia fallback) saved to:\", OUTPUT_CSV)\n",
    "\n",
    "df_master.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
