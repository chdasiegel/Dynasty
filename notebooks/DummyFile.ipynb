{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aabdf1-326b-4538-bf8d-d823a9405890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/chase/Desktop/Comp_Sci/Capstone/Dynasty/data/CFB_Data\")\n",
    "\n",
    "from src.process.process_college import build_player_dict\n",
    "\n",
    "player_dict = build_player_dict(verbose=False)\n",
    "print(len(player_dict))\n",
    "print(player_dict.get(\"Rashee Rice\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e9e61-f8c9-4e23-a507-be3786662be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/chase/Desktop/Comp_Sci/Capstone/Dynasty\")\n",
    "\n",
    "from src.process.process_combine import build_combine_dict\n",
    "\n",
    "player_combine = build_combine_dict(verbose=False)\n",
    "print(len(player_combine))\n",
    "print(player_combine.get(\"Amon-Ra St Brown\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eac278-a380-4867-8a89-8410ff50ccc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.process.process_pro_qb import run_pro_qb_player\n",
    "\n",
    "qb_dict = run_pro_qb_player(years=range(2016, 2025), s_type=\"REG\", verbose=False)\n",
    "print(len(qb_dict))\n",
    "print(qb_dict.get(\"Patrick Mahomes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b566f-c224-4708-bc51-12aa66289e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.process.process_pro_wr import run_pro_wr_player\n",
    "\n",
    "wr_dict = run_pro_wr_player(years=range(2016, 2025), s_type=\"REG\", verbose=False)\n",
    "print(len(wr_dict))\n",
    "print(wr_dict.get(\"Rashee Rice\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b105db8-4934-4516-853b-89d4c51c9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.process.process_pro_rb import run_pro_rb_player\n",
    "\n",
    "rb_dict = run_pro_rb_player(years=range(2016, 2025), s_type=\"REG\", verbose=False)\n",
    "print(len(rb_dict))\n",
    "print(rb_dict.get(\"Christian McCaffrey\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd833bc-28cd-4c7b-aa1c-ef181c054503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.process.process_pro_te import run_pro_te_player\n",
    "\n",
    "te_dict = run_pro_te_player(years=range(2016, 2025), s_type=\"REG\", verbose=False)\n",
    "print(len(te_dict))\n",
    "print(te_dict.get(\"Travis Kelce\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16f46e-8b40-40fe-85e0-c18c2a2415d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scrapers.fantasycalc_client import (\n",
    "        get_player_value, search_players,\n",
    "        get_rankings_df, save_current_rankings\n",
    "    )\n",
    "\n",
    "# Look up one player\n",
    "row = get_player_value(\"Breece Hall\")\n",
    "print(row)\n",
    "\n",
    "# Search for possible name matches\n",
    "print(search_players(\"Harrison\"))\n",
    "\n",
    "# Get full rankings as a DataFrame\n",
    "df = get_rankings_df(dynasty=True, num_qbs=2, teams=12, ppr=1.0)\n",
    "print(df.head())\n",
    "\n",
    "# Save CSV snapshot(s) to Market_Value/\n",
    "path = save_current_rankings(dynasty=True, num_qbs=2, teams=12, ppr=1.0)\n",
    "print(\"Saved:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f8c77-0650-49e6-95aa-78b79e7e458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visuals.plot_feature_scatter_batch import main\n",
    "\n",
    "# RB plots using RB CSV under data/Bakery/RB and saving under tests/feature_scatter/RB\n",
    "main(position=\"RB\", max_plots=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b66bc-f4b8-4615-9aee-c0e1bf6f058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from src.utils import clean_player_name\n",
    "\n",
    "print(clean_player_name(\"Amon-Ra St. Brown\"))   # Amon-Ra StBrown\n",
    "print(clean_player_name(\"amon-ra st brown\"))    # Amon-Ra StBrown\n",
    "print(clean_player_name(None))                  # \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c2913-3ce6-433e-b58f-104921f301c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_player_name(player_name):\n",
    "    \"\"\"Clean names while preserving apostrophes/hyphens, fusing 'St. X' -> 'StX',\n",
    "    and normalizing casing to Title Case.\"\"\"\n",
    "    if not isinstance(player_name, str):\n",
    "        return player_name\n",
    "\n",
    "    s = player_name.strip()\n",
    "\n",
    "    # 1) Remove common suffixes (Jr, Sr, II, III, IV, V), case-insensitive\n",
    "    suffixes = ['Jr', 'Sr', 'II', 'III', 'IV', 'V']\n",
    "    s = re.sub(r'\\b(?:' + '|'.join(suffixes) + r')\\b\\.?', '', s, flags=re.IGNORECASE)\n",
    "\n",
    "    # 2) Keep only word chars, whitespace, apostrophes, and hyphens\n",
    "    s = re.sub(r\"[^\\w\\s'-]\", '', s)\n",
    "\n",
    "    # 3) Fuse 'St. ' or 'St ' (any case) before a capitalized surname -> 'StSurname'\n",
    "    s = re.sub(r\"\\bSt[.\\s]+(?=[A-Z])\", \"St\", s, flags=re.IGNORECASE)\n",
    "\n",
    "    # 4) Collapse extra spaces\n",
    "    s = ' '.join(s.split())\n",
    "\n",
    "    # 5) Normalize to Title Case (preserves apostrophes/hyphens properly)\n",
    "    s = s.title()\n",
    "\n",
    "    # Fix common cases where title-casing breaks (e.g., \"O'Neal\" -> \"O'Neal\", not \"O'Neal\")\n",
    "    # The default .title() already does this okay, but just in case:\n",
    "    s = re.sub(r\"\\bO'([A-Z])\", lambda m: \"O'\" + m.group(1).upper(), s)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def strip_name_marks(s: object) -> object:\n",
    "    \"\"\"Strip common extraneous marks like '*' without touching apostrophes or hyphens.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return s.replace(\"*\", \"\")\n",
    "\n",
    "\n",
    "# ---- quick checks ----\n",
    "tests = [\n",
    "    \"Amon-Ra St. Brown\",\n",
    "    \"amon-ra st. brown\",\n",
    "    \"O'Neal Jr.\",\n",
    "    \"jean-baptiste iii\",\n",
    "    \"ST. JOHN\",\n",
    "]\n",
    "for t in tests:\n",
    "    print(t, \"->\", clean_player_name(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360cb063-842c-415f-9bf1-4951a8716d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RB] Wrote multi-page PDF → /Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty/tests/feature_scatter/RB/RB_feature_scatters.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/chasesiegel/Desktop/Comp_Sci/Capstone/Dynasty/tests/feature_scatter/RB/RB_feature_scatters.pdf')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.visuals.plot_feature_scatter_batch import main\n",
    "\n",
    "# Generate up to 20 plots, 6 per PDF page, for RB\n",
    "main(position=\"RB\", max_plots=900, cols=3, rows=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec903649-c891-4226-9ea6-9108bf7a3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# adjust path (this assumes you’re in Dynasty/notebooks/)\n",
    "df = pd.read_csv(\"./data/Bakery/RB/Bakery_RB_2017.csv\")\n",
    "\n",
    "# Clean up column names\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e169a4b7-20a2-47be-a7ee-c8988a46e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"DOM++\", \"40 Time\", \"BMI\", \"YPC\",\n",
    "    \"ELU\", \"YCO/A\", \"Break%\", \"Draft Cap\", \"BAMA\"\n",
    "]\n",
    "\n",
    "target = \"RB Grade\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea14265-2f6c-41c4-9d9b-a7a349813c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[target]\n",
    "\n",
    "# invert \"lower is better\"\n",
    "X[\"40 Time\"]      = -X[\"40 Time\"]\n",
    "X[\"Draft Cap\"] = -X[\"Draft Cap\"]\n",
    "\n",
    "# drop rows with missing values\n",
    "mask = X.notna().all(axis=1) & y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b35907-65ad-493d-8d0f-bb0046c79595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "weights = pd.Series(model.coef_, index=features).sort_values(ascending=False)\n",
    "print(\"Approximate Weights for RB Grade:\")\n",
    "print(weights)\n",
    "\n",
    "print(\"\\nIntercept:\", model.intercept_)\n",
    "print(\"R² (fit quality):\", round(model.score(X_scaled, y), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f149b-63e6-4a3e-a632-d082b4530b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_pos = Lasso(alpha=0.01, positive=True, max_iter=10000)\n",
    "lasso_pos.fit(X_scaled, y)\n",
    "\n",
    "weights_lasso = pd.Series(lasso_pos.coef_, index=features).sort_values(ascending=False)\n",
    "print(\"Lasso (positive, shrunk weights):\\n\", weights_lasso)\n",
    "print(\"\\nR²:\", round(lasso_pos.score(X_scaled, y), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbda8ad-152f-458e-803e-46e5c986265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_pos = LinearRegression(positive=True)\n",
    "model_pos.fit(X_scaled, y)\n",
    "\n",
    "weights_pos = pd.Series(model_pos.coef_, index=features).sort_values(ascending=False)\n",
    "print(\"Non-Negative Weights:\\n\", weights_pos)\n",
    "print(\"\\nR²:\", round(model_pos.score(X_scaled, y), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c5c49-8db6-4f1c-8f94-94de4441bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Reverse-engineer Bakery RB Grade from Bakery_RB_Overall.csv (non-negative weights, no Breakout Age) =====\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "ROOT = CSV_PATH.parent\n",
    "OUT_DIR = Path(\"./data/Bakery/_derived\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\", \"\", c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\", \"\", cand).lower()\n",
    "        if key in norm:\n",
    "            return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "# canonical features to look for (NO Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    # optional extras if present\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Vertical\":     [\"Vertical\",\"Vertical Jump\"],\n",
    "    \"Broad\":        [\"Broad\",\"Broad Jump\"],\n",
    "    \"Speed Score\":  [\"Speed Score\",\"SpeedScore\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Targets\":      [\"Targets\",\"Target Share\",\"Tgt%\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "\n",
    "# ---------- load ----------\n",
    "if not CSV_PATH.exists():\n",
    "    # fall back to any similarly named overall file\n",
    "    candidates = list(ROOT.glob(\"Bakery_RB_Overall*.csv\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"Could not find {CSV_PATH} or any Bakery_RB_Overall*.csv under {ROOT}\")\n",
    "    CSV_PATH = candidates[0]\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "print(\"Loaded:\", CSV_PATH)\n",
    "print(\"Rows x Cols:\", df.shape)\n",
    "\n",
    "# ---------- map target + features ----------\n",
    "y_col = find_col(df, TARGET_CANDS)\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Could not find RB Grade in columns:\\n{df.columns.tolist()}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found. Inspect df.columns for header names.\")\n",
    "\n",
    "print(\"\\nUsing features (canonical <- sheet column):\")\n",
    "for k,v in mapped.items():\n",
    "    print(f\"  {k:<12} <- {v}\")\n",
    "\n",
    "X_raw = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_raw = to_num(df[y_col])\n",
    "\n",
    "# ---------- drop rows with NaN TARGET ----------\n",
    "mask = y_raw.notna()\n",
    "dropped = len(y_raw) - mask.sum()\n",
    "if dropped:\n",
    "    print(f\"\\nDropped {dropped} rows with NaN RB Grade.\")\n",
    "X_raw = X_raw.loc[mask].reset_index(drop=True)\n",
    "y = y_raw.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# ---------- keep columns with enough data (loose thresholds for real-world sheets) ----------\n",
    "keep = [c for c in X_raw.columns if X_raw[c].notna().sum() >= 5 and X_raw[c].nunique(dropna=True) > 1]\n",
    "if not keep:\n",
    "    raise ValueError(\"All candidate features are too sparse/constant. \"\n",
    "                     \"Relax thresholds or ensure the Overall file has those columns filled.\")\n",
    "X_raw = X_raw[keep]\n",
    "print(\"Kept features:\", keep)\n",
    "\n",
    "# ---------- invert where lower is better (NO Breakout Age) ----------\n",
    "for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\"]:\n",
    "    if c in X_raw.columns:\n",
    "        X_raw[c] = -X_raw[c]\n",
    "\n",
    "# ---------- impute X (median) + standardize ----------\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = imp.fit_transform(X_raw)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# final NaN guards\n",
    "if np.isnan(X_scaled).any():\n",
    "    raise ValueError(\"X still contains NaNs after imputation/standardization. Please inspect your data.\")\n",
    "\n",
    "if y.isna().any():\n",
    "    raise ValueError(\"y contains NaNs after filtering; this should not happen.\")\n",
    "\n",
    "# ---------- fit non-negative models ----------\n",
    "results = {}\n",
    "\n",
    "# (A) Positive OLS\n",
    "ols_pos = LinearRegression(positive=True)\n",
    "ols_pos.fit(X_scaled, y)\n",
    "r2_ols = float(ols_pos.score(X_scaled, y)) if y.var() > 0 else float(\"nan\")\n",
    "results[\"OLS_Positive\"] = (r2_ols, pd.Series(ols_pos.coef_, index=X_raw.columns))\n",
    "\n",
    "# (B) NNLS with mean intercept (stable, non-negative)\n",
    "y_mean = float(y.mean())\n",
    "w_nnls, _ = nnls(X_scaled, (y - y_mean).to_numpy())\n",
    "y_pred = y_mean + X_scaled @ w_nnls\n",
    "r2_nnls = float(1 - np.sum((y - y_pred)**2) / np.sum((y - y_mean)**2)) if y.var() > 0 else float(\"nan\")\n",
    "results[\"NNLS_Positive\"] = (r2_nnls, pd.Series(w_nnls, index=X_raw.columns))\n",
    "\n",
    "# ---------- report ----------\n",
    "rows = []\n",
    "for name, (r2, coefs) in results.items():\n",
    "    row = {\"Model\": name, \"R2\": r2}\n",
    "    row.update({f\"w:{k}\": v for k,v in coefs.items()})\n",
    "    rows.append(row)\n",
    "\n",
    "comp = pd.DataFrame(rows).set_index(\"Model\").sort_values(\"R2\", ascending=False)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"\\n=== Model comparison (non-negative only) ===\")\n",
    "display(comp.round(4))\n",
    "\n",
    "best_name = comp.index[0]\n",
    "best_r2, best_coefs = results[best_name]\n",
    "print(f\"\\nBest non-negative model: {best_name}  (R²={best_r2:.3f})\")\n",
    "print(\"\\nSorted weights (standardized):\")\n",
    "print(best_coefs.sort_values(ascending=False).round(4))\n",
    "\n",
    "# ---------- save artifacts for reuse ----------\n",
    "weights_path = OUT_DIR / f\"rb_weights_{best_name}.csv\"\n",
    "scaler_path  = OUT_DIR / \"rb_scaler.json\"\n",
    "meta_path    = OUT_DIR / \"rb_feature_mapping.json\"\n",
    "\n",
    "best_coefs.to_csv(weights_path, header=[\"coef\"])\n",
    "with open(scaler_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"means\": scaler.mean_.tolist(),\n",
    "        \"scales\": scaler.scale_.tolist(),\n",
    "        \"feature_order\": list(X_raw.columns),\n",
    "        \"intercept_mean\": y_mean,\n",
    "        \"model\": best_name\n",
    "    }, f, indent=2)\n",
    "\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump({\"mapped_columns\": mapped, \"kept_features\": keep, \"target\": y_col}, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved weights → {weights_path}\")\n",
    "print(f\"Saved scaler   → {scaler_path}\")\n",
    "print(f\"Saved mapping  → {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3c373-f374-4e6b-a219-9268597bc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Bakery RB Grade: Multi-seed 80/20 Train/Test Model Bake-off\n",
    "# - Source file: ./data/Bakery/RB/Bakery_RB_Overall.csv\n",
    "# - No Breakout Age is used\n",
    "# - Cleans, imputes, standardizes (per split, train-only to avoid leakage)\n",
    "# - Models: OLS_Positive, Ridge, Lasso, RandomForest, GradientBoosting, NNLS_Positive\n",
    "# - Metrics averaged across multiple random seeds: R2, MAE, RMSE\n",
    "# =========================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# ---------- Config ----------\n",
    "CSV_PATH = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "SEEDS = [0, 1, 2, 3, 4, 42, 123, 777, 1337, 2025]  # 10 runs\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "# Canonical feature names (NO Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    # Optional extras if present (auto-used)\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Vertical\":     [\"Vertical\",\"Vertical Jump\"],\n",
    "    \"Broad\":        [\"Broad\",\"Broad Jump\"],\n",
    "    \"Speed Score\":  [\"Speed Score\",\"SpeedScore\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Targets\":      [\"Targets\",\"Target Share\",\"Tgt%\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def find_col(frame: pd.DataFrame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\", \"\", c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\", \"\", cand).lower()\n",
    "        if key in norm:\n",
    "            return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace(\"%\", \"\", regex=False)\n",
    "           .str.replace(r\"(?i)round\\s*\", \"\", regex=True)\n",
    "           .str.replace(r\"(?i)^r\\s*\", \"\", regex=True)\n",
    "           .str.replace(r\"(?i)(st|nd|rd|th)$\", \"\", regex=True)\n",
    "           .str.replace(\",\", \"\", regex=False)\n",
    "           .str.replace(r\"[^0-9\\.\\-]\", \"\", regex=True))\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def build_clean_matrix(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    y_col = find_col(df, TARGET_CANDS)\n",
    "    if not y_col:\n",
    "        raise ValueError(f\"Could not find RB Grade. Columns:\\n{df.columns.tolist()}\")\n",
    "\n",
    "    mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "    mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "    X = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "    y = to_num(df[y_col])\n",
    "\n",
    "    # Drop rows with missing target\n",
    "    mask = y.notna()\n",
    "    X, y = X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    # Keep loose: feature must have at least 5 non-NaN values and >1 unique value\n",
    "    keep = [c for c in X.columns if X[c].notna().sum() >= 5 and X[c].nunique(dropna=True) > 1]\n",
    "    if not keep:\n",
    "        raise ValueError(\"All candidate features are too sparse/constant. Check your CSV.\")\n",
    "    X = X[keep]\n",
    "\n",
    "    # Invert \"lower is better\"\n",
    "    for c in [\"40 Time\", \"Draft Capital\", \"Shuttle\", \"Three Cone\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = -X[c]\n",
    "\n",
    "    return X, y, keep\n",
    "\n",
    "# ---------- Load & prepare once (feature set decided here) ----------\n",
    "X_all, y_all, kept_features = build_clean_matrix(CSV_PATH)\n",
    "print(\"Features used:\", kept_features)\n",
    "print(\"Dataset size:\", len(y_all))\n",
    "\n",
    "# ---------- Model factory (per-seed) ----------\n",
    "def get_models(random_state: int):\n",
    "    return {\n",
    "        \"OLS_Positive\": LinearRegression(positive=True),\n",
    "        \"Ridge\": Ridge(alpha=1.0, random_state=random_state) if \"random_state\" in Ridge().get_params() else Ridge(alpha=1.0),\n",
    "        \"Lasso\": Lasso(alpha=0.01, max_iter=5000, random_state=random_state) if \"random_state\" in Lasso().get_params() else Lasso(alpha=0.01, max_iter=5000),\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=300, max_depth=None, random_state=random_state),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(random_state=random_state),\n",
    "        \"NNLS_Positive\": \"custom\"\n",
    "    }\n",
    "\n",
    "# ---------- Run multiple 80/20 splits ----------\n",
    "all_runs = []  # collects per-run metrics\n",
    "\n",
    "for seed in SEEDS:\n",
    "    # Split indices (avoid leakage by fitting imputer/scaler on train only)\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "        X_all, y_all, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Impute + scale (train-only fit)\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_imp = imp.fit_transform(X_train_raw)\n",
    "    X_test_imp  = imp.transform(X_test_raw)\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train_imp)\n",
    "    X_test  = scaler.transform(X_test_imp)\n",
    "\n",
    "    models = get_models(seed)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if name == \"NNLS_Positive\":\n",
    "            # Non-negative least squares with mean intercept from TRAIN\n",
    "            mu = float(y_train.mean())\n",
    "            w, _ = nnls(X_train, (y_train - mu).to_numpy())\n",
    "            y_pred = mu + X_test @ w\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "        all_runs.append({\n",
    "            \"seed\": seed,\n",
    "            \"model\": name,\n",
    "            \"R2\": r2_score(y_test, y_pred),\n",
    "            \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred))  # version-safe RMSE\n",
    "        })\n",
    "\n",
    "# ---------- Aggregate results ----------\n",
    "runs_df = pd.DataFrame(all_runs)\n",
    "summary = (runs_df\n",
    "           .groupby(\"model\")\n",
    "           .agg(R2_mean=(\"R2\",\"mean\"),   R2_std=(\"R2\",\"std\"),\n",
    "                MAE_mean=(\"MAE\",\"mean\"), MAE_std=(\"MAE\",\"std\"),\n",
    "                RMSE_mean=(\"RMSE\",\"mean\"), RMSE_std=(\"RMSE\",\"std\"))\n",
    "           .sort_values(\"R2_mean\", ascending=False))\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"\\n=== 10x 80/20 Train/Test — Averaged Metrics ===\")\n",
    "display(summary.round(4))\n",
    "\n",
    "print(\"\\nPer-run results (first few rows):\")\n",
    "display(runs_df.sort_values([\"model\",\"seed\"]).head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f2bdf-bb93-4abc-97bd-a863ea9c867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Use same X_all, y_all from your pipeline\n",
    "\n",
    "param_dist_rf = {\n",
    "    \"n_estimators\": [300, 500, 800, 1200],\n",
    "    \"max_depth\": [None, 5, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", 0.5, 0.7, 1.0],\n",
    "}\n",
    "\n",
    "param_dist_gb = {\n",
    "    \"n_estimators\": [300, 500, 1000, 2000],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [2, 3, 4, 5],\n",
    "    \"subsample\": [0.7, 0.85, 1.0],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "rf_search = RandomizedSearchCV(rf, param_dist_rf, n_iter=30, cv=cv,\n",
    "                               scoring=\"r2\", n_jobs=-1, random_state=42, verbose=2)\n",
    "gb_search = RandomizedSearchCV(gb, param_dist_gb, n_iter=30, cv=cv,\n",
    "                               scoring=\"r2\", n_jobs=-1, random_state=42, verbose=2)\n",
    "\n",
    "# Fit searches\n",
    "rf_search.fit(X_all, y_all)\n",
    "gb_search.fit(X_all, y_all)\n",
    "\n",
    "print(\"Best RF params:\", rf_search.best_params_)\n",
    "print(\"Best RF R²:\", rf_search.best_score_)\n",
    "\n",
    "print(\"Best GB params:\", gb_search.best_params_)\n",
    "print(\"Best GB R²:\", gb_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b775da-f834-418a-941f-76adf0145d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Bakery RB Grade — Feature Engineering + XGBoost/LightGBM + Stacking\n",
    "# Source: ./data/Bakery/RB/Bakery_RB_Overall.csv\n",
    "# Goals:\n",
    "#   - richer features (interactions/ratios)\n",
    "#   - tuned GB/RF + (optional) XGBoost/LightGBM\n",
    "#   - stacking & simple blending\n",
    "# Outputs:\n",
    "#   - Test-set metrics for each model and ensembles\n",
    "# ===========================\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Optional boosters (handled gracefully if not installed)\n",
    "_has_xgb = _has_lgbm = False\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    _has_xgb = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    _has_lgbm = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------- Config ----------\n",
    "CSV_PATH = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = -1\n",
    "CV = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Canonical features (NO Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    # optional extras if present\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Vertical\":     [\"Vertical\",\"Vertical Jump\"],\n",
    "    \"Broad\":        [\"Broad\",\"Broad Jump\"],\n",
    "    \"Speed Score\":  [\"Speed Score\",\"SpeedScore\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Targets\":      [\"Targets\",\"Target Share\",\"Tgt%\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\", \"\", c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\", \"\", cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def load_and_map(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    y_col = find_col(df, TARGET_CANDS)\n",
    "    if not y_col:\n",
    "        raise ValueError(f\"Could not find RB Grade. Columns:\\n{df.columns.tolist()}\")\n",
    "    mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "    mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "    X = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "    y = to_num(df[y_col])\n",
    "\n",
    "    # drop NaN target\n",
    "    mask = y.notna()\n",
    "    X, y = X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    # keep loose: ≥5 non-nan and >1 unique\n",
    "    keep = [c for c in X.columns if X[c].notna().sum() >= 5 and X[c].nunique(dropna=True) > 1]\n",
    "    if not keep:\n",
    "        raise ValueError(\"All candidate features are too sparse/constant. Check your CSV.\")\n",
    "    X = X[keep]\n",
    "    return X, y\n",
    "\n",
    "def add_engineered_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # Invert \"lower is better\" BEFORE interactions\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = -X[c]\n",
    "\n",
    "    # Interactions / ratios (created only if inputs exist)\n",
    "    def safe_mul(a,b,name):\n",
    "        if a in X.columns and b in X.columns:\n",
    "            X[name] = X[a] * X[b]\n",
    "    def safe_div(a,b,name):\n",
    "        if a in X.columns and b in X.columns:\n",
    "            X[name] = X[a] / X[b].replace(0,np.nan)\n",
    "\n",
    "    safe_mul(\"BMI\", \"40 Time\", \"BMIx40\")\n",
    "    safe_mul(\"ELU\", \"YCO/A\", \"ELUxYCOA\")\n",
    "    safe_mul(\"DOM++\", \"Draft Capital\", \"DOMxDraft\")\n",
    "    safe_mul(\"YPC\", \"ELU\", \"YPCxELU\")\n",
    "    safe_div(\"YCO/A\", \"YPC\", \"YCOA_to_YPC\")\n",
    "    safe_div(\"Rec Yards\", \"DOM++\", \"RecYds_to_DOM\")\n",
    "\n",
    "    # Clip extreme ratios to reduce noise\n",
    "    for c in [\"YCOA_to_YPC\",\"RecYds_to_DOM\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].clip(lower=-10, upper=10)\n",
    "\n",
    "    # (Optional) small quantile bins to capture nonlinearity for linear models\n",
    "    for c in [\"DOM++\",\"YPC\",\"ELU\",\"YCO/A\",\"Break%\"]:\n",
    "        if c in X.columns:\n",
    "            X[f\"{c}_q\"] = pd.qcut(X[c].rank(method=\"first\"), q=5, labels=False)\n",
    "\n",
    "    return X\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return dict(\n",
    "        R2 = r2_score(y_true, y_pred),\n",
    "        MAE = mean_absolute_error(y_true, y_pred),\n",
    "        RMSE = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    )\n",
    "\n",
    "# ---------- Build dataset ----------\n",
    "X_raw, y = load_and_map(CSV_PATH)\n",
    "X_feat = add_engineered_features(X_raw)\n",
    "\n",
    "# Split\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_feat, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Impute (median) — train only\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_train_imp = imp.fit_transform(X_train_raw)\n",
    "X_test_imp  = imp.transform(X_test_raw)\n",
    "\n",
    "# Standardize for linear/meta models\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train_imp)\n",
    "X_test_std  = scaler.transform(X_test_imp)\n",
    "\n",
    "# ---------- Base models (with tuning) ----------\n",
    "results = []\n",
    "\n",
    "# Gradient Boosting (tuned)\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "gb_param = {\n",
    "    \"n_estimators\": [500, 1000, 2000],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "    \"max_depth\": [2,3,4,5],\n",
    "    \"subsample\": [0.7, 0.85, 1.0],\n",
    "    \"min_samples_split\": [2,5,10],\n",
    "    \"min_samples_leaf\": [1,2,4],\n",
    "}\n",
    "gb_search = RandomizedSearchCV(\n",
    "    gb, gb_param, n_iter=35, scoring=\"r2\", cv=CV, n_jobs=N_JOBS, random_state=RANDOM_STATE, verbose=0\n",
    ")\n",
    "gb_search.fit(X_train_imp, y_train)\n",
    "gb_best = gb_search.best_estimator_\n",
    "y_pred = gb_best.predict(X_test_imp)\n",
    "results.append((\"GradientBoosting(Tuned)\", gb_search.best_score_, metrics(y_test, y_pred)))\n",
    "\n",
    "# Random Forest (tuned)\n",
    "rf = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "rf_param = {\n",
    "    \"n_estimators\": [600, 1000, 1500],\n",
    "    \"max_depth\": [None, 8, 12, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", 0.5, 0.7, 1.0],\n",
    "}\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf, rf_param, n_iter=35, scoring=\"r2\", cv=CV, n_jobs=N_JOBS, random_state=RANDOM_STATE, verbose=0\n",
    ")\n",
    "rf_search.fit(X_train_imp, y_train)\n",
    "rf_best = rf_search.best_estimator_\n",
    "y_pred = rf_best.predict(X_test_imp)\n",
    "results.append((\"RandomForest(Tuned)\", rf_search.best_score_, metrics(y_test, y_pred)))\n",
    "\n",
    "# Lasso / Ridge on standardized features (baseline linear)\n",
    "lasso = Lasso(alpha=0.005, max_iter=20000).fit(X_train_std, y_train)\n",
    "y_pred = lasso.predict(X_test_std)\n",
    "results.append((\"Lasso\", np.nan, metrics(y_test, y_pred)))\n",
    "\n",
    "ridge = Ridge(alpha=1.0).fit(X_train_std, y_train)\n",
    "y_pred = ridge.predict(X_test_std)\n",
    "results.append((\"Ridge\", np.nan, metrics(y_test, y_pred)))\n",
    "\n",
    "# XGBoost (tuned) — if available\n",
    "if _has_xgb:\n",
    "    xgb = XGBRegressor(\n",
    "        random_state=RANDOM_STATE, objective=\"reg:squarederror\", nthread=-1\n",
    "    )\n",
    "    xgb_param = {\n",
    "        \"n_estimators\": [800, 1200, 2000],\n",
    "        \"max_depth\": [3,4,5,6],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "        \"subsample\": [0.7, 0.85, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "        \"reg_alpha\": [0, 0.1, 1.0],\n",
    "        \"reg_lambda\": [1.0, 2.0, 5.0],\n",
    "    }\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        xgb, xgb_param, n_iter=40, scoring=\"r2\", cv=CV, n_jobs=N_JOBS, random_state=RANDOM_STATE, verbose=0\n",
    "    )\n",
    "    xgb_search.fit(X_train_imp, y_train)\n",
    "    xgb_best = xgb_search.best_estimator__\n",
    "    y_pred = xgb_best.predict(X_test_imp)\n",
    "    results.append((\"XGBoost(Tuned)\", xgb_search.best_score_, metrics(y_test, y_pred)))\n",
    "else:\n",
    "    xgb_best = None\n",
    "\n",
    "# LightGBM (tuned) — if available\n",
    "if _has_lgbm:\n",
    "    lgbm = LGBMRegressor(random_state=RANDOM_STATE)\n",
    "    lgbm_param = {\n",
    "        \"n_estimators\": [800, 1200, 2000],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "        \"max_depth\": [-1, 4, 6, 8],\n",
    "        \"num_leaves\": [31, 63, 127],\n",
    "        \"subsample\": [0.7, 0.85, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "        \"min_child_samples\": [5, 10, 20],\n",
    "        \"reg_alpha\": [0, 0.1, 1.0],\n",
    "        \"reg_lambda\": [1.0, 2.0, 5.0],\n",
    "    }\n",
    "    lgbm_search = RandomizedSearchCV(\n",
    "        lgbm, lgbm_param, n_iter=40, scoring=\"r2\", cv=CV, n_jobs=N_JOBS, random_state=RANDOM_STATE, verbose=0\n",
    "    )\n",
    "    lgbm_search.fit(X_train_imp, y_train)\n",
    "    lgbm_best = lgbm_search.best_estimator_\n",
    "    y_pred = lgbm_best.predict(X_test_imp)\n",
    "    results.append((\"LightGBM(Tuned)\", lgbm_search.best_score_, metrics(y_test, y_pred)))\n",
    "else:\n",
    "    lgbm_best = None\n",
    "\n",
    "# ---------- Simple blend (average of available boosted/forest models) ----------\n",
    "blend_preds = []\n",
    "for model in [gb_best, rf_best, xgb_best, lgbm_best]:\n",
    "    if model is not None:\n",
    "        blend_preds.append(model.predict(X_test_imp))\n",
    "if len(blend_preds) >= 2:\n",
    "    y_blend = np.mean(blend_preds, axis=0)\n",
    "    results.append((\"Blend(GB+RF+XGB+LGBM avail.)\", np.nan, metrics(y_test, y_blend)))\n",
    "\n",
    "# ---------- Stacking ensemble (meta: Ridge on standardized features) ----------\n",
    "base_estimators = []\n",
    "base_for_stack_preds = []\n",
    "\n",
    "# fit base models on TRAIN to produce stack features for TEST\n",
    "for name, model in [\n",
    "    (\"gb\", gb_best),\n",
    "    (\"rf\", rf_best),\n",
    "    (\"xgb\", xgb_best if xgb_best is not None else None),\n",
    "    (\"lgbm\", lgbm_best if lgbm_best is not None else None),\n",
    "]:\n",
    "    if model is not None:\n",
    "        base_estimators.append((name, model))\n",
    "\n",
    "if len(base_estimators) >= 2:\n",
    "    stack = StackingRegressor(\n",
    "        estimators=base_estimators,\n",
    "        final_estimator=Ridge(alpha=1.0),\n",
    "        passthrough=False, n_jobs=N_JOBS\n",
    "    )\n",
    "    stack.fit(X_train_imp, y_train)\n",
    "    y_pred = stack.predict(X_test_imp)\n",
    "    results.append((\"Stacking(Ridge meta)\", np.nan, metrics(y_test, y_pred)))\n",
    "\n",
    "# ---------- Report ----------\n",
    "rows = []\n",
    "for name, cv_best_r2, m in results:\n",
    "    row = {\"Model\": name, \"CV_R2_best\": cv_best_r2, **m}\n",
    "    rows.append(row)\n",
    "report = pd.DataFrame(rows).sort_values(\"R2\", ascending=False)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "display(report.round(4))\n",
    "\n",
    "# Tip: If you're still under 0.90 R², consider:\n",
    "# - Adding more years/features (agility jumps, bench, SOS, OL strength, conference)\n",
    "# - More interactions (e.g., ELU×Break%, DOM×YPC, BMI×YCO/A)\n",
    "# - Bayesian optimization (Optuna) for tighter tuning\n",
    "# - Calibrating/denoising target (e.g., year-wise z-score of RB Grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c04ab-f0f2-4468-a49b-51f668f7247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Leakage-free OOF stacking + NNLS blending for best test R²\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# Optional boosters (skip if not installed)\n",
    "_has_xgb = _has_lgbm = False\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    _has_xgb = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    _has_lgbm = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = -1\n",
    "CV = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Vertical\":     [\"Vertical\",\"Vertical Jump\"],\n",
    "    \"Broad\":        [\"Broad\",\"Broad Jump\"],\n",
    "    \"Speed Score\":  [\"Speed Score\",\"SpeedScore\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Targets\":      [\"Targets\",\"Target Share\",\"Tgt%\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\", \"\", c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\", \"\", cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def load_and_map(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    y_col = find_col(df, TARGET_CANDS)\n",
    "    if not y_col:\n",
    "        raise ValueError(\"RB Grade column not found.\")\n",
    "    mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "    mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "    X = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "    y = to_num(df[y_col])\n",
    "    mask = y.notna()\n",
    "    X, y = X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True)\n",
    "    keep = [c for c in X.columns if X[c].notna().sum() >= 5 and X[c].nunique(dropna=True) > 1]\n",
    "    if not keep: raise ValueError(\"All candidate features are too sparse/constant.\")\n",
    "    return X[keep], y\n",
    "\n",
    "def add_features(X):\n",
    "    X = X.copy()\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    def mul(a,b,n): \n",
    "        if a in X.columns and b in X.columns: X[n] = X[a]*X[b]\n",
    "    def div(a,b,n): \n",
    "        if a in X.columns and b in X.columns: X[n] = X[a]/X[b].replace(0,np.nan)\n",
    "    mul(\"BMI\",\"40 Time\",\"BMIx40\")\n",
    "    mul(\"ELU\",\"YCO/A\",\"ELUxYCOA\")\n",
    "    mul(\"DOM++\",\"Draft Capital\",\"DOMxDraft\")\n",
    "    mul(\"YPC\",\"ELU\",\"YPCxELU\")\n",
    "    div(\"YCO/A\",\"YPC\",\"YCOA_to_YPC\")\n",
    "    div(\"Rec Yards\",\"DOM++\",\"RecYds_to_DOM\")\n",
    "    for c in [\"YCOA_to_YPC\",\"RecYds_to_DOM\"]:\n",
    "        if c in X.columns: X[c] = X[c].clip(-10,10)\n",
    "    for c in [\"DOM++\",\"YPC\",\"ELU\",\"YCO/A\",\"Break%\"]:\n",
    "        if c in X.columns: X[f\"{c}_q\"] = pd.qcut(X[c].rank(method=\"first\"), 5, labels=False)\n",
    "    return X\n",
    "\n",
    "def rmse(y_true,y_pred): \n",
    "    return float(np.sqrt(((y_true-y_pred)**2).mean()))\n",
    "\n",
    "# 1) build data\n",
    "X0, y = load_and_map(CSV_PATH)\n",
    "X = add_features(X0)\n",
    "\n",
    "# 2) split once (final evaluation split)\n",
    "X_tr_raw, X_te_raw, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 3) imputers (fit on train only)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_tr_imp = imp.fit_transform(X_tr_raw)\n",
    "X_te_imp = imp.transform(X_te_raw)\n",
    "\n",
    "# 4) base models with light tuning on TRAIN ONLY\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "gb_param = {\n",
    "    \"n_estimators\": [600,1000,1500,2000],\n",
    "    \"learning_rate\": [0.01,0.03,0.05,0.1],\n",
    "    \"max_depth\": [2,3,4,5],\n",
    "    \"subsample\": [0.7,0.85,1.0],\n",
    "    \"min_samples_split\": [2,5,10],\n",
    "    \"min_samples_leaf\": [1,2,4],\n",
    "}\n",
    "gb_search = RandomizedSearchCV(gb, gb_param, n_iter=35, scoring=\"r2\", cv=KFold(4, shuffle=True, random_state=7),\n",
    "                               n_jobs=N_JOBS, random_state=RANDOM_STATE, verbose=0)\n",
    "gb_search.fit(X_tr_imp, y_tr)\n",
    "gb_best = gb_search.best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "rf_param = {\n",
    "    \"n_estimators\": [800,1200,1600],\n",
    "    \"max_depth\": [None,10,14,20],\n",
    "    \"min_samples_split\": [2,5,10],\n",
    "    \"min_samples_leaf\": [1,2,4],\n",
    "    \"max_features\": [\"sqrt\",0.6,0.9,1.0],\n",
    "}\n",
    "rf_search = RandomizedSearchCV(rf, rf_param, n_iter=30, scoring=\"r2\", cv=KFold(4, shuffle=True, random_state=8),\n",
    "                               n_jobs=N_JOBS, random_state=RANDOM_STATE, verbose=0)\n",
    "rf_search.fit(X_tr_imp, y_tr)\n",
    "rf_best = rf_search.best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "if _has_xgb:\n",
    "    xgb = XGBRegressor(objective=\"reg:squarederror\", random_state=RANDOM_STATE, nthread=-1)\n",
    "    xgb_param = {\n",
    "        \"n_estimators\":[800,1200,2000],\n",
    "        \"max_depth\":[3,4,5,6],\n",
    "        \"learning_rate\":[0.01,0.03,0.05,0.1],\n",
    "        \"subsample\":[0.7,0.85,1.0],\n",
    "        \"colsample_bytree\":[0.7,0.9,1.0],\n",
    "        \"reg_alpha\":[0,0.1,1.0], \"reg_lambda\":[1.0,2.0,5.0]\n",
    "    }\n",
    "    xgb_search = RandomizedSearchCV(xgb, xgb_param, n_iter=35, scoring=\"r2\", cv=KFold(4, shuffle=True, random_state=9),\n",
    "                                    n_jobs=N_JOBS, random_state=RANDOM_STATE, verbose=0)\n",
    "    xgb_search.fit(X_tr_imp, y_tr)\n",
    "    base_models.append((\"xgb\", xgb_search.best_estimator_))\n",
    "\n",
    "if _has_lgbm:\n",
    "    lgbm = LGBMRegressor(random_state=RANDOM_STATE)\n",
    "    lgbm_param = {\n",
    "        \"n_estimators\":[800,1200,2000],\n",
    "        \"learning_rate\":[0.01,0.03,0.05,0.1],\n",
    "        \"max_depth\":[-1,4,6,8],\n",
    "        \"num_leaves\":[31,63,127],\n",
    "        \"subsample\":[0.7,0.85,1.0],\n",
    "        \"colsample_bytree\":[0.7,0.9,1.0],\n",
    "        \"min_child_samples\":[5,10,20],\n",
    "        \"reg_alpha\":[0,0.1,1.0], \"reg_lambda\":[1.0,2.0,5.0],\n",
    "    }\n",
    "    lgbm_search = RandomizedSearchCV(lgbm, lgbm_param, n_iter=35, scoring=\"r2\", cv=KFold(4, shuffle=True, random_state=10),\n",
    "                                     n_jobs=N_JOBS, random_state=RANDOM_STATE, verbose=0)\n",
    "    lgbm_search.fit(X_tr_imp, y_tr)\n",
    "    base_models.append((\"lgbm\", lgbm_search.best_estimator_))\n",
    "\n",
    "# 5) OOF predictions for meta training (no leakage)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_preds = np.zeros((len(X_tr_imp), len(base_models)))\n",
    "te_preds = np.zeros((len(X_te_imp), len(base_models)))\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    fold_te = []\n",
    "    oof = np.zeros(len(X_tr_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_tr_imp):\n",
    "        X_tr_f, X_val_f = X_tr_imp[tr_idx], X_tr_imp[val_idx]\n",
    "        y_tr_f, y_val_f = y_tr.iloc[tr_idx], y_tr.iloc[val_idx]\n",
    "        model.fit(X_tr_f, y_tr_f)\n",
    "        oof[val_idx] = model.predict(X_val_f)\n",
    "        fold_te.append(model.predict(X_te_imp))\n",
    "    oof_preds[:, m_idx] = oof\n",
    "    te_preds[:, m_idx] = np.mean(fold_te, axis=0)\n",
    "\n",
    "# 6) meta models on OOF preds\n",
    "sc_meta = StandardScaler()\n",
    "Z_tr = sc_meta.fit_transform(oof_preds)\n",
    "Z_te = sc_meta.transform(te_preds)\n",
    "\n",
    "ridge_meta = RidgeCV(alphas=np.logspace(-3, 2, 30)).fit(Z_tr, y_tr)\n",
    "elas_meta  = ElasticNetCV(l1_ratio=[.1,.3,.5,.7,.9], alphas=np.logspace(-3,1,20), max_iter=20000).fit(Z_tr, y_tr)\n",
    "\n",
    "# small GB meta (nonlinear combiner)\n",
    "gb_meta = GradientBoostingRegressor(random_state=RANDOM_STATE, n_estimators=500, learning_rate=0.03, max_depth=2)\n",
    "gb_meta.fit(oof_preds, y_tr)\n",
    "\n",
    "# 7) non-negative weighted blend (NNLS) on train\n",
    "w_nnls, _ = nnls(oof_preds, (y_tr - y_tr.mean()).to_numpy())\n",
    "blend_tr = y_tr.mean() + oof_preds @ w_nnls\n",
    "blend_te = y_tr.mean() + te_preds @ w_nnls\n",
    "\n",
    "# 8) evaluate on TEST\n",
    "def eval_and_print(name, y_true, y_hat):\n",
    "    r2 = r2_score(y_true, y_hat); mae = float(np.mean(np.abs(y_true - y_hat))); r = rmse(y_true, y_hat)\n",
    "    print(f\"{name:28s}  R2={r2:.4f}  MAE={mae:.4f}  RMSE={r:.4f}\")\n",
    "    return (name, r2, mae, r)\n",
    "\n",
    "results = []\n",
    "# individual tuned bases\n",
    "for name, model in base_models:\n",
    "    y_hat = model.fit(X_tr_imp, y_tr).predict(X_te_imp)\n",
    "    results.append(eval_and_print(f\"BASE {name}\", y_te, y_hat))\n",
    "\n",
    "# stackers\n",
    "results.append(eval_and_print(\"Stack Ridge meta\", y_te, ridge_meta.predict(Z_te)))\n",
    "results.append(eval_and_print(\"Stack ElasticNet meta\", y_te, elas_meta.predict(Z_te)))\n",
    "results.append(eval_and_print(\"Stack GB meta\", y_te, gb_meta.predict(te_preds)))\n",
    "results.append(eval_and_print(\"NNLS non-neg blend\", y_te, blend_te))\n",
    "\n",
    "# simple average of all bases\n",
    "avg_te = te_preds.mean(axis=1)\n",
    "results.append(eval_and_print(\"Simple average blend\", y_te, avg_te))\n",
    "\n",
    "# summary table\n",
    "summary = pd.DataFrame(results, columns=[\"Model\",\"R2\",\"MAE\",\"RMSE\"]).sort_values(\"R2\", ascending=False)\n",
    "display(summary.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f80b7-c8f2-4899-abb1-46c391884228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Final: Stack Ridge Meta on RB Grade\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "N_JOBS = -1\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\": [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\": [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\": [\"BMI\"],\n",
    "    \"YPC\": [\"YPC\",\"Yards per Carry\",\"Yards/Carry\"],\n",
    "    \"ELU\": [\"ELU\",\"Elusiveness\"],\n",
    "    \"YCO/A\": [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\"],\n",
    "    \"Break%\": [\"Break%\",\"Breakaway %\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\"],\n",
    "    \"Bama\": [\"Bama\",\"Bama Rating\"],\n",
    "    \"Rec Yards\":[\"Receiving Yards\",\"Rec Yds\"]\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower():c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def load_and_map(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    y_col = find_col(df, TARGET_CANDS)\n",
    "    mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "    mapped = {k:v for k,v in mapped.items() if v}\n",
    "    X = pd.DataFrame({feat: to_num(df[col]) for feat,col in mapped.items()})\n",
    "    y = to_num(df[y_col])\n",
    "    mask = y.notna()\n",
    "    return X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "def add_features(X):\n",
    "    X = X.copy()\n",
    "    for c in [\"40 Time\",\"Draft Capital\"]:  # invert \"lower is better\"\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    return X\n",
    "\n",
    "def rmse(y_true,y_pred): return np.sqrt(((y_true-y_pred)**2).mean())\n",
    "\n",
    "# ---------- load ----------\n",
    "X0,y = load_and_map(CSV_PATH)\n",
    "X = add_features(X0)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# impute\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_train_imp = imp.fit_transform(X_train)\n",
    "X_test_imp  = imp.transform(X_test)\n",
    "\n",
    "# ---------- base models ----------\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "gb_param = {\n",
    "    \"n_estimators\":[800,1200],\n",
    "    \"learning_rate\":[0.03,0.05],\n",
    "    \"max_depth\":[3,4],\n",
    "    \"subsample\":[0.8,1.0]\n",
    "}\n",
    "gb_best = RandomizedSearchCV(gb, gb_param, n_iter=5, scoring=\"r2\",\n",
    "                             cv=3, n_jobs=N_JOBS, random_state=RANDOM_STATE).fit(X_train_imp,y_train).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "rf_param = {\n",
    "    \"n_estimators\":[800,1200],\n",
    "    \"max_depth\":[None,12,16],\n",
    "    \"min_samples_split\":[2,5],\n",
    "    \"min_samples_leaf\":[1,2]\n",
    "}\n",
    "rf_best = RandomizedSearchCV(rf, rf_param, n_iter=5, scoring=\"r2\",\n",
    "                             cv=3, n_jobs=N_JOBS, random_state=RANDOM_STATE).fit(X_train_imp,y_train).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "# ---------- OOF stacking ----------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_preds = np.zeros((len(X_train_imp), len(base_models)))\n",
    "test_preds = np.zeros((len(X_test_imp), len(base_models)))\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    fold_preds = []\n",
    "    oof = np.zeros(len(X_train_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_train_imp):\n",
    "        model.fit(X_train_imp[tr_idx], y_train.iloc[tr_idx])\n",
    "        oof[val_idx] = model.predict(X_train_imp[val_idx])\n",
    "        fold_preds.append(model.predict(X_test_imp))\n",
    "    oof_preds[:,m_idx] = oof\n",
    "    test_preds[:,m_idx] = np.mean(fold_preds, axis=0)\n",
    "\n",
    "sc_meta = StandardScaler()\n",
    "Z_train = sc_meta.fit_transform(oof_preds)\n",
    "Z_test  = sc_meta.transform(test_preds)\n",
    "\n",
    "ridge_meta = RidgeCV(alphas=np.logspace(-3,2,30)).fit(Z_train,y_train)\n",
    "y_pred = ridge_meta.predict(Z_test)\n",
    "\n",
    "# ---------- evaluate ----------\n",
    "print(\"\\n=== Stack Ridge Meta Results on Test Set ===\")\n",
    "print(\"R²:\", round(r2_score(y_test,y_pred),4))\n",
    "print(\"MAE:\", round(mean_absolute_error(y_test,y_pred),4))\n",
    "print(\"RMSE:\", round(rmse(y_test,y_pred),4))\n",
    "\n",
    "# show predictions vs actuals\n",
    "results = pd.DataFrame({\n",
    "    \"Actual_RB_Grade\": y_test.values,\n",
    "    \"Predicted_RB_Grade\": y_pred\n",
    "})\n",
    "print(\"\\nSample predictions:\")\n",
    "print(results.head(15).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30796a3-13bf-40fb-91ee-42c300530f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Stack Ridge Meta — 80/20 split, year-aware features, names in output\n",
    "# Source CSV: ./data/Bakery/RB/Bakery_RB_Overall.csv\n",
    "# Output CSV: ./data/Bakery/_derived/stack_ridge_predictions.csv\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH   = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_PATH   = Path(\"./data/Bakery/_derived/stack_ridge_predictions.csv\")\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE   = 0.20\n",
    "N_JOBS      = -1\n",
    "\n",
    "# canonical feature names (NO Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    # optional, used if present\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "YEAR_CANDS   = [\"Year\",\"Draft Year\",\"Class Year\",\"class_year\",\"Draft Class\",\"DraftClass\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def load_sheet(path: Path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    y_col   = find_col(df, TARGET_CANDS)\n",
    "    name_c  = find_col(df, NAME_CANDS) or \"Player\"\n",
    "    year_c  = find_col(df, YEAR_CANDS)   # can be None\n",
    "\n",
    "    mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "    mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "    X = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "    y = to_num(df[y_col])\n",
    "    names = df[name_c].astype(str).fillna(\"\").values\n",
    "    years = df[year_c].astype(int).values if year_c else None\n",
    "\n",
    "    # drop rows with missing target\n",
    "    mask = y.notna()\n",
    "    return X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True), \\\n",
    "           pd.Series(names)[mask].reset_index(drop=True), \\\n",
    "           (pd.Series(years)[mask].reset_index(drop=True) if years is not None else None)\n",
    "\n",
    "def basic_interactions(X):\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\" BEFORE interactions\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    # light interactions\n",
    "    def mul(a,b,n): \n",
    "        if a in X.columns and b in X.columns: X[n] = X[a]*X[b]\n",
    "    def div(a,b,n):\n",
    "        if a in X.columns and b in X.columns:\n",
    "            denom = X[b].replace(0, np.nan)\n",
    "            X[n] = (X[a]/denom).clip(-10,10)\n",
    "    mul(\"BMI\",\"40 Time\",\"BMIx40\")\n",
    "    mul(\"ELU\",\"YCO/A\",\"ELUxYCOA\")\n",
    "    mul(\"DOM++\",\"Draft Capital\",\"DOMxDraft\")\n",
    "    mul(\"YPC\",\"ELU\",\"YPCxELU\")\n",
    "    div(\"YCO/A\",\"YPC\",\"YCOA_to_YPC\")\n",
    "    return X\n",
    "\n",
    "def add_year_z(train_df, test_df, year_series, cols_for_z):\n",
    "    \"\"\"\n",
    "    Compute per-year z-scores using TRAIN-only stats; apply to both train/test.\n",
    "    \"\"\"\n",
    "    if year_series is None:\n",
    "        return train_df, test_df  # nothing to do\n",
    "\n",
    "    train = train_df.copy(); test = test_df.copy()\n",
    "    train[\"__YR__\"] = year_series.loc[train.index].values\n",
    "    # stats on TRAIN only\n",
    "    grp = train.groupby(\"__YR__\")[cols_for_z].agg([\"mean\",\"std\"])\n",
    "    # helper\n",
    "    def zify(df):\n",
    "        df = df.copy()\n",
    "        df[\"__YR__\"] = year_series.loc[df.index].values\n",
    "        for c in cols_for_z:\n",
    "            mu = df[\"__YR__\"].map(grp[(c,\"mean\")])\n",
    "            sd = df[\"__YR__\"].map(grp[(c,\"std\")]).replace(0,np.nan)\n",
    "            z  = (df[c]-mu)/sd\n",
    "            df[c+\"_yrz\"] = z.fillna(0.0).values\n",
    "        return df.drop(columns=\"__YR__\")\n",
    "\n",
    "    return zify(train_df), zify(test_df)\n",
    "\n",
    "# ---------- pipeline ----------\n",
    "# 1) load\n",
    "X0, y, names, years = load_sheet(CSV_PATH)\n",
    "\n",
    "# 2) interactions first\n",
    "X1 = basic_interactions(X0)\n",
    "\n",
    "# 3) split (keep names/years aligned)\n",
    "X_tr_raw, X_te_raw, y_tr, y_te, names_tr, names_te, years_tr, years_te = train_test_split(\n",
    "    X1, y, names, years, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# 4) year-aware z features (train-only stats applied to test)\n",
    "cols_for_z = [c for c in [\"DOM++\",\"40 Time\",\"BMI\",\"YPC\",\"ELU\",\"YCO/A\",\"Break%\",\"Draft Capital\",\"Bama\"] if c in X_tr_raw.columns]\n",
    "X_tr_raw, X_te_raw = add_year_z(X_tr_raw, X_te_raw, pd.Series(years) if years is not None else None, cols_for_z)\n",
    "\n",
    "# 5) impute (median) — fit on train only\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_tr_imp = imp.fit_transform(X_tr_raw)\n",
    "X_te_imp = imp.transform(X_te_raw)\n",
    "\n",
    "# 6) base models (light tuning to keep fast/reproducible)\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = {\n",
    "    \"n_estimators\":[800,1200],\n",
    "    \"learning_rate\":[0.03,0.05],\n",
    "    \"max_depth\":[3,4],\n",
    "    \"subsample\":[0.8,1.0],\n",
    "    \"min_samples_split\":[2,5],\n",
    "    \"min_samples_leaf\":[1,2],\n",
    "}\n",
    "gb_best = RandomizedSearchCV(gb, gb_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                             random_state=RANDOM_SEED).fit(X_tr_imp, y_tr).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "rf_param = {\n",
    "    \"n_estimators\":[800,1200],\n",
    "    \"max_depth\":[None,12,16],\n",
    "    \"min_samples_split\":[2,5],\n",
    "    \"min_samples_leaf\":[1,2],\n",
    "    \"max_features\":[\"sqrt\", 0.8, 1.0],\n",
    "}\n",
    "rf_best = RandomizedSearchCV(rf, rf_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                             random_state=RANDOM_SEED).fit(X_tr_imp, y_tr).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "# 7) OOF stacking (leakage-free)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_tr_imp), len(base_models)))\n",
    "te  = np.zeros((len(X_te_imp), len(base_models)))\n",
    "for j,(nm, mdl) in enumerate(base_models):\n",
    "    preds_te_folds = []\n",
    "    fold_oof = np.zeros(len(X_tr_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_tr_imp):\n",
    "        mdl.fit(X_tr_imp[tr_idx], y_tr.iloc[tr_idx])\n",
    "        fold_oof[val_idx] = mdl.predict(X_tr_imp[val_idx])\n",
    "        preds_te_folds.append(mdl.predict(X_te_imp))\n",
    "    oof[:,j] = fold_oof\n",
    "    te[:,j]  = np.mean(preds_te_folds, axis=0)\n",
    "\n",
    "# 8) meta learner (RidgeCV on standardized OOF)\n",
    "sc_meta = StandardScaler()\n",
    "Z_tr = sc_meta.fit_transform(oof)\n",
    "Z_te = sc_meta.transform(te)\n",
    "\n",
    "ridge_meta = RidgeCV(alphas=np.logspace(-3, 2, 30)).fit(Z_tr, y_tr)\n",
    "y_hat = ridge_meta.predict(Z_te)\n",
    "\n",
    "# 9) metrics + output\n",
    "r2   = r2_score(y_te, y_hat)\n",
    "mae  = mean_absolute_error(y_te, y_hat)\n",
    "rmse_val = rmse(y_te, y_hat)\n",
    "\n",
    "print(\"\\n=== Stack Ridge Meta — Test Results ===\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse_val:.4f}\")\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"Player\": names_te.values,\n",
    "    \"Actual_RB_Grade\": y_te.values,\n",
    "    \"Predicted_RB_Grade\": y_hat,\n",
    "    \"Error\": (y_hat - y_te.values)\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 by Actual RB Grade (test set):\")\n",
    "print(pred_df.head(15).round(3))\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "pred_df.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\nSaved predictions → {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85475807-b567-4bcc-b722-495d46a72687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Apply Stack Ridge Meta to entire dataset\n",
    "# Train on ALL rows, predict for ALL rows\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH   = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_PATH   = Path(\"./data/Bakery/_derived/stack_ridge_all_players.csv\")\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "\n",
    "# canonical features (no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def basic_interactions(X):\n",
    "    X = X.copy()\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    if \"BMI\" in X and \"40 Time\" in X: X[\"BMIx40\"] = X[\"BMI\"]*X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X: X[\"ELUxYCOA\"] = X[\"ELU\"]*X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"]*X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X: X[\"YPCxELU\"] = X[\"YPC\"]*X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "X = pd.DataFrame({feat: to_num(df[col]) for feat,col in mapped.items()})\n",
    "y = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y.notna()\n",
    "X, y, names = X[mask], y[mask], names[mask]\n",
    "\n",
    "# interactions\n",
    "X = basic_interactions(X)\n",
    "\n",
    "# impute\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X)\n",
    "\n",
    "# ---------- train base models (with light tuning) ----------\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = {\"n_estimators\":[800],\"learning_rate\":[0.05],\"max_depth\":[4],\"subsample\":[0.85]}\n",
    "gb_best = RandomizedSearchCV(gb, gb_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS).fit(X_imp,y).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "rf_param = {\"n_estimators\":[800],\"max_depth\":[12],\"min_samples_split\":[2],\"min_samples_leaf\":[1]}\n",
    "rf_best = RandomizedSearchCV(rf, rf_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS).fit(X_imp,y).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "# ---------- stacking with OOF ----------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_imp), len(base_models)))\n",
    "for j,(nm,mdl) in enumerate(base_models):\n",
    "    fold_preds = np.zeros(len(X_imp))\n",
    "    for tr_idx,val_idx in kf.split(X_imp):\n",
    "        mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "        fold_preds[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "    oof[:,j] = fold_preds\n",
    "\n",
    "# meta learner (Ridge on OOF)\n",
    "sc_meta = StandardScaler()\n",
    "Z = sc_meta.fit_transform(oof)\n",
    "ridge_meta = RidgeCV(alphas=np.logspace(-3,2,30)).fit(Z,y)\n",
    "\n",
    "# retrain base models on full data\n",
    "full_preds = []\n",
    "for nm,mdl in base_models:\n",
    "    mdl.fit(X_imp,y)\n",
    "    full_preds.append(mdl.predict(X_imp))\n",
    "stack_inputs = np.vstack(full_preds).T\n",
    "stack_inputs = sc_meta.transform(stack_inputs)\n",
    "\n",
    "y_hat = ridge_meta.predict(stack_inputs)\n",
    "\n",
    "# ---------- results ----------\n",
    "out = pd.DataFrame({\n",
    "    \"Player\": names.values,\n",
    "    \"Actual_RB_Grade\": y.values,\n",
    "    \"Predicted_RB_Grade\": y_hat,\n",
    "    \"Error\": y_hat - y.values\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Full Dataset Results (Stack Ridge Meta) ===\")\n",
    "print(out.head(20).round(3))\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\nSaved → {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b659a-64ea-4926-808d-9a2eb468e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Full-dataset application + metrics + feature impact (Stack Ridge Meta)\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "PLAYERS_CSV = OUT_DIR / \"stack_ridge_all_players.csv\"\n",
    "IMPACT_CSV  = OUT_DIR / \"stack_ridge_feature_impact.csv\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "N_SPLITS    = 5  # for OOF stacking\n",
    "\n",
    "# canonical features (no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\"\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    # a few high-signal interactions\n",
    "    if \"BMI\" in X and \"40 Time\" in X: X[\"BMIx40\"] = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X: X[\"ELUxYCOA\"] = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X: X[\"YPCxELU\"] = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "X0 = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y  = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "# drop rows with missing target\n",
    "mask = y.notna()\n",
    "X0, y, names = X0.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True), names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# interactions\n",
    "X = basic_interactions(X0)\n",
    "\n",
    "# impute full matrix (for later refit + predictions)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X)\n",
    "feature_order = list(X.columns)\n",
    "\n",
    "# ---------- base models (tuned lightly) ----------\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = { \"n_estimators\":[800,1200], \"learning_rate\":[0.03,0.05], \"max_depth\":[3,4], \"subsample\":[0.85,1.0],\n",
    "             \"min_samples_split\":[2,5], \"min_samples_leaf\":[1,2] }\n",
    "gb_best = RandomizedSearchCV(gb, gb_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                             random_state=RANDOM_SEED).fit(X_imp, y).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "rf_param = { \"n_estimators\":[800,1200], \"max_depth\":[None,12,16], \"min_samples_split\":[2,5], \"min_samples_leaf\":[1,2],\n",
    "             \"max_features\":[\"sqrt\",0.8,1.0] }\n",
    "rf_best = RandomizedSearchCV(rf, rf_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                             random_state=RANDOM_SEED).fit(X_imp, y).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "# ---------- OOF stacking for honest metrics ----------\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_imp), len(base_models)))\n",
    "for j,(nm, mdl) in enumerate(base_models):\n",
    "    fold_preds = np.zeros(len(X_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_imp):\n",
    "        mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "        fold_preds[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "    oof[:, j] = fold_preds\n",
    "\n",
    "# meta on standardized OOF\n",
    "sc_meta = StandardScaler()\n",
    "Z = sc_meta.fit_transform(oof)\n",
    "ridge_meta = RidgeCV(alphas=np.logspace(-3, 2, 30)).fit(Z, y)\n",
    "\n",
    "# OOF metrics (generalization estimate)\n",
    "y_oof_hat = ridge_meta.predict(Z)\n",
    "oof_r2  = r2_score(y, y_oof_hat)\n",
    "oof_mae = mean_absolute_error(y, y_oof_hat)\n",
    "oof_rmse= rmse(y, y_oof_hat)\n",
    "\n",
    "# ---------- full refit on all data, then predict all rows ----------\n",
    "# refit bases on ALL data\n",
    "base_preds_full = []\n",
    "for nm, mdl in base_models:\n",
    "    mdl.fit(X_imp, y)\n",
    "    base_preds_full.append(mdl.predict(X_imp))\n",
    "stack_inputs = np.vstack(base_preds_full).T\n",
    "stack_inputs_std = sc_meta.transform(stack_inputs)  # use same scaler as OOF\n",
    "y_full_hat = ridge_meta.predict(stack_inputs_std)\n",
    "\n",
    "# full fit metrics (in-sample)\n",
    "full_r2  = r2_score(y, y_full_hat)\n",
    "full_mae = mean_absolute_error(y, y_full_hat)\n",
    "full_rmse= rmse(y, y_full_hat)\n",
    "\n",
    "# ---------- export per-player predictions ----------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "players_out = pd.DataFrame({\n",
    "    \"Player\": names.values,\n",
    "    \"Actual_RB_Grade\": y.values,\n",
    "    \"Predicted_RB_Grade\": y_full_hat,\n",
    "    \"Error\": (y_full_hat - y.values)\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "players_out.to_csv(PLAYERS_CSV, index=False)\n",
    "\n",
    "# ---------- feature impact ----------\n",
    "# 1) raw importances from bases\n",
    "gb_imp = pd.Series(gb_best.feature_importances_, index=feature_order)\n",
    "rf_imp = pd.Series(rf_best.feature_importances_, index=feature_order)\n",
    "\n",
    "# 2) normalize each to sum to 1 (avoid bias from scale)\n",
    "gb_imp_n = gb_imp / (gb_imp.sum() + 1e-12)\n",
    "rf_imp_n = rf_imp / (rf_imp.sum() + 1e-12)\n",
    "\n",
    "# 3) meta weights from Ridge on standardized base predictions\n",
    "meta_coef = pd.Series(ridge_meta.coef_, index=[nm for nm,_ in base_models])\n",
    "# Allow signs but normalize by L1 to represent relative influence\n",
    "meta_w = meta_coef / (meta_coef.abs().sum() + 1e-12)\n",
    "\n",
    "# 4) combine: weighted sum of normalized importances\n",
    "# (Only GB and RF here; extend if you add more bases)\n",
    "combined = meta_w.get(\"gb\",0.0)*gb_imp_n + meta_w.get(\"rf\",0.0)*rf_imp_n\n",
    "impact_df = pd.DataFrame({\n",
    "    \"Impact_MetaWeighted\": combined,\n",
    "    \"GB_Importance\": gb_imp_n,\n",
    "    \"RF_Importance\": rf_imp_n,\n",
    "}).sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "impact_df.to_csv(IMPACT_CSV, index=False)\n",
    "\n",
    "# ---------- print summary ----------\n",
    "print(\"\\n=== Stack Ridge Meta — Metrics ===\")\n",
    "print(f\"OOF R²:   {oof_r2:.4f}   (generalization estimate)\")\n",
    "print(f\"OOF MAE:  {oof_mae:.4f}\")\n",
    "print(f\"OOF RMSE: {oof_rmse:.4f}\")\n",
    "print(f\"\\nFull Fit R²:   {full_r2:.4f}   (in-sample)\")\n",
    "print(f\"Full Fit MAE:  {full_mae:.4f}\")\n",
    "print(f\"Full Fit RMSE: {full_rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nSaved per-player predictions → {PLAYERS_CSV}\")\n",
    "print(f\"Saved feature impact table  → {IMPACT_CSV}\")\n",
    "\n",
    "# Show top 15 players + top 15 features\n",
    "print(\"\\nTop players by Actual RB Grade:\")\n",
    "print(players_out.head(15).round(3))\n",
    "\n",
    "print(\"\\nTop features by meta-weighted impact:\")\n",
    "print(impact_df.head(15).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb17b0c-3ddd-437e-99ce-f5ca75a2419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Ablation: does DOMxDraft add value?\n",
    "# Trains Stack Ridge Meta twice on ALL rows:\n",
    "#   (A) with interactions: [\"BMIx40\",\"ELUxYCOA\",\"DOMxDraft\",\"YPCxELU\"]\n",
    "#   (B) same but WITHOUT \"DOMxDraft\"\n",
    "# Saves metrics, per-player predictions, and feature impacts for both runs.\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "N_SPLITS    = 5\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true,y_pred): return float(np.sqrt(mean_squared_error(y_true,y_pred)))\n",
    "\n",
    "def load_base_table(path: Path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    y_col = find_col(df, TARGET_CANDS)\n",
    "    name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "    mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "    mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "    X = pd.DataFrame({feat: to_num(df[col]) for feat,col in mapped.items()})\n",
    "    y = to_num(df[y_col])\n",
    "    names = df[name_col].astype(str).fillna(\"\")\n",
    "    mask = y.notna()\n",
    "    return X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True), names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "def add_interactions(X: pd.DataFrame, interactions: list) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\" BEFORE interactions\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    def mul(a,b,n):\n",
    "        if a in X.columns and b in X.columns:\n",
    "            X[n] = X[a]*X[b]\n",
    "    if \"BMIx40\" in interactions:\n",
    "        mul(\"BMI\",\"40 Time\",\"BMIx40\")\n",
    "    if \"ELUxYCOA\" in interactions:\n",
    "        mul(\"ELU\",\"YCO/A\",\"ELUxYCOA\")\n",
    "    if \"DOMxDraft\" in interactions:\n",
    "        mul(\"DOM++\",\"Draft Capital\",\"DOMxDraft\")\n",
    "    if \"YPCxELU\" in interactions:\n",
    "        mul(\"YPC\",\"ELU\",\"YPCxELU\")\n",
    "    return X\n",
    "\n",
    "def train_stack_ridge_allrows(X: pd.DataFrame, y: pd.Series, names: pd.Series, tag: str):\n",
    "    \"\"\"\n",
    "    Trains GB+RF bases + Ridge meta on ALL rows.\n",
    "    Uses OOF predictions for meta training to report honest OOF metrics.\n",
    "    Then refits bases on ALL rows and predicts everyone.\n",
    "    Saves per-player CSV and feature-impact CSV with suffix `tag`.\n",
    "    Returns metrics and path info.\n",
    "    \"\"\"\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    players_csv = OUT_DIR / f\"stack_ridge_all_players_{tag}.csv\"\n",
    "    impact_csv  = OUT_DIR / f\"stack_ridge_feature_impact_{tag}.csv\"\n",
    "\n",
    "    # impute\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    X_imp = imp.fit_transform(X)\n",
    "    feature_order = list(X.columns)\n",
    "\n",
    "    # base models (light but solid tuning)\n",
    "    gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "    gb_param = {\n",
    "        \"n_estimators\":[800,1200],\n",
    "        \"learning_rate\":[0.03,0.05],\n",
    "        \"max_depth\":[3,4],\n",
    "        \"subsample\":[0.85,1.0],\n",
    "        \"min_samples_split\":[2,5],\n",
    "        \"min_samples_leaf\":[1,2],\n",
    "    }\n",
    "    gb_best = RandomizedSearchCV(gb, gb_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                                 random_state=RANDOM_SEED).fit(X_imp,y).best_estimator_\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "    rf_param = {\n",
    "        \"n_estimators\":[800,1200],\n",
    "        \"max_depth\":[None,12,16],\n",
    "        \"min_samples_split\":[2,5],\n",
    "        \"min_samples_leaf\":[1,2],\n",
    "        \"max_features\":[\"sqrt\",0.8,1.0],\n",
    "    }\n",
    "    rf_best = RandomizedSearchCV(rf, rf_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                                 random_state=RANDOM_SEED).fit(X_imp,y).best_estimator_\n",
    "\n",
    "    base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "    # OOF stacking (no leakage)\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    oof = np.zeros((len(X_imp), len(base_models)))\n",
    "    for j,(nm, mdl) in enumerate(base_models):\n",
    "        fold_preds = np.zeros(len(X_imp))\n",
    "        for tr_idx, val_idx in kf.split(X_imp):\n",
    "            mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "            fold_preds[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "        oof[:,j] = fold_preds\n",
    "\n",
    "    sc_meta = StandardScaler()\n",
    "    Z = sc_meta.fit_transform(oof)\n",
    "    ridge_meta = RidgeCV(alphas=np.logspace(-3,2,30)).fit(Z, y)\n",
    "\n",
    "    # OOF metrics\n",
    "    y_oof = ridge_meta.predict(Z)\n",
    "    oof_r2, oof_mae, oof_rmse = r2_score(y,y_oof), mean_absolute_error(y,y_oof), rmse(y,y_oof)\n",
    "\n",
    "    # Refit bases on ALL rows, predict ALL rows\n",
    "    base_full_preds = []\n",
    "    for nm, mdl in base_models:\n",
    "        mdl.fit(X_imp, y)\n",
    "        base_full_preds.append(mdl.predict(X_imp))\n",
    "    stack_inputs_std = sc_meta.transform(np.vstack(base_full_preds).T)\n",
    "    y_hat = ridge_meta.predict(stack_inputs_std)\n",
    "\n",
    "    # Full-fit metrics (in-sample)\n",
    "    full_r2, full_mae, full_rmse = r2_score(y,y_hat), mean_absolute_error(y,y_hat), rmse(y,y_hat)\n",
    "\n",
    "    # Save per-player predictions\n",
    "    players_out = pd.DataFrame({\n",
    "        \"Player\": names.values,\n",
    "        \"Actual_RB_Grade\": y.values,\n",
    "        \"Predicted_RB_Grade\": y_hat,\n",
    "        \"Error\": y_hat - y.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    players_out.to_csv(players_csv, index=False)\n",
    "\n",
    "    # Feature impact (meta-weighted)\n",
    "    gb_imp = pd.Series(gb_best.feature_importances_, index=feature_order)\n",
    "    rf_imp = pd.Series(rf_best.feature_importances_, index=feature_order)\n",
    "    gb_imp_n = gb_imp / (gb_imp.sum() + 1e-12)\n",
    "    rf_imp_n = rf_imp / (rf_imp.sum() + 1e-12)\n",
    "    meta_coef = pd.Series(ridge_meta.coef_, index=[nm for nm,_ in base_models])\n",
    "    meta_w = meta_coef / (meta_coef.abs().sum() + 1e-12)\n",
    "    combined = meta_w.get(\"gb\",0.0)*gb_imp_n + meta_w.get(\"rf\",0.0)*rf_imp_n\n",
    "    impact_df = pd.DataFrame({\n",
    "        \"Impact_MetaWeighted\": combined,\n",
    "        \"GB_Importance\": gb_imp_n,\n",
    "        \"RF_Importance\": rf_imp_n,\n",
    "    }).sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "    impact_df.to_csv(impact_csv, index=False)\n",
    "\n",
    "    metrics = {\n",
    "        \"tag\": tag,\n",
    "        \"OOF_R2\": oof_r2, \"OOF_MAE\": oof_mae, \"OOF_RMSE\": oof_rmse,\n",
    "        \"Full_R2\": full_r2, \"Full_MAE\": full_mae, \"Full_RMSE\": full_rmse,\n",
    "        \"players_csv\": str(players_csv),\n",
    "        \"impact_csv\": str(impact_csv),\n",
    "    }\n",
    "    return metrics, players_out.head(8), impact_df.head(12)\n",
    "\n",
    "# ===========================\n",
    "# Run ablation\n",
    "# ===========================\n",
    "X_base, y, names = load_base_table(CSV_PATH)\n",
    "\n",
    "interactions_with = [\"BMIx40\",\"ELUxYCOA\",\"DOMxDraft\",\"YPCxELU\"]\n",
    "interactions_wo   = [\"BMIx40\",\"ELUxYCOA\",\"YPCxELU\"]  # DOMxDraft removed\n",
    "\n",
    "X_with = add_interactions(X_base, interactions_with)\n",
    "X_wo   = add_interactions(X_base, interactions_wo)\n",
    "\n",
    "m_with, sample_players_with, top_feats_with = train_stack_ridge_allrows(X_with, y, names, tag=\"with_domxdraft\")\n",
    "m_wo,   sample_players_wo,   top_feats_wo   = train_stack_ridge_allrows(X_wo,   y, names, tag=\"no_domxdraft\")\n",
    "\n",
    "# Compare\n",
    "summary = pd.DataFrame([m_with, m_wo]).drop(columns=[\"players_csv\",\"impact_csv\"])\n",
    "delta = summary.set_index(\"tag\")\n",
    "print(\"\\n=== Ablation Summary (OOF = generalization estimate) ===\")\n",
    "print(delta[[\"OOF_R2\",\"OOF_MAE\",\"OOF_RMSE\",\"Full_R2\",\"Full_MAE\",\"Full_RMSE\"]].round(4))\n",
    "\n",
    "print(\"\\nTop players (with DOMxDraft):\")\n",
    "print(sample_players_with.round(3))\n",
    "print(\"\\nTop players (no DOMxDraft):\")\n",
    "print(sample_players_wo.round(3))\n",
    "\n",
    "print(\"\\nTop features by impact (with DOMxDraft):\")\n",
    "print(top_feats_with.round(4))\n",
    "print(\"\\nTop features by impact (no DOMxDraft):\")\n",
    "print(top_feats_wo.round(4))\n",
    "\n",
    "print(f\"\\nPer-player CSVs:\\n  {m_with['players_csv']}\\n  {m_wo['players_csv']}\")\n",
    "print(f\"Feature impact CSVs:\\n  {m_with['impact_csv']}\\n  {m_wo['impact_csv']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780cff0-1ad0-457e-a0cd-39cd04ebe63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# One-by-one interaction ablation for Stack Ridge Meta\n",
    "# - Baseline uses all interactions in `ALL_INTERACTIONS`\n",
    "# - Runs ablation removing each interaction individually + \"no interactions\"\n",
    "# - Outputs OOF (CV) metrics, Full-fit metrics, ranked Δ vs baseline\n",
    "# - Saves per-player predictions and feature impact for every run\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived/ablation\")\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "N_SPLITS    = 5\n",
    "\n",
    "# choose your baseline interactions here\n",
    "ALL_INTERACTIONS = [\"BMIx40\", \"ELUxYCOA\", \"DOMxDraft\", \"YPCxELU\"]\n",
    "\n",
    "# canonical features (no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true,y_pred): return float(np.sqrt(mean_squared_error(y_true,y_pred)))\n",
    "\n",
    "def load_base_table(path: Path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    y_col = find_col(df, TARGET_CANDS)\n",
    "    name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "    mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "    mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "    X = pd.DataFrame({feat: to_num(df[col]) for feat,col in mapped.items()})\n",
    "    y = to_num(df[y_col])\n",
    "    names = df[name_col].astype(str).fillna(\"\")\n",
    "    mask = y.notna()\n",
    "    return X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True), names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "def add_interactions(X: pd.DataFrame, interactions: list) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\" BEFORE interactions\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    def mul(a,b,n):\n",
    "        if a in X.columns and b in X.columns:\n",
    "            X[n] = X[a]*X[b]\n",
    "    if \"BMIx40\" in interactions:   mul(\"BMI\",\"40 Time\",\"BMIx40\")\n",
    "    if \"ELUxYCOA\" in interactions: mul(\"ELU\",\"YCO/A\",\"ELUxYCOA\")\n",
    "    if \"DOMxDraft\" in interactions:mul(\"DOM++\",\"Draft Capital\",\"DOMxDraft\")\n",
    "    if \"YPCxELU\" in interactions:  mul(\"YPC\",\"ELU\",\"YPCxELU\")\n",
    "    return X\n",
    "\n",
    "def train_stack_ridge_allrows(X: pd.DataFrame, y: pd.Series, names: pd.Series, tag: str):\n",
    "    \"\"\"\n",
    "    Trains GB+RF bases + Ridge meta on ALL rows.\n",
    "    Uses OOF predictions for meta training => OOF metrics (honest).\n",
    "    Refit bases on ALL rows => per-player predictions.\n",
    "    Saves per-player CSV + feature-impact CSV (meta-weighted).\n",
    "    \"\"\"\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    players_csv = OUT_DIR / f\"players_{tag}.csv\"\n",
    "    impact_csv  = OUT_DIR / f\"impact_{tag}.csv\"\n",
    "\n",
    "    # impute\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    X_imp = imp.fit_transform(X)\n",
    "    feature_order = list(X.columns)\n",
    "\n",
    "    # base models (light tuning to keep fast)\n",
    "    gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "    gb_param = {\n",
    "        \"n_estimators\":[800,1200],\n",
    "        \"learning_rate\":[0.03,0.05],\n",
    "        \"max_depth\":[3,4],\n",
    "        \"subsample\":[0.85,1.0],\n",
    "        \"min_samples_split\":[2,5],\n",
    "        \"min_samples_leaf\":[1,2],\n",
    "    }\n",
    "    gb_best = RandomizedSearchCV(gb, gb_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                                 random_state=RANDOM_SEED).fit(X_imp,y).best_estimator_\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "    rf_param = {\n",
    "        \"n_estimators\":[800,1200],\n",
    "        \"max_depth\":[None,12,16],\n",
    "        \"min_samples_split\":[2,5],\n",
    "        \"min_samples_leaf\":[1,2],\n",
    "        \"max_features\":[\"sqrt\",0.8,1.0],\n",
    "    }\n",
    "    rf_best = RandomizedSearchCV(rf, rf_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                                 random_state=RANDOM_SEED).fit(X_imp,y).best_estimator_\n",
    "    base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "    # OOF stacking (no leakage)\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    oof = np.zeros((len(X_imp), len(base_models)))\n",
    "    for j,(nm, mdl) in enumerate(base_models):\n",
    "        fold_preds = np.zeros(len(X_imp))\n",
    "        for tr_idx, val_idx in kf.split(X_imp):\n",
    "            mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "            fold_preds[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "        oof[:,j] = fold_preds\n",
    "\n",
    "    sc_meta = StandardScaler()\n",
    "    Z = sc_meta.fit_transform(oof)\n",
    "    ridge_meta = RidgeCV(alphas=np.logspace(-3, 2, 30)).fit(Z, y)\n",
    "\n",
    "    # OOF metrics\n",
    "    y_oof = ridge_meta.predict(Z)\n",
    "    oof_r2, oof_mae, oof_rmse = r2_score(y,y_oof), mean_absolute_error(y,y_oof), rmse(y,y_oof)\n",
    "\n",
    "    # Refit bases on ALL rows => predict ALL rows\n",
    "    full_preds = []\n",
    "    for nm, mdl in base_models:\n",
    "        mdl.fit(X_imp, y)\n",
    "        full_preds.append(mdl.predict(X_imp))\n",
    "    stack_inputs_std = sc_meta.transform(np.vstack(full_preds).T)\n",
    "    y_hat = ridge_meta.predict(stack_inputs_std)\n",
    "\n",
    "    # Full-fit metrics\n",
    "    full_r2, full_mae, full_rmse = r2_score(y,y_hat), mean_absolute_error(y,y_hat), rmse(y,y_hat)\n",
    "\n",
    "    # Save per-player predictions\n",
    "    players_out = pd.DataFrame({\n",
    "        \"Player\": names.values,\n",
    "        \"Actual_RB_Grade\": y.values,\n",
    "        \"Predicted_RB_Grade\": y_hat,\n",
    "        \"Error\": y_hat - y.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    players_out.to_csv(players_csv, index=False)\n",
    "\n",
    "    # Feature impact (meta-weighted)\n",
    "    gb_imp = pd.Series(gb_best.feature_importances_, index=feature_order)\n",
    "    rf_imp = pd.Series(rf_best.feature_importances_, index=feature_order)\n",
    "    gb_imp_n = gb_imp / (gb_imp.sum() + 1e-12)\n",
    "    rf_imp_n = rf_imp / (rf_imp.sum() + 1e-12)\n",
    "    meta_coef = pd.Series(ridge_meta.coef_, index=[nm for nm,_ in base_models])\n",
    "    meta_w = meta_coef / (meta_coef.abs().sum() + 1e-12)\n",
    "    combined = meta_w.get(\"gb\",0.0)*gb_imp_n + meta_w.get(\"rf\",0.0)*rf_imp_n\n",
    "    impact_df = pd.DataFrame({\n",
    "        \"Impact_MetaWeighted\": combined,\n",
    "        \"GB_Importance\": gb_imp_n,\n",
    "        \"RF_Importance\": rf_imp_n,\n",
    "    }).sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "    impact_df.to_csv(impact_csv, index=False)\n",
    "\n",
    "    return {\n",
    "        \"tag\": tag,\n",
    "        \"OOF_R2\": oof_r2, \"OOF_MAE\": oof_mae, \"OOF_RMSE\": oof_rmse,\n",
    "        \"Full_R2\": full_r2, \"Full_MAE\": full_mae, \"Full_RMSE\": full_rmse,\n",
    "        \"players_csv\": str(players_csv),\n",
    "        \"impact_csv\": str(impact_csv),\n",
    "    }\n",
    "\n",
    "# ---------- load base table once ----------\n",
    "X_base, y, names = load_base_table(CSV_PATH)\n",
    "\n",
    "# ---------- run baseline (all interactions) ----------\n",
    "X_all = add_interactions(X_base, ALL_INTERACTIONS)\n",
    "baseline = train_stack_ridge_allrows(X_all, y, names, tag=\"baseline_all_interactions\")\n",
    "\n",
    "# ---------- ablations: remove one interaction at a time ----------\n",
    "results = [baseline]\n",
    "for rem in ALL_INTERACTIONS:\n",
    "    interactions = [x for x in ALL_INTERACTIONS if x != rem]\n",
    "    X_ab = add_interactions(X_base, interactions)\n",
    "    res = train_stack_ridge_allrows(X_ab, y, names, tag=f\"minus_{rem}\")\n",
    "    results.append(res)\n",
    "\n",
    "# also try \"no interactions\"\n",
    "X_none = add_interactions(X_base, [])\n",
    "res_none = train_stack_ridge_allrows(X_none, y, names, tag=\"no_interactions\")\n",
    "results.append(res_none)\n",
    "\n",
    "# ---------- summarize ----------\n",
    "tab = pd.DataFrame(results)\n",
    "tab[\"ΔOOF_R2_vsBaseline\"] = tab[\"OOF_R2\"] - float(baseline[\"OOF_R2\"])\n",
    "tab[\"ΔFull_R2_vsBaseline\"] = tab[\"Full_R2\"] - float(baseline[\"Full_R2\"])\n",
    "\n",
    "ranked = (tab\n",
    "          .sort_values([\"ΔOOF_R2_vsBaseline\",\"OOF_R2\"], ascending=[True, False])\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"\\n=== Interaction Ablation — Ranked by ΔOOF_R2 (higher is better; negative means worse than baseline) ===\")\n",
    "display(ranked[[\"tag\",\"OOF_R2\",\"ΔOOF_R2_vsBaseline\",\"Full_R2\",\"ΔFull_R2_vsBaseline\",\"players_csv\",\"impact_csv\"]]\n",
    "        .round(4))\n",
    "\n",
    "print(\"\\nOpen these CSVs to inspect predictions & feature impacts for any run (paths shown above).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc20e17-3b05-454a-baa5-3f86166be5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Ablation: does DOMxDraft add value? (Now includes Draft Age)\n",
    "# Trains Stack Ridge Meta twice on ALL rows:\n",
    "#   (A) with interactions: [\"BMIx40\",\"ELUxYCOA\",\"DOMxDraft\",\"YPCxELU\"]\n",
    "#   (B) same but WITHOUT \"DOMxDraft\"\n",
    "# Adds \"Draft Age\" as a feature (younger is better → inverted in preprocessing).\n",
    "# Saves metrics, per-player predictions, and feature impacts for both runs.\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "N_SPLITS    = 5\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BAMA\",\"BamaAdj\"],\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    # NEW: Draft Age (younger is better -> will be inverted)\n",
    "    \"Draft Age\":    [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"]\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true,y_pred): return float(np.sqrt(mean_squared_error(y_true,y_pred)))\n",
    "\n",
    "def load_base_table(path: Path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    y_col = find_col(df, TARGET_CANDS)\n",
    "    name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "    mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "    mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "    X = pd.DataFrame({feat: to_num(df[col]) for feat,col in mapped.items()})\n",
    "    y = to_num(df[y_col])\n",
    "    names = df[name_col].astype(str).fillna(\"\")\n",
    "    mask = y.notna()\n",
    "    return X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True), names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "def add_interactions(X: pd.DataFrame, interactions: list) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\" BEFORE interactions\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = -X[c]  # faster 40, earlier round, quicker shuttle/3-cone, YOUNGER age => larger is better after invert\n",
    "    def mul(a,b,n):\n",
    "        if a in X.columns and b in X.columns:\n",
    "            X[n] = X[a]*X[b]\n",
    "    if \"BMIx40\" in interactions:\n",
    "        mul(\"BMI\",\"40 Time\",\"BMIx40\")\n",
    "    if \"ELUxYCOA\" in interactions:\n",
    "        mul(\"ELU\",\"YCO/A\",\"ELUxYCOA\")\n",
    "    if \"DOMxDraft\" in interactions:\n",
    "        mul(\"DOM++\",\"Draft Capital\",\"DOMxDraft\")\n",
    "    if \"YPCxELU\" in interactions:\n",
    "        mul(\"YPC\",\"ELU\",\"YPCxELU\")\n",
    "    return X\n",
    "\n",
    "def train_stack_ridge_allrows(X: pd.DataFrame, y: pd.Series, names: pd.Series, tag: str):\n",
    "    \"\"\"\n",
    "    Trains GB+RF bases + Ridge meta on ALL rows.\n",
    "    Uses OOF predictions for meta training to report honest OOF metrics.\n",
    "    Then refits bases on ALL rows and predicts everyone.\n",
    "    Saves per-player CSV and feature-impact CSV with suffix `tag`.\n",
    "    Returns metrics and path info.\n",
    "    \"\"\"\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    players_csv = OUT_DIR / f\"stack_ridge_all_players_{tag}.csv\"\n",
    "    impact_csv  = OUT_DIR / f\"stack_ridge_feature_impact_{tag}.csv\"\n",
    "\n",
    "    # impute\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    X_imp = imp.fit_transform(X)\n",
    "    feature_order = list(X.columns)\n",
    "\n",
    "    # base models (light but solid tuning)\n",
    "    gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "    gb_param = {\n",
    "        \"n_estimators\":[800,1200],\n",
    "        \"learning_rate\":[0.03,0.05],\n",
    "        \"max_depth\":[3,4],\n",
    "        \"subsample\":[0.85,1.0],\n",
    "        \"min_samples_split\":[2,5],\n",
    "        \"min_samples_leaf\":[1,2],\n",
    "    }\n",
    "    gb_best = RandomizedSearchCV(gb, gb_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                                 random_state=RANDOM_SEED).fit(X_imp,y).best_estimator_\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "    rf_param = {\n",
    "        \"n_estimators\":[800,1200],\n",
    "        \"max_depth\":[None,12,16],\n",
    "        \"min_samples_split\":[2,5],\n",
    "        \"min_samples_leaf\":[1,2],\n",
    "        \"max_features\":[\"sqrt\",0.8,1.0],\n",
    "    }\n",
    "    rf_best = RandomizedSearchCV(rf, rf_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                                 random_state=RANDOM_SEED).fit(X_imp,y).best_estimator_\n",
    "\n",
    "    base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "    # OOF stacking (no leakage)\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    oof = np.zeros((len(X_imp), len(base_models)))\n",
    "    for j,(nm, mdl) in enumerate(base_models):\n",
    "        fold_preds = np.zeros(len(X_imp))\n",
    "        for tr_idx, val_idx in kf.split(X_imp):\n",
    "            mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "            fold_preds[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "        oof[:,j] = fold_preds\n",
    "\n",
    "    sc_meta = StandardScaler()\n",
    "    Z = sc_meta.fit_transform(oof)\n",
    "    ridge_meta = RidgeCV(alphas=np.logspace(-3,2,30)).fit(Z, y)\n",
    "\n",
    "    # OOF metrics\n",
    "    y_oof = ridge_meta.predict(Z)\n",
    "    oof_r2, oof_mae, oof_rmse = r2_score(y,y_oof), mean_absolute_error(y,y_oof), rmse(y,y_oof)\n",
    "\n",
    "    # Refit bases on ALL rows, predict ALL rows\n",
    "    base_full_preds = []\n",
    "    for nm, mdl in base_models:\n",
    "        mdl.fit(X_imp, y)\n",
    "        base_full_preds.append(mdl.predict(X_imp))\n",
    "    stack_inputs_std = sc_meta.transform(np.vstack(base_full_preds).T)\n",
    "    y_hat = ridge_meta.predict(stack_inputs_std)\n",
    "\n",
    "    # Full-fit metrics (in-sample)\n",
    "    full_r2, full_mae, full_rmse = r2_score(y,y_hat), mean_absolute_error(y,y_hat), rmse(y,y_hat)\n",
    "\n",
    "    # Save per-player predictions\n",
    "    players_out = pd.DataFrame({\n",
    "        \"Player\": names.values,\n",
    "        \"Actual_RB_Grade\": y.values,\n",
    "        \"Predicted_RB_Grade\": y_hat,\n",
    "        \"Error\": y_hat - y.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    players_out.to_csv(players_csv, index=False)\n",
    "\n",
    "    # Feature impact (meta-weighted)\n",
    "    gb_imp = pd.Series(gb_best.feature_importances_, index=feature_order)\n",
    "    rf_imp = pd.Series(rf_best.feature_importances_, index=feature_order)\n",
    "    gb_imp_n = gb_imp / (gb_imp.sum() + 1e-12)\n",
    "    rf_imp_n = rf_imp / (rf_imp.sum() + 1e-12)\n",
    "    meta_coef = pd.Series(ridge_meta.coef_, index=[nm for nm,_ in base_models])\n",
    "    meta_w = meta_coef / (meta_coef.abs().sum() + 1e-12)\n",
    "    combined = meta_w.get(\"gb\",0.0)*gb_imp_n + meta_w.get(\"rf\",0.0)*rf_imp_n\n",
    "    impact_df = pd.DataFrame({\n",
    "        \"Impact_MetaWeighted\": combined,\n",
    "        \"GB_Importance\": gb_imp_n,\n",
    "        \"RF_Importance\": rf_imp_n,\n",
    "    }).sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "    impact_df.to_csv(impact_csv, index=False)\n",
    "\n",
    "    metrics = {\n",
    "        \"tag\": tag,\n",
    "        \"OOF_R2\": oof_r2, \"OOF_MAE\": oof_mae, \"OOF_RMSE\": oof_rmse,\n",
    "        \"Full_R2\": full_r2, \"Full_MAE\": full_mae, \"Full_RMSE\": full_rmse,\n",
    "        \"players_csv\": str(players_csv),\n",
    "        \"impact_csv\": str(impact_csv),\n",
    "    }\n",
    "    return metrics, players_out.head(8), impact_df.head(12)\n",
    "\n",
    "# ===========================\n",
    "# Run ablation\n",
    "# ===========================\n",
    "X_base, y, names = load_base_table(CSV_PATH)\n",
    "\n",
    "interactions_with = [\"BMIx40\",\"ELUxYCOA\",\"DOMxDraft\",\"YPCxELU\"]\n",
    "interactions_wo   = [\"BMIx40\",\"ELUxYCOA\",\"YPCxELU\"]  # DOMxDraft removed\n",
    "\n",
    "X_with = add_interactions(X_base, interactions_with)\n",
    "X_wo   = add_interactions(X_base, interactions_wo)\n",
    "\n",
    "m_with, sample_players_with, top_feats_with = train_stack_ridge_allrows(X_with, y, names, tag=\"with_domxdraft_plus_draftage\")\n",
    "m_wo,   sample_players_wo,   top_feats_wo   = train_stack_ridge_allrows(X_wo,   y, names, tag=\"no_domxdraft_plus_draftage\")\n",
    "\n",
    "# Compare\n",
    "summary = pd.DataFrame([m_with, m_wo]).drop(columns=[\"players_csv\",\"impact_csv\"])\n",
    "delta = summary.set_index(\"tag\")\n",
    "print(\"\\n=== Ablation Summary (OOF = generalization estimate) ===\")\n",
    "print(delta[[\"OOF_R2\",\"OOF_MAE\",\"OOF_RMSE\",\"Full_R2\",\"Full_MAE\",\"Full_RMSE\"]].round(4))\n",
    "\n",
    "print(\"\\nTop players (with DOMxDraft):\")\n",
    "print(sample_players_with.round(3))\n",
    "print(\"\\nTop players (no DOMxDraft):\")\n",
    "print(sample_players_wo.round(3))\n",
    "\n",
    "print(\"\\nTop features by impact (with DOMxDraft):\")\n",
    "print(top_feats_with.round(4))\n",
    "print(\"\\nTop features by impact (no DOMxDraft):\")\n",
    "print(top_feats_wo.round(4))\n",
    "\n",
    "print(f\"\\nPer-player CSVs:\\n  {m_with['players_csv']}\\n  {m_wo['players_csv']}\")\n",
    "print(f\"Feature impact CSVs:\\n  {m_with['impact_csv']}\\n  {m_wo['impact_csv']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df040483-74c7-4d88-897e-973456041693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Full-dataset application + metrics + feature impact (Stack Ridge Meta)\n",
    "# Includes Draft Age feature (younger is better -> inverted)\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "PLAYERS_CSV = OUT_DIR / \"stack_ridge_all_players.csv\"\n",
    "IMPACT_CSV  = OUT_DIR / \"stack_ridge_feature_impact.csv\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "N_SPLITS    = 5  # for OOF stacking\n",
    "\n",
    "# canonical features (no Breakout Age) + Draft Age\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    # NEW: Draft Age (younger is better -> will be inverted)\n",
    "    \"Draft Age\":    [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"]\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\" — faster times, earlier draft round, younger draft age\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns: \n",
    "            X[c] = -X[c]\n",
    "    # a few high-signal interactions (unchanged)\n",
    "    if \"BMI\" in X and \"40 Time\" in X: X[\"BMIx40\"] = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X: X[\"ELUxYCOA\"] = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X: X[\"YPCxELU\"] = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "X0 = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y  = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "# drop rows with missing target\n",
    "mask = y.notna()\n",
    "X0, y, names = X0.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True), names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# interactions (+ invert step)\n",
    "X = basic_interactions(X0)\n",
    "\n",
    "# impute full matrix (for later refit + predictions)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X)\n",
    "feature_order = list(X.columns)\n",
    "\n",
    "# ---------- base models (tuned lightly) ----------\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = { \"n_estimators\":[800,1200], \"learning_rate\":[0.03,0.05], \"max_depth\":[3,4], \"subsample\":[0.85,1.0],\n",
    "             \"min_samples_split\":[2,5], \"min_samples_leaf\":[1,2] }\n",
    "gb_best = RandomizedSearchCV(gb, gb_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                             random_state=RANDOM_SEED).fit(X_imp, y).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "rf_param = { \"n_estimators\":[800,1200], \"max_depth\":[None,12,16], \"min_samples_split\":[2,5], \"min_samples_leaf\":[1,2],\n",
    "             \"max_features\":[\"sqrt\",0.8,1.0] }\n",
    "rf_best = RandomizedSearchCV(rf, rf_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=N_JOBS,\n",
    "                             random_state=RANDOM_SEED).fit(X_imp, y).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "# ---------- OOF stacking for honest metrics ----------\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_imp), len(base_models)))\n",
    "for j,(nm, mdl) in enumerate(base_models):\n",
    "    fold_preds = np.zeros(len(X_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_imp):\n",
    "        mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "        fold_preds[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "    oof[:, j] = fold_preds\n",
    "\n",
    "# meta on standardized OOF\n",
    "sc_meta = StandardScaler()\n",
    "Z = sc_meta.fit_transform(oof)\n",
    "ridge_meta = RidgeCV(alphas=np.logspace(-3, 2, 30)).fit(Z, y)\n",
    "\n",
    "# OOF metrics (generalization estimate)\n",
    "y_oof_hat = ridge_meta.predict(Z)\n",
    "oof_r2  = r2_score(y, y_oof_hat)\n",
    "oof_mae = mean_absolute_error(y, y_oof_hat)\n",
    "oof_rmse= rmse(y, y_oof_hat)\n",
    "\n",
    "# ---------- full refit on all data, then predict all rows ----------\n",
    "# refit bases on ALL data\n",
    "base_preds_full = []\n",
    "for nm, mdl in base_models:\n",
    "    mdl.fit(X_imp, y)\n",
    "    base_preds_full.append(mdl.predict(X_imp))\n",
    "stack_inputs = np.vstack(base_preds_full).T\n",
    "stack_inputs_std = sc_meta.transform(stack_inputs)  # use same scaler as OOF\n",
    "y_full_hat = ridge_meta.predict(stack_inputs_std)\n",
    "\n",
    "# full fit metrics (in-sample)\n",
    "full_r2  = r2_score(y, y_full_hat)\n",
    "full_mae = mean_absolute_error(y, y_full_hat)\n",
    "full_rmse= rmse(y, y_full_hat)\n",
    "\n",
    "# ---------- export per-player predictions ----------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "players_out = pd.DataFrame({\n",
    "    \"Player\": names.values,\n",
    "    \"Actual_RB_Grade\": y.values,\n",
    "    \"Predicted_RB_Grade\": y_full_hat,\n",
    "    \"Error\": (y_full_hat - y.values)\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "players_out.to_csv(PLAYERS_CSV, index=False)\n",
    "\n",
    "# ---------- feature impact ----------\n",
    "# 1) raw importances from bases\n",
    "gb_imp = pd.Series(gb_best.feature_importances_, index=feature_order)\n",
    "rf_imp = pd.Series(rf_best.feature_importances_, index=feature_order)\n",
    "\n",
    "# 2) normalize each to sum to 1 (avoid bias from scale)\n",
    "gb_imp_n = gb_imp / (gb_imp.sum() + 1e-12)\n",
    "rf_imp_n = rf_imp / (rf_imp.sum() + 1e-12)\n",
    "\n",
    "# 3) meta weights from Ridge on standardized base predictions\n",
    "meta_coef = pd.Series(ridge_meta.coef_, index=[nm for nm,_ in base_models])\n",
    "# Allow signs but normalize by L1 to represent relative influence\n",
    "meta_w = meta_coef / (meta_coef.abs().sum() + 1e-12)\n",
    "\n",
    "# 4) combine: weighted sum of normalized importances\n",
    "combined = meta_w.get(\"gb\",0.0)*gb_imp_n + meta_w.get(\"rf\",0.0)*rf_imp_n\n",
    "impact_df = pd.DataFrame({\n",
    "    \"Impact_MetaWeighted\": combined,\n",
    "    \"GB_Importance\": gb_imp_n,\n",
    "    \"RF_Importance\": rf_imp_n,\n",
    "}).sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "impact_df.to_csv(IMPACT_CSV, index=False)\n",
    "\n",
    "# ---------- print summary ----------\n",
    "print(\"\\n=== Stack Ridge Meta — Metrics (with Draft Age) ===\")\n",
    "print(f\"OOF R²:   {oof_r2:.4f}   (generalization estimate)\")\n",
    "print(f\"OOF MAE:  {oof_mae:.4f}\")\n",
    "print(f\"OOF RMSE: {oof_rmse:.4f}\")\n",
    "print(f\"\\nFull Fit R²:   {full_r2:.4f}   (in-sample)\")\n",
    "print(f\"Full Fit MAE:  {full_mae:.4f}\")\n",
    "print(f\"Full Fit RMSE: {full_rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nSaved per-player predictions → {PLAYERS_CSV}\")\n",
    "print(f\"Saved feature impact table  → {IMPACT_CSV}\")\n",
    "\n",
    "# Show top 15 players + top 15 features\n",
    "print(\"\\nTop players by Actual RB Grade:\")\n",
    "print(players_out.head(15).round(3))\n",
    "\n",
    "print(\"\\nTop features by meta-weighted impact:\")\n",
    "print(impact_df.head(15).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43708520-d13c-4ce8-85a8-10b19ac239c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Tightened Stack Meta (with Draft Age, isotonic calibration, NNLS meta)\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RepeatedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "from sklearn.linear_model import RidgeCV  # kept for feature impact ref\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "PLAYERS_CSV = OUT_DIR / \"stack_ridge_all_players_tight.csv\"\n",
    "IMPACT_CSV  = OUT_DIR / \"stack_ridge_feature_impact_tight.csv\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "N_SPLITS    = 5\n",
    "N_REPEATS   = 3   # more stable OOF\n",
    "N_ITER      = 30  # random search iterations per base\n",
    "\n",
    "# canonical features + Draft Age\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BAMA\",\"BamaAdj\"],\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":    [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def winsorize_df(X: pd.DataFrame, lower=0.01, upper=0.99):\n",
    "    X = X.copy()\n",
    "    qs = X.quantile([lower, upper])\n",
    "    for c in X.columns:\n",
    "        lo, hi = qs.loc[lower, c], qs.loc[upper, c]\n",
    "        if np.isfinite(lo) and np.isfinite(hi):\n",
    "            X[c] = X[c].clip(lo, hi)\n",
    "    return X\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\"\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    # high-signal interactions\n",
    "    if \"BMI\" in X and \"40 Time\" in X: X[\"BMIx40\"] = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X: X[\"ELUxYCOA\"] = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X: X[\"YPCxELU\"] = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "# ---------- load & build feature matrix ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "X_raw = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y     = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y.notna()\n",
    "X_raw, y, names = X_raw.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True), names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# filter sparse/constant features\n",
    "keep = [c for c in X_raw.columns if X_raw[c].notna().sum() >= max(10, int(0.25*len(X_raw))) and X_raw[c].nunique(dropna=True) > 2]\n",
    "X_raw = X_raw[keep]\n",
    "\n",
    "# winsorize to reduce outlier drag\n",
    "X_raw = winsorize_df(X_raw, 0.01, 0.99)\n",
    "\n",
    "# add interactions + invert step\n",
    "X = basic_interactions(X_raw)\n",
    "\n",
    "# impute\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X)\n",
    "feature_order = list(X.columns)\n",
    "\n",
    "# ---------- build tuned base models ----------\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = {\n",
    "    \"n_estimators\":   rng.randint(600, 1400, N_ITER),\n",
    "    \"learning_rate\":  rng.uniform(0.02, 0.08, N_ITER),\n",
    "    \"max_depth\":      rng.randint(2, 5, N_ITER),\n",
    "    \"subsample\":      rng.uniform(0.75, 1.0, N_ITER),\n",
    "    \"min_samples_split\": rng.randint(2, 8, N_ITER),\n",
    "    \"min_samples_leaf\":  rng.randint(1, 4, N_ITER),\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "rf_param = {\n",
    "    \"n_estimators\":   rng.randint(700, 1400, N_ITER),\n",
    "    \"max_depth\":      list(np.random.choice([None, 10, 12, 16, 20], N_ITER)),\n",
    "    \"min_samples_split\": rng.randint(2, 8, N_ITER),\n",
    "    \"min_samples_leaf\":  rng.randint(1, 4, N_ITER),\n",
    "    \"max_features\":   list(np.random.choice([\"sqrt\", 0.6, 0.8, 1.0], N_ITER)),\n",
    "}\n",
    "\n",
    "et = ExtraTreesRegressor(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "et_param = {\n",
    "    \"n_estimators\":   rng.randint(700, 1400, N_ITER),\n",
    "    \"max_depth\":      list(np.random.choice([None, 12, 16, 20], N_ITER)),\n",
    "    \"min_samples_split\": rng.randint(2, 8, N_ITER),\n",
    "    \"min_samples_leaf\":  rng.randint(1, 4, N_ITER),\n",
    "    \"max_features\":   list(np.random.choice([\"sqrt\", 0.6, 0.8, 1.0], N_ITER)),\n",
    "}\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "hgb_param = {\n",
    "    \"learning_rate\":  rng.uniform(0.02, 0.1, N_ITER),\n",
    "    \"max_depth\":      rng.randint(2, 10, N_ITER),\n",
    "    \"max_bins\":       rng.randint(128, 255, N_ITER),\n",
    "    \"l2_regularization\": rng.uniform(0.0, 1.0, N_ITER),\n",
    "}\n",
    "\n",
    "def fit_best(base, param_dist):\n",
    "    search = RandomizedSearchCV(base, param_distributions=param_dist, n_iter=N_ITER,\n",
    "                                scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=RANDOM_SEED, verbose=0)\n",
    "    return search.fit(X_imp, y).best_estimator_\n",
    "\n",
    "gb_best  = fit_best(gb,  gb_param)\n",
    "rf_best  = fit_best(rf,  rf_param)\n",
    "et_best  = fit_best(et,  et_param)\n",
    "hgb_best = fit_best(hgb, hgb_param)\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"hgb\", hgb_best), (\"rf\", rf_best), (\"et\", et_best)]\n",
    "\n",
    "# ---------- OOF predictions with RepeatedKFold ----------\n",
    "rkf = RepeatedKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_imp), len(base_models)))\n",
    "\n",
    "for j,(nm, mdl) in enumerate(base_models):\n",
    "    fold_pred = np.zeros(len(X_imp))\n",
    "    # average multiple OOF passes per sample across repeats/folds\n",
    "    counts = np.zeros(len(X_imp))\n",
    "    for tr_idx, val_idx in rkf.split(X_imp):\n",
    "        mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "        p = mdl.predict(X_imp[val_idx])\n",
    "        fold_pred[val_idx] += p\n",
    "        counts[val_idx] += 1\n",
    "    fold_pred = np.divide(fold_pred, counts, out=np.zeros_like(fold_pred), where=counts>0)\n",
    "    oof[:, j] = fold_pred\n",
    "\n",
    "# ---------- non-negative meta (NNLS) on OOF + isotonic calibration ----------\n",
    "# standardize OOF before NNLS? For NNLS we keep raw scale and let it pick non-negative weights\n",
    "w_nnls, _ = nnls(oof, y.values)\n",
    "stack_oof = oof @ w_nnls\n",
    "\n",
    "# calibrate with isotonic regression\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "stack_oof_cal = iso.fit_transform(stack_oof, y.values)\n",
    "\n",
    "# OOF metrics\n",
    "oof_r2   = r2_score(y, stack_oof_cal)\n",
    "oof_mae  = mean_absolute_error(y, stack_oof_cal)\n",
    "oof_rmse = rmse(y, stack_oof_cal)\n",
    "\n",
    "# ---------- refit bases on ALL rows + produce calibrated predictions ----------\n",
    "base_full_preds = []\n",
    "for nm, mdl in base_models:\n",
    "    mdl.fit(X_imp, y)\n",
    "    base_full_preds.append(mdl.predict(X_imp))\n",
    "base_full_preds = np.vstack(base_full_preds).T\n",
    "\n",
    "y_full_raw = base_full_preds @ w_nnls\n",
    "y_full_hat = iso.transform(y_full_raw)  # calibrated\n",
    "\n",
    "# full fit (in-sample) metrics\n",
    "full_r2   = r2_score(y, y_full_hat)\n",
    "full_mae  = mean_absolute_error(y, y_full_hat)\n",
    "full_rmse = rmse(y, y_full_hat)\n",
    "\n",
    "# ---------- export per-player predictions ----------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "players_out = pd.DataFrame({\n",
    "    \"Player\": names.values,\n",
    "    \"Actual_RB_Grade\": y.values,\n",
    "    \"Predicted_RB_Grade\": y_full_hat,\n",
    "    \"Error\": (y_full_hat - y.values)\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "players_out.to_csv(PLAYERS_CSV, index=False)\n",
    "\n",
    "# ---------- feature impact (meta-weighted) ----------\n",
    "# compute normalized importances for each base\n",
    "def norm_imp(estimator, feats):\n",
    "    if hasattr(estimator, \"feature_importances_\"):\n",
    "        s = pd.Series(estimator.feature_importances_, index=feats)\n",
    "    elif hasattr(estimator, \"feature_names_in_\"):\n",
    "        # HistGB exposes no permutation by default; fallback to zero vec to avoid misreport\n",
    "        s = pd.Series(np.zeros(len(feats)), index=feats)\n",
    "    else:\n",
    "        s = pd.Series(np.zeros(len(feats)), index=feats)\n",
    "    s = s / (s.sum() + 1e-12)\n",
    "    return s\n",
    "\n",
    "imps = [norm_imp(m, feature_order) for _,m in base_models]\n",
    "\n",
    "# convert meta weights to convex weights (non-negative, L1 normalized)\n",
    "meta_w = w_nnls / (np.sum(w_nnls) + 1e-12)\n",
    "\n",
    "# blend importances\n",
    "combined = sum(meta_w[j] * imps[j] for j in range(len(imps)))\n",
    "impact_df = pd.DataFrame({\"Impact_MetaWeighted\": combined})\n",
    "for j,(nm,_) in enumerate(base_models):\n",
    "    impact_df[f\"{nm.upper()}_Importance\"] = imps[j]\n",
    "impact_df = impact_df.sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "impact_df.to_csv(IMPACT_CSV, index=False)\n",
    "\n",
    "# ---------- summary ----------\n",
    "print(\"\\n=== Tightened Stack Meta — Metrics (Draft Age included) ===\")\n",
    "print(f\"OOF R²:   {oof_r2:.4f}\")\n",
    "print(f\"OOF MAE:  {oof_mae:.4f}\")\n",
    "print(f\"OOF RMSE: {oof_rmse:.4f}\")\n",
    "print(f\"\\nFull Fit R²:   {full_r2:.4f}\")\n",
    "print(f\"Full Fit MAE:  {full_mae:.4f}\")\n",
    "print(f\"Full Fit RMSE: {full_rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nSaved per-player predictions → {PLAYERS_CSV}\")\n",
    "print(f\"Saved feature impact table  → {IMPACT_CSV}\")\n",
    "\n",
    "print(\"\\nTop players by Actual RB Grade:\")\n",
    "print(players_out.head(15).round(3))\n",
    "\n",
    "print(\"\\nTop features by meta-weighted impact:\")\n",
    "print(impact_df.head(15).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8fb8c-0990-49ee-8a36-ee17bd7d7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Apply Stack Ridge Meta to entire dataset\n",
    "# Train on ALL rows, predict for ALL rows\n",
    "# Now includes Draft Age (younger is better -> inverted)\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_PATH    = Path(\"./data/Bakery/_derived/stack_ridge_all_players.csv\")\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "\n",
    "# canonical features (+ Draft Age; still no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    # NEW: Draft Age (younger is better -> will invert)\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\" (faster times, earlier rounds, younger age)\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = -X[c]\n",
    "    # a few high-signal interactions (unchanged from your baseline)\n",
    "    if \"BMI\" in X and \"40 Time\" in X:  X[\"BMIx40\"]   = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X:    X[\"ELUxYCOA\"] = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X:      X[\"YPCxELU\"]  = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col   = find_col(df, TARGET_CANDS)\n",
    "name_col= find_col(df, NAME_CANDS) or \"Player\"\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "X = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y.notna()\n",
    "X, y, names = X.loc[mask], y.loc[mask], names.loc[mask]\n",
    "\n",
    "# interactions (+ invert step)\n",
    "X = basic_interactions(X)\n",
    "\n",
    "# impute\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X)\n",
    "\n",
    "# ---------- train base models (with light tuning) ----------\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = {\"n_estimators\":[800], \"learning_rate\":[0.05], \"max_depth\":[4], \"subsample\":[0.85]}\n",
    "gb_best = RandomizedSearchCV(gb, gb_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS).fit(X_imp, y).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "rf_param = {\"n_estimators\":[800], \"max_depth\":[12], \"min_samples_split\":[2], \"min_samples_leaf\":[1]}\n",
    "rf_best = RandomizedSearchCV(rf, rf_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS).fit(X_imp, y).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "# ---------- stacking with OOF ----------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_imp), len(base_models)))\n",
    "for j,(nm, mdl) in enumerate(base_models):\n",
    "    fold_preds = np.zeros(len(X_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_imp):\n",
    "        mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "        fold_preds[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "    oof[:, j] = fold_preds\n",
    "\n",
    "# meta learner (Ridge on standardized OOF)\n",
    "sc_meta = StandardScaler()\n",
    "Z = sc_meta.fit_transform(oof)\n",
    "ridge_meta = RidgeCV(alphas=np.logspace(-3, 2, 30)).fit(Z, y)\n",
    "\n",
    "# retrain base models on full data\n",
    "full_preds = []\n",
    "for nm, mdl in base_models:\n",
    "    mdl.fit(X_imp, y)\n",
    "    full_preds.append(mdl.predict(X_imp))\n",
    "stack_inputs = np.vstack(full_preds).T\n",
    "stack_inputs = sc_meta.transform(stack_inputs)\n",
    "\n",
    "y_hat = ridge_meta.predict(stack_inputs)\n",
    "\n",
    "# ---------- results ----------\n",
    "out = pd.DataFrame({\n",
    "    \"Player\": names.values,\n",
    "    \"Actual_RB_Grade\": y.values,\n",
    "    \"Predicted_RB_Grade\": y_hat,\n",
    "    \"Error\": y_hat - y.values\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Full Dataset Results (Stack Ridge Meta + Draft Age) ===\")\n",
    "print(out.head(20).round(3))\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\nSaved → {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c8c97-4979-43a9-b23b-fcd105ee3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Apply Stack Ridge Meta to entire dataset\n",
    "# Train on ALL rows, predict for ALL rows\n",
    "# Draft Age is INCLUDED in the STACK META features (not just in bases)\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_PATH    = Path(\"./data/Bakery/_derived/stack_ridge_all_players.csv\")\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "\n",
    "# canonical features (+ Draft Age; still no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    # Draft Age (younger is better -> will invert)\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\" (faster times, earlier rounds, younger age)\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = -X[c]\n",
    "    # baseline interactions\n",
    "    if \"BMI\" in X and \"40 Time\" in X:        X[\"BMIx40\"]   = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X:          X[\"ELUxYCOA\"] = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X:            X[\"YPCxELU\"]  = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "X = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y.notna()\n",
    "X, y, names = X.loc[mask], y.loc[mask], names.loc[mask]\n",
    "\n",
    "# interactions (+ invert step)\n",
    "X = basic_interactions(X)\n",
    "\n",
    "# impute\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X)\n",
    "feature_order = list(X.columns)\n",
    "\n",
    "# keep track of Draft Age column index (post inversion)\n",
    "draft_age_idx = feature_order.index(\"Draft Age\") if \"Draft Age\" in feature_order else None\n",
    "\n",
    "# ---------- train base models (with light tuning) ----------\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = {\"n_estimators\":[800], \"learning_rate\":[0.05], \"max_depth\":[4], \"subsample\":[0.85]}\n",
    "gb_best = RandomizedSearchCV(gb, gb_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS).fit(X_imp, y).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "rf_param = {\"n_estimators\":[800], \"max_depth\":[12], \"min_samples_split\":[2], \"min_samples_leaf\":[1]}\n",
    "rf_best = RandomizedSearchCV(rf, rf_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS).fit(X_imp, y).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "# ---------- stacking with OOF ----------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_imp), len(base_models)))\n",
    "for j,(nm, mdl) in enumerate(base_models):\n",
    "    fold_preds = np.zeros(len(X_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_imp):\n",
    "        mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "        fold_preds[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "    oof[:, j] = fold_preds\n",
    "\n",
    "# === META INPUTS: OOF preds + Draft Age (imputed & standardized) ===\n",
    "if draft_age_idx is not None:\n",
    "    draft_age_meta = X_imp[:, draft_age_idx].reshape(-1, 1)\n",
    "    sc_meta = StandardScaler()\n",
    "    Z = np.hstack([oof, sc_meta.fit_transform(draft_age_meta)])\n",
    "else:\n",
    "    sc_meta = StandardScaler()\n",
    "    Z = sc_meta.fit_transform(oof)\n",
    "\n",
    "# meta learner (Ridge on meta inputs above)\n",
    "ridge_meta = RidgeCV(alphas=np.logspace(-3, 2, 30)).fit(Z, y)\n",
    "\n",
    "# retrain base models on full data\n",
    "full_preds = []\n",
    "for nm, mdl in base_models:\n",
    "    mdl.fit(X_imp, y)\n",
    "    full_preds.append(mdl.predict(X_imp))\n",
    "stack_inputs = np.vstack(full_preds).T\n",
    "\n",
    "# add Draft Age to meta at predict-time too (use same scaler)\n",
    "if draft_age_idx is not None:\n",
    "    draft_age_meta_full = X_imp[:, draft_age_idx].reshape(-1, 1)\n",
    "    stack_inputs_meta = np.hstack([stack_inputs, sc_meta.transform(draft_age_meta_full)])\n",
    "else:\n",
    "    stack_inputs_meta = sc_meta.transform(stack_inputs)\n",
    "\n",
    "y_hat = ridge_meta.predict(stack_inputs_meta)\n",
    "\n",
    "# ---------- results ----------\n",
    "out = pd.DataFrame({\n",
    "    \"Player\": names.values,\n",
    "    \"Actual_RB_Grade\": y.values,\n",
    "    \"Predicted_RB_Grade\": y_hat,\n",
    "    \"Error\": y_hat - y.values\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Full Dataset Results (Stack Ridge Meta + Draft Age in META) ===\")\n",
    "print(out.head(20).round(3))\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\nSaved → {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a34c10-2902-491a-9c7e-46343f7f3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Reverse-engineer Bakery RB Grade from Bakery_RB_Overall.csv (non-negative weights, no Breakout Age) =====\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "ROOT = CSV_PATH.parent\n",
    "OUT_DIR = Path(\"./data/Bakery/_derived\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\", \"\", c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\", \"\", cand).lower()\n",
    "        if key in norm:\n",
    "            return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "# canonical features to look for (NO Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":        [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":      [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "    \"BMI\":          [\"BMI\"],\n",
    "    \"YPC\":          [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":          [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":        [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":       [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\":[\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":         [\"Bama\",\"Bama Rating\",\"BamaAdj\"],\n",
    "    # optional extras if present\n",
    "    \"Shuttle\":      [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":   [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Vertical\":     [\"Vertical\",\"Vertical Jump\"],\n",
    "    \"Broad\":        [\"Broad\",\"Broad Jump\"],\n",
    "    \"Speed Score\":  [\"Speed Score\",\"SpeedScore\"],\n",
    "    \"Rec Yards\":    [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Targets\":      [\"Targets\",\"Target Share\",\"Tgt%\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "\n",
    "# ---------- load ----------\n",
    "if not CSV_PATH.exists():\n",
    "    # fall back to any similarly named overall file\n",
    "    candidates = list(ROOT.glob(\"Bakery_RB_Overall*.csv\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"Could not find {CSV_PATH} or any Bakery_RB_Overall*.csv under {ROOT}\")\n",
    "    CSV_PATH = candidates[0]\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "print(\"Loaded:\", CSV_PATH)\n",
    "print(\"Rows x Cols:\", df.shape)\n",
    "\n",
    "# ---------- map target + features ----------\n",
    "y_col = find_col(df, TARGET_CANDS)\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Could not find RB Grade in columns:\\n{df.columns.tolist()}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found. Inspect df.columns for header names.\")\n",
    "\n",
    "print(\"\\nUsing features (canonical <- sheet column):\")\n",
    "for k,v in mapped.items():\n",
    "    print(f\"  {k:<12} <- {v}\")\n",
    "\n",
    "X_raw = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_raw = to_num(df[y_col])\n",
    "\n",
    "# ---------- drop rows with NaN TARGET ----------\n",
    "mask = y_raw.notna()\n",
    "dropped = len(y_raw) - mask.sum()\n",
    "if dropped:\n",
    "    print(f\"\\nDropped {dropped} rows with NaN RB Grade.\")\n",
    "X_raw = X_raw.loc[mask].reset_index(drop=True)\n",
    "y = y_raw.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# ---------- keep columns with enough data (loose thresholds for real-world sheets) ----------\n",
    "keep = [c for c in X_raw.columns if X_raw[c].notna().sum() >= 5 and X_raw[c].nunique(dropna=True) > 1]\n",
    "if not keep:\n",
    "    raise ValueError(\"All candidate features are too sparse/constant. \"\n",
    "                     \"Relax thresholds or ensure the Overall file has those columns filled.\")\n",
    "X_raw = X_raw[keep]\n",
    "print(\"Kept features:\", keep)\n",
    "\n",
    "# ---------- invert where lower is better (NO Breakout Age) ----------\n",
    "for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\"]:\n",
    "    if c in X_raw.columns:\n",
    "        X_raw[c] = -X_raw[c]\n",
    "\n",
    "# ---------- impute X (median) + standardize ----------\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = imp.fit_transform(X_raw)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# final NaN guards\n",
    "if np.isnan(X_scaled).any():\n",
    "    raise ValueError(\"X still contains NaNs after imputation/standardization. Please inspect your data.\")\n",
    "\n",
    "if y.isna().any():\n",
    "    raise ValueError(\"y contains NaNs after filtering; this should not happen.\")\n",
    "\n",
    "# ---------- fit non-negative models ----------\n",
    "results = {}\n",
    "\n",
    "# (A) Positive OLS\n",
    "ols_pos = LinearRegression(positive=True)\n",
    "ols_pos.fit(X_scaled, y)\n",
    "r2_ols = float(ols_pos.score(X_scaled, y)) if y.var() > 0 else float(\"nan\")\n",
    "results[\"OLS_Positive\"] = (r2_ols, pd.Series(ols_pos.coef_, index=X_raw.columns))\n",
    "\n",
    "# (B) NNLS with mean intercept (stable, non-negative)\n",
    "y_mean = float(y.mean())\n",
    "w_nnls, _ = nnls(X_scaled, (y - y_mean).to_numpy())\n",
    "y_pred = y_mean + X_scaled @ w_nnls\n",
    "r2_nnls = float(1 - np.sum((y - y_pred)**2) / np.sum((y - y_mean)**2)) if y.var() > 0 else float(\"nan\")\n",
    "results[\"NNLS_Positive\"] = (r2_nnls, pd.Series(w_nnls, index=X_raw.columns))\n",
    "\n",
    "# ---------- report ----------\n",
    "rows = []\n",
    "for name, (r2, coefs) in results.items():\n",
    "    row = {\"Model\": name, \"R2\": r2}\n",
    "    row.update({f\"w:{k}\": v for k,v in coefs.items()})\n",
    "    rows.append(row)\n",
    "\n",
    "comp = pd.DataFrame(rows).set_index(\"Model\").sort_values(\"R2\", ascending=False)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"\\n=== Model comparison (non-negative only) ===\")\n",
    "display(comp.round(4))\n",
    "\n",
    "best_name = comp.index[0]\n",
    "best_r2, best_coefs = results[best_name]\n",
    "print(f\"\\nBest non-negative model: {best_name}  (R²={best_r2:.3f})\")\n",
    "print(\"\\nSorted weights (standardized):\")\n",
    "print(best_coefs.sort_values(ascending=False).round(4))\n",
    "\n",
    "# ---------- save artifacts for reuse ----------\n",
    "weights_path = OUT_DIR / f\"rb_weights_{best_name}.csv\"\n",
    "scaler_path  = OUT_DIR / \"rb_scaler.json\"\n",
    "meta_path    = OUT_DIR / \"rb_feature_mapping.json\"\n",
    "\n",
    "best_coefs.to_csv(weights_path, header=[\"coef\"])\n",
    "with open(scaler_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"means\": scaler.mean_.tolist(),\n",
    "        \"scales\": scaler.scale_.tolist(),\n",
    "        \"feature_order\": list(X_raw.columns),\n",
    "        \"intercept_mean\": y_mean,\n",
    "        \"model\": best_name\n",
    "    }, f, indent=2)\n",
    "\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump({\"mapped_columns\": mapped, \"kept_features\": keep, \"target\": y_col}, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved weights → {weights_path}\")\n",
    "print(f\"Saved scaler   → {scaler_path}\")\n",
    "print(f\"Saved mapping  → {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651ccdd-feb7-419e-a7a7-a33a4596402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Apply Stack Ridge Meta to entire dataset\n",
    "# Train on ALL rows, predict for ALL rows\n",
    "# Draft Age treated like any other feature (base models only)\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_PATH    = Path(\"./data/Bakery/_derived/stack_ridge_all_players.csv\")\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "\n",
    "# canonical features (+ Draft Age; no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    # Draft Age (treated as a normal feature)\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\" (faster times, earlier rounds, younger age)\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = -X[c]\n",
    "    # baseline interactions (unchanged; Draft Age is NOT injected into meta)\n",
    "    if \"BMI\" in X and \"40 Time\" in X:        X[\"BMIx40\"]   = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X:          X[\"ELUxYCOA\"] = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X:            X[\"YPCxELU\"]  = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "\n",
    "X = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y.notna()\n",
    "X, y, names = X.loc[mask], y.loc[mask], names.loc[mask]\n",
    "\n",
    "# interactions (+ invert)\n",
    "X = basic_interactions(X)\n",
    "\n",
    "# impute\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X)\n",
    "\n",
    "# ---------- base models (light tuning) ----------\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = {\"n_estimators\":[800], \"learning_rate\":[0.05], \"max_depth\":[4], \"subsample\":[0.85]}\n",
    "gb_best = RandomizedSearchCV(gb, gb_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS).fit(X_imp, y).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "rf_param = {\"n_estimators\":[800], \"max_depth\":[12], \"min_samples_split\":[2], \"min_samples_leaf\":[1]}\n",
    "rf_best = RandomizedSearchCV(rf, rf_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS).fit(X_imp, y).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "# ---------- stacking with OOF (meta sees ONLY OOF predictions) ----------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_imp), len(base_models)))\n",
    "for j,(nm, mdl) in enumerate(base_models):\n",
    "    fold_preds = np.zeros(len(X_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_imp):\n",
    "        mdl.fit(X_imp[tr_idx], y.iloc[tr_idx])\n",
    "        fold_preds[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "    oof[:, j] = fold_preds\n",
    "\n",
    "# meta learner (Ridge on standardized OOF preds)\n",
    "sc_meta = StandardScaler()\n",
    "Z = sc_meta.fit_transform(oof)\n",
    "ridge_meta = RidgeCV(alphas=np.logspace(-3, 2, 30)).fit(Z, y)\n",
    "\n",
    "# retrain base models on full data\n",
    "full_preds = []\n",
    "for nm, mdl in base_models:\n",
    "    mdl.fit(X_imp, y)\n",
    "    full_preds.append(mdl.predict(X_imp))\n",
    "stack_inputs = np.vstack(full_preds).T\n",
    "stack_inputs = sc_meta.transform(stack_inputs)\n",
    "\n",
    "y_hat = ridge_meta.predict(stack_inputs)\n",
    "\n",
    "# ---------- results ----------\n",
    "out = pd.DataFrame({\n",
    "    \"Player\": names.values,\n",
    "    \"Actual_RB_Grade\": y.values,\n",
    "    \"Predicted_RB_Grade\": y_hat,\n",
    "    \"Error\": y_hat - y.values\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Full Dataset Results (Stack Ridge Meta; Draft Age as normal feature) ===\")\n",
    "print(out.head(20).round(3))\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\nSaved → {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efa8d7-b1e4-4e15-b0ea-a17812ad78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Stack Ridge Meta with 80/20 Train-Test (Draft Age included as a normal feature)\n",
    "# - Base models: GradientBoosting + RandomForest\n",
    "# - Meta: RidgeCV on OOF predictions\n",
    "# - Interactions: BMIx40, ELUxYCOA, DOMxDraft, YPCxELU\n",
    "# - Inversions: 40 Time, Draft Capital, Shuttle, Three Cone, Draft Age (lower is better)\n",
    "# - Predictions clipped to [0, 15]\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "TEST_SIZE   = 0.20\n",
    "SEEDS       = [42, 1337, 7]   # run multiple iterations; add/remove seeds as desired\n",
    "N_JOBS      = -1\n",
    "\n",
    "# canonical features (+ Draft Age; no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    # Draft Age (treated as a normal feature)\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\" (faster times, earlier rounds, younger age)\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = -X[c]\n",
    "    # high-signal interactions\n",
    "    if \"BMI\" in X and \"40 Time\" in X:              X[\"BMIx40\"]    = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X:                X[\"ELUxYCOA\"]  = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X:      X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X:                  X[\"YPCxELU\"]   = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Could not find target column in {TARGET_CANDS}. Available: {list(df.columns)}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found from ALIASES.\")\n",
    "\n",
    "X_all = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all, y_all, names_all = X_all.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# build features\n",
    "X_all = basic_interactions(X_all)\n",
    "\n",
    "# ---------- runner for one split ----------\n",
    "def run_one(seed: int):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    pred_path = OUT_DIR / f\"stack_ridge_test_predictions_seed{seed}.csv\"\n",
    "\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test, n_train, n_test = train_test_split(\n",
    "        X_all, y_all, names_all, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    # impute (fit on train only)\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    Xtr = imp.fit_transform(X_train)\n",
    "    Xte = imp.transform(X_test)\n",
    "\n",
    "    # base models (light tuning, CV on train only)\n",
    "    gb = GradientBoostingRegressor(random_state=seed)\n",
    "    gb_param = {\"n_estimators\":[800], \"learning_rate\":[0.05], \"max_depth\":[4], \"subsample\":[0.85]}\n",
    "    gb_best = RandomizedSearchCV(gb, gb_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=seed, n_jobs=N_JOBS)\n",
    "    rf_param = {\"n_estimators\":[800], \"max_depth\":[12], \"min_samples_split\":[2], \"min_samples_leaf\":[1]}\n",
    "    rf_best = RandomizedSearchCV(rf, rf_param, n_iter=1, scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    base_models = [(\"gb\", gb_best), (\"rf\", rf_best)]\n",
    "\n",
    "    # OOF for meta — on TRAIN only\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(Xtr), len(base_models)))\n",
    "    for j,(nm, mdl) in enumerate(base_models):\n",
    "        fold_preds = np.zeros(len(Xtr))\n",
    "        for tr_idx, val_idx in kf.split(Xtr):\n",
    "            mdl.fit(Xtr[tr_idx], y_train.iloc[tr_idx])\n",
    "            fold_preds[val_idx] = mdl.predict(Xtr[val_idx])\n",
    "        oof[:, j] = fold_preds\n",
    "\n",
    "    # meta (Ridge) on standardized OOF\n",
    "    sc_meta = StandardScaler()\n",
    "    Ztr = sc_meta.fit_transform(oof)\n",
    "    ridge_meta = RidgeCV(alphas=np.logspace(-3, 2, 30)).fit(Ztr, y_train)\n",
    "\n",
    "    # fit bases on full TRAIN, predict TEST\n",
    "    base_preds_test = []\n",
    "    for nm, mdl in base_models:\n",
    "        mdl.fit(Xtr, y_train)\n",
    "        base_preds_test.append(mdl.predict(Xte))\n",
    "    stack_inputs_te = np.vstack(base_preds_test).T\n",
    "    stack_inputs_te = sc_meta.transform(stack_inputs_te)\n",
    "\n",
    "    y_pred_test = ridge_meta.predict(stack_inputs_te)\n",
    "\n",
    "    # clip predictions to [0, 15] per requirement\n",
    "    y_pred_test = np.clip(y_pred_test, 0.0, 15.0)\n",
    "\n",
    "    # metrics on TEST\n",
    "    r2   = r2_score(y_test, y_pred_test)\n",
    "    mae  = mean_absolute_error(y_test, y_pred_test)\n",
    "    rmse_val = rmse(y_test, y_pred_test)\n",
    "\n",
    "    # save test predictions\n",
    "    out = pd.DataFrame({\n",
    "        \"Player\": n_test.values,\n",
    "        \"Actual_RB_Grade\": y_test.values,\n",
    "        \"Predicted_RB_Grade\": y_pred_test,\n",
    "        \"Error\": y_pred_test - y_test.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    out.to_csv(pred_path, index=False)\n",
    "\n",
    "    # print quick preview\n",
    "    print(f\"\\n=== Seed {seed} — Test Metrics ===\")\n",
    "    print(f\"R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse_val:.4f}\")\n",
    "    print(f\"Max predicted (test): {out['Predicted_RB_Grade'].max():.3f}\")\n",
    "    print(f\"Saved test predictions → {pred_path}\")\n",
    "\n",
    "    return {\"seed\": seed, \"R2\": r2, \"MAE\": mae, \"RMSE\": rmse_val, \"csv\": str(pred_path)}\n",
    "\n",
    "# ---------- run multiple iterations ----------\n",
    "results = [run_one(s) for s in SEEDS]\n",
    "summary = pd.DataFrame(results).sort_values(\"R2\", ascending=False)\n",
    "print(\"\\n=== Summary across seeds (80/20 splits) ===\")\n",
    "print(summary.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c1f5a-7892-42b5-8ee8-3d047b0a221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 80/20 Stack with NNLS + Isotonic (push R^2 toward .90)\n",
    "# Bases: GB, RF, ExtraTrees, HistGB\n",
    "# Meta: NNLS (non-negative) + Isotonic calibration\n",
    "# Draft Age treated like any other feature (inverted)\n",
    "# Predictions clipped to [0, 15]\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "TEST_SIZE   = 0.20\n",
    "SEEDS       = [42, 1337, 7]\n",
    "N_JOBS      = -1\n",
    "N_FOLDS     = 5\n",
    "N_ITER      = 20   # random search iterations per base (cap by grid size)\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\"\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    # a few strong interactions\n",
    "    if \"BMI\" in X and \"40 Time\" in X:         X[\"BMIx40\"]    = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X:           X[\"ELUxYCOA\"]  = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X:             X[\"YPCxELU\"]   = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def space_size(param_grid: dict) -> int:\n",
    "    \"\"\"Number of unique combinations in a randomized grid (product of list lengths).\"\"\"\n",
    "    n = 1\n",
    "    for v in param_grid.values():\n",
    "        n *= len(v)\n",
    "    return n\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target column not found. Available: {list(df.columns)}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found from ALIASES.\")\n",
    "\n",
    "X_all = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all, y_all, names_all = X_all.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "X_all = basic_interactions(X_all)\n",
    "\n",
    "# ---------- run one seed ----------\n",
    "from sklearn.utils._param_validation import InvalidParameterError  # safe import; unused but helps debuggers\n",
    "\n",
    "def run_one(seed: int):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    pred_path = OUT_DIR / f\"stack_nnls_iso_test_predictions_seed{seed}.csv\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test, n_train, n_test = train_test_split(\n",
    "        X_all, y_all, names_all, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    Xtr = imp.fit_transform(X_train)\n",
    "    Xte = imp.transform(X_test)\n",
    "\n",
    "    # --- tune bases on TRAIN only ---\n",
    "    gb = GradientBoostingRegressor(random_state=seed)\n",
    "    gb_param = {\n",
    "        \"n_estimators\": [600, 900, 1200],\n",
    "        \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "        \"max_depth\": [3, 4],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "    }\n",
    "    gb_best = RandomizedSearchCV(\n",
    "        gb, gb_param, n_iter=min(N_ITER, space_size(gb_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=seed, n_jobs=N_JOBS)\n",
    "    rf_param = {\n",
    "        \"n_estimators\": [700, 1000, 1300],\n",
    "        \"max_depth\": [None, 12, 16],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "    }\n",
    "    rf_best = RandomizedSearchCV(\n",
    "        rf, rf_param, n_iter=min(N_ITER, space_size(rf_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    et = ExtraTreesRegressor(random_state=seed, n_jobs=N_JOBS)\n",
    "    et_param = {\n",
    "        \"n_estimators\": [700, 1000, 1300],\n",
    "        \"max_depth\": [None, 12, 16],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "    }\n",
    "    et_best = RandomizedSearchCV(\n",
    "        et, et_param, n_iter=min(N_ITER, space_size(et_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    hgb = HistGradientBoostingRegressor(random_state=seed)\n",
    "    hgb_param = {\n",
    "        \"learning_rate\": [0.03, 0.05, 0.08],\n",
    "        \"max_depth\": [3, 6, 9],\n",
    "        \"l2_regularization\": [0.0, 0.1, 0.5],\n",
    "        \"max_bins\": [128, 255],\n",
    "    }\n",
    "    hgb_best = RandomizedSearchCV(\n",
    "        hgb, hgb_param, n_iter=min(N_ITER, space_size(hgb_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    base_models = [(\"gb\", gb_best), (\"rf\", rf_best), (\"et\", et_best), (\"hgb\", hgb_best)]\n",
    "\n",
    "    # --- OOF for meta (TRAIN only) ---\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(Xtr), len(base_models)))\n",
    "    for j,(nm, mdl) in enumerate(base_models):\n",
    "        fold_pred = np.zeros(len(Xtr))\n",
    "        for tr_idx, val_idx in kf.split(Xtr):\n",
    "            mdl.fit(Xtr[tr_idx], y_train.iloc[tr_idx])\n",
    "            fold_pred[val_idx] = mdl.predict(Xtr[val_idx])\n",
    "        oof[:, j] = fold_pred\n",
    "\n",
    "    # --- NNLS meta on OOF ---\n",
    "    w_nnls, _ = nnls(oof, y_train.values)\n",
    "    stack_train_raw = oof @ w_nnls\n",
    "\n",
    "    # --- Isotonic calibration on TRAIN ---\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    _ = iso.fit_transform(stack_train_raw, y_train.values)  # fit only; discard transformed train\n",
    "\n",
    "    # --- Predict TEST ---\n",
    "    base_preds_test = []\n",
    "    for nm, mdl in base_models:\n",
    "        mdl.fit(Xtr, y_train)\n",
    "        base_preds_test.append(mdl.predict(Xte))\n",
    "    base_preds_test = np.vstack(base_preds_test).T\n",
    "\n",
    "    y_pred_raw = base_preds_test @ w_nnls\n",
    "    y_pred_test = iso.transform(y_pred_raw)\n",
    "\n",
    "    # clip to [0, 15]\n",
    "    y_pred_test = np.clip(y_pred_test, 0.0, 15.0)\n",
    "\n",
    "    # metrics\n",
    "    r2   = r2_score(y_test, y_pred_test)\n",
    "    mae  = mean_absolute_error(y_test, y_pred_test)\n",
    "    rmse_val = rmse(y_test, y_pred_test)\n",
    "\n",
    "    # save\n",
    "    out = pd.DataFrame({\n",
    "        \"Player\": n_test.values,\n",
    "        \"Actual_RB_Grade\": y_test.values,\n",
    "        \"Predicted_RB_Grade\": y_pred_test,\n",
    "        \"Error\": y_pred_test - y_test.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(pred_path, index=False)\n",
    "\n",
    "    preview15 = out.head(15)\n",
    "\n",
    "    print(f\"\\n=== Seed {seed} — TEST ===\")\n",
    "    print(f\"R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse_val:.4f} | MaxPred: {out['Predicted_RB_Grade'].max():.3f}\")\n",
    "    return {\"seed\": seed, \"R2\": r2, \"MAE\": mae, \"RMSE\": rmse_val, \"csv\": str(pred_path), \"top15\": preview15}\n",
    "\n",
    "# ---------- run ----------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "results = [run_one(s) for s in SEEDS]\n",
    "summary = pd.DataFrame([{k:v for k,v in r.items() if k!='top15'} for r in results]).sort_values(\"R2\", ascending=False)\n",
    "best = max(results, key=lambda d: d[\"R2\"])\n",
    "\n",
    "print(\"\\n=== Summary across seeds ===\")\n",
    "print(summary.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "print(f\"\\nBest seed: {best['seed']}  R²={best['R2']:.4f}  MAE={best['MAE']:.4f}  RMSE={best['RMSE']:.4f}\")\n",
    "print(f\"Test predictions CSV: {best['csv']}\")\n",
    "\n",
    "print(\"\\n=== Top 15 players (Actual vs Predicted) — Best Seed ===\")\n",
    "print(best[\"top15\"][[\"Player\",\"Actual_RB_Grade\",\"Predicted_RB_Grade\",\"Error\"]].round(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107d1e2-4de0-4f01-8cc4-6663a1ec571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Full-dataset stack (ALL rows) with NNLS + Isotonic\n",
    "# Bases: GB, RF, ExtraTrees, HistGB\n",
    "# Meta: NNLS (non-negative) + Isotonic calibration\n",
    "# Draft Age treated like any other feature (inverted)\n",
    "# Predictions clipped to [0, 15]\n",
    "# Saves:\n",
    "#   - data/Bakery/_derived/stack_nnls_iso_all_players.csv\n",
    "#   - data/Bakery/_derived/stack_nnls_iso_feature_impact.csv\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "ALL_PRED_CSV  = OUT_DIR / \"stack_nnls_iso_all_players.csv\"\n",
    "IMPACT_CSV    = OUT_DIR / \"stack_nnls_iso_feature_impact.csv\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "N_FOLDS     = 5\n",
    "N_ITER      = 20   # cap by grid-size helper below\n",
    "\n",
    "# canonical features (+ Draft Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\"\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    # a few strong interactions\n",
    "    if \"BMI\" in X and \"40 Time\" in X:         X[\"BMIx40\"]    = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X:           X[\"ELUxYCOA\"]  = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X:             X[\"YPCxELU\"]   = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def space_size(param_grid: dict) -> int:\n",
    "    n = 1\n",
    "    for v in param_grid.values():\n",
    "        n *= len(v)\n",
    "    return n\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target column not found. Available: {list(df.columns)}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found from ALIASES.\")\n",
    "\n",
    "X_all = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all, y_all, names_all = X_all.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "X_all = basic_interactions(X_all)\n",
    "\n",
    "# impute (fit on ALL, we are producing in-sample predictions)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X_all)\n",
    "\n",
    "# ---------- tune base models on ALL (for final refit/pred) ----------\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = {\n",
    "    \"n_estimators\": [600, 900, 1200],\n",
    "    \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "    \"max_depth\": [3, 4],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "}\n",
    "gb_best = RandomizedSearchCV(\n",
    "    gb, gb_param, n_iter=min(N_ITER, space_size(gb_param)),\n",
    "    scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=RANDOM_SEED\n",
    ").fit(X_imp, y_all).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "rf_param = {\n",
    "    \"n_estimators\": [700, 1000, 1300],\n",
    "    \"max_depth\": [None, 12, 16],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "}\n",
    "rf_best = RandomizedSearchCV(\n",
    "    rf, rf_param, n_iter=min(N_ITER, space_size(rf_param)),\n",
    "    scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=RANDOM_SEED\n",
    ").fit(X_imp, y_all).best_estimator_\n",
    "\n",
    "et = ExtraTreesRegressor(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "et_param = {\n",
    "    \"n_estimators\": [700, 1000, 1300],\n",
    "    \"max_depth\": [None, 12, 16],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "}\n",
    "et_best = RandomizedSearchCV(\n",
    "    et, et_param, n_iter=min(N_ITER, space_size(et_param)),\n",
    "    scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=RANDOM_SEED\n",
    ").fit(X_imp, y_all).best_estimator_\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "hgb_param = {\n",
    "    \"learning_rate\": [0.03, 0.05, 0.08],\n",
    "    \"max_depth\": [3, 6, 9],\n",
    "    \"l2_regularization\": [0.0, 0.1, 0.5],\n",
    "    \"max_bins\": [128, 255],\n",
    "}\n",
    "hgb_best = RandomizedSearchCV(\n",
    "    hgb, hgb_param, n_iter=min(N_ITER, space_size(hgb_param)),\n",
    "    scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=RANDOM_SEED\n",
    ").fit(X_imp, y_all).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best), (\"et\", et_best), (\"hgb\", hgb_best)]\n",
    "\n",
    "# ---------- build OOF on ALL rows for meta + calib (no leakage in meta fit) ----------\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_imp), len(base_models)))\n",
    "for j,(nm, mdl) in enumerate(base_models):\n",
    "    fold_pred = np.zeros(len(X_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_imp):\n",
    "        mdl.fit(X_imp[tr_idx], y_all.iloc[tr_idx])\n",
    "        fold_pred[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "    oof[:, j] = fold_pred\n",
    "\n",
    "# NNLS meta on OOF\n",
    "w_nnls, _ = nnls(oof, y_all.values)\n",
    "stack_oof_raw = oof @ w_nnls\n",
    "\n",
    "# Isotonic calibration on OOF\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "_ = iso.fit_transform(stack_oof_raw, y_all.values)  # fit only\n",
    "\n",
    "# ---------- final fit on ALL, predict ALL ----------\n",
    "base_preds_all = []\n",
    "for nm, mdl in base_models:\n",
    "    mdl.fit(X_imp, y_all)\n",
    "    base_preds_all.append(mdl.predict(X_imp))\n",
    "base_preds_all = np.vstack(base_preds_all).T\n",
    "\n",
    "y_pred_raw = base_preds_all @ w_nnls\n",
    "y_pred_all = iso.transform(y_pred_raw)\n",
    "y_pred_all = np.clip(y_pred_all, 0.0, 15.0)   # enforce [0, 15]\n",
    "\n",
    "# ---------- summary + save per-player ----------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "per_player = pd.DataFrame({\n",
    "    \"Player\": names_all.values,\n",
    "    \"Actual_RB_Grade\": y_all.values,\n",
    "    \"Predicted_RB_Grade\": y_pred_all,\n",
    "    \"Error\": y_pred_all - y_all.values\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "per_player.to_csv(ALL_PRED_CSV, index=False)\n",
    "\n",
    "# in-sample fit quality (for context only)\n",
    "r2_full  = r2_score(y_all, y_pred_all)\n",
    "mae_full = mean_absolute_error(y_all, y_pred_all)\n",
    "rmse_full= rmse(y_all, y_pred_all)\n",
    "\n",
    "print(\"\\n=== Full-dataset (in-sample) fit quality ===\")\n",
    "print(f\"R²: {r2_full:.4f} | MAE: {mae_full:.4f} | RMSE: {rmse_full:.4f}\")\n",
    "print(f\"Saved per-player predictions → {ALL_PRED_CSV}\")\n",
    "\n",
    "print(\"\\nTop 20 by Actual RB Grade:\")\n",
    "print(per_player.head(20).round(3).to_string(index=False))\n",
    "\n",
    "# ---------- feature impact (meta-weighted) ----------\n",
    "# Combine GB/RF/ET importances using NNLS weights (normalized)\n",
    "def safe_importances(model, feature_names):\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        s = pd.Series(model.feature_importances_, index=feature_names)\n",
    "        return s / (s.sum() + 1e-12)\n",
    "    return pd.Series(0.0, index=feature_names)\n",
    "\n",
    "feature_order = list(X_all.columns)   # after interactions\n",
    "imp_gb = safe_importances(gb_best, feature_order)\n",
    "imp_rf = safe_importances(rf_best, feature_order)\n",
    "imp_et = safe_importances(et_best, feature_order)\n",
    "# HistGB often lacks feature_importances_; ignore for impact aggregation\n",
    "\n",
    "# normalize NNLS weights and use only the bases we included in impact\n",
    "meta_w = pd.Series(w_nnls, index=[nm for nm,_ in base_models])\n",
    "meta_w = meta_w / (meta_w.abs().sum() + 1e-12)\n",
    "\n",
    "combined = (\n",
    "    meta_w.get(\"gb\",0.0)*imp_gb +\n",
    "    meta_w.get(\"rf\",0.0)*imp_rf +\n",
    "    meta_w.get(\"et\",0.0)*imp_et\n",
    ")\n",
    "\n",
    "impact_df = pd.DataFrame({\n",
    "    \"Impact_MetaWeighted\": combined,\n",
    "    \"GB_Importance\": imp_gb,\n",
    "    \"RF_Importance\": imp_rf,\n",
    "    \"ET_Importance\": imp_et\n",
    "}).sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "\n",
    "impact_df.to_csv(IMPACT_CSV, index=False)\n",
    "print(f\"\\nSaved feature impact table → {IMPACT_CSV}\")\n",
    "print(\"\\nTop 15 features by impact:\")\n",
    "print(impact_df.head(15).round(4).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb301c00-3566-4f2e-b2ec-223753311ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Full-dataset stack (ALL rows) with NNLS (no isotonic for in-sample use)\n",
    "# Bases: GB, RF, ExtraTrees, HistGB\n",
    "# Draft Age de-emphasized: winsorize + scale\n",
    "# Predictions clipped to [0, 15]\n",
    "# Saves:\n",
    "#   - data/Bakery/_derived/stack_nnls_all_players.csv\n",
    "#   - data/Bakery/_derived/stack_nnls_feature_impact.csv\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "ALL_PRED_CSV  = OUT_DIR / \"stack_nnls_all_players.csv\"\n",
    "IMPACT_CSV    = OUT_DIR / \"stack_nnls_feature_impact.csv\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS      = -1\n",
    "N_FOLDS     = 5\n",
    "N_ITER      = 20   # cap by grid-size helper below\n",
    "\n",
    "# De-emphasize Draft Age\n",
    "DRAFT_AGE_SCALE = 0.30      # try 0.2–0.4 to lower its influence\n",
    "WINSOR = {                  # tame heavy tails\n",
    "    \"Draft Age\": (0.05, 0.95),\n",
    "    \"Break%\":    (0.01, 0.99),\n",
    "}\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def winsorize(col: pd.Series, lo_hi=(0.01, 0.99)) -> pd.Series:\n",
    "    lo, hi = col.quantile(lo_hi[0]), col.quantile(lo_hi[1])\n",
    "    return col.clip(lo, hi)\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # invert \"lower is better\"\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    # scale down Draft Age to soften its influence\n",
    "    if \"Draft Age\" in X.columns:\n",
    "        X[\"Draft Age\"] = X[\"Draft Age\"] * DRAFT_AGE_SCALE\n",
    "    # a few strong interactions (no Draft Age interactions)\n",
    "    if \"BMI\" in X and \"40 Time\" in X:         X[\"BMIx40\"]    = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X:           X[\"ELUxYCOA\"]  = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X:             X[\"YPCxELU\"]   = X[\"YPC\"] * X[\"ELU\"]\n",
    "    return X\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def space_size(param_grid: dict) -> int:\n",
    "    n = 1\n",
    "    for v in param_grid.values():\n",
    "        n *= len(v)\n",
    "    return n\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target column not found. Available: {list(df.columns)}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found from ALIASES.\")\n",
    "\n",
    "X_all = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all, y_all, names_all = X_all.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# winsorize selected columns BEFORE inversion/interaction\n",
    "for col, q in WINSOR.items():\n",
    "    if col in X_all.columns:\n",
    "        X_all[col] = winsorize(X_all[col], q)\n",
    "\n",
    "# interactions + inversions + DraftAge scale\n",
    "X_all = basic_interactions(X_all)\n",
    "\n",
    "# impute (fit on ALL; in-sample use)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X_all)\n",
    "\n",
    "# ---------- tune base models on ALL ----------\n",
    "gb = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "gb_param = {\n",
    "    \"n_estimators\": [600, 900, 1200],\n",
    "    \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "    \"max_depth\": [3, 4],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "}\n",
    "gb_best = RandomizedSearchCV(\n",
    "    gb, gb_param, n_iter=min(N_ITER, space_size(gb_param)),\n",
    "    scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=RANDOM_SEED\n",
    ").fit(X_imp, y_all).best_estimator_\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "rf_param = {\n",
    "    \"n_estimators\": [700, 1000, 1300],\n",
    "    \"max_depth\": [None, 12, 16],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "}\n",
    "rf_best = RandomizedSearchCV(\n",
    "    rf, rf_param, n_iter=min(N_ITER, space_size(rf_param)),\n",
    "    scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=RANDOM_SEED\n",
    ").fit(X_imp, y_all).best_estimator_\n",
    "\n",
    "et = ExtraTreesRegressor(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "et_param = {\n",
    "    \"n_estimators\": [700, 1000, 1300],\n",
    "    \"max_depth\": [None, 12, 16],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "}\n",
    "et_best = RandomizedSearchCV(\n",
    "    et, et_param, n_iter=min(N_ITER, space_size(et_param)),\n",
    "    scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=RANDOM_SEED\n",
    ").fit(X_imp, y_all).best_estimator_\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "hgb_param = {\n",
    "    \"learning_rate\": [0.03, 0.05, 0.08],\n",
    "    \"max_depth\": [3, 6, 9],\n",
    "    \"l2_regularization\": [0.0, 0.1, 0.5],\n",
    "    \"max_bins\": [128, 255],\n",
    "}\n",
    "hgb_best = RandomizedSearchCV(\n",
    "    hgb, hgb_param, n_iter=min(N_ITER, space_size(hgb_param)),\n",
    "    scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=RANDOM_SEED\n",
    ").fit(X_imp, y_all).best_estimator_\n",
    "\n",
    "base_models = [(\"gb\", gb_best), (\"rf\", rf_best), (\"et\", et_best), (\"hgb\", hgb_best)]\n",
    "\n",
    "# ---------- build OOF on ALL rows for NNLS meta ----------\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros((len(X_imp), len(base_models)))\n",
    "for j,(nm, mdl) in enumerate(base_models):\n",
    "    fold_pred = np.zeros(len(X_imp))\n",
    "    for tr_idx, val_idx in kf.split(X_imp):\n",
    "        mdl.fit(X_imp[tr_idx], y_all.iloc[tr_idx])\n",
    "        fold_pred[val_idx] = mdl.predict(X_imp[val_idx])\n",
    "    oof[:, j] = fold_pred\n",
    "\n",
    "# NNLS meta on OOF (no isotonic to avoid flat steps in-sample)\n",
    "w_nnls, _ = nnls(oof, y_all.values)\n",
    "\n",
    "# ---------- final fit on ALL, predict ALL ----------\n",
    "base_preds_all = []\n",
    "for nm, mdl in base_models:\n",
    "    mdl.fit(X_imp, y_all)\n",
    "    base_preds_all.append(mdl.predict(X_imp))\n",
    "base_preds_all = np.vstack(base_preds_all).T\n",
    "\n",
    "y_pred_all = base_preds_all @ w_nnls\n",
    "y_pred_all = np.clip(y_pred_all, 0.0, 15.0)\n",
    "\n",
    "# ---------- summary + save per-player ----------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "per_player = pd.DataFrame({\n",
    "    \"Player\": names_all.values,\n",
    "    \"Actual_RB_Grade\": y_all.values,\n",
    "    \"Predicted_RB_Grade\": y_pred_all,\n",
    "    \"Error\": y_pred_all - y_all.values\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "per_player.to_csv(ALL_PRED_CSV, index=False)\n",
    "\n",
    "r2_full  = r2_score(y_all, y_pred_all)\n",
    "mae_full = mean_absolute_error(y_all, y_pred_all)\n",
    "rmse_full= rmse(y_all, y_pred_all)\n",
    "\n",
    "print(\"\\n=== Full-dataset (in-sample) fit quality (no isotonic) ===\")\n",
    "print(f\"R²: {r2_full:.4f} | MAE: {mae_full:.4f} | RMSE: {rmse_full:.4f}\")\n",
    "print(f\"Saved per-player predictions → {ALL_PRED_CSV}\")\n",
    "\n",
    "print(\"\\nTop 20 by Actual RB Grade:\")\n",
    "print(per_player.head(20).round(3).to_string(index=False))\n",
    "\n",
    "# ---------- feature impact (meta-weighted) ----------\n",
    "def safe_importances(model, feature_names):\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        s = pd.Series(model.feature_importances_, index=feature_names)\n",
    "        return s / (s.sum() + 1e-12)\n",
    "    return pd.Series(0.0, index=feature_names)\n",
    "\n",
    "feature_order = list(X_all.columns)   # after interactions & scaling\n",
    "imp_gb = safe_importances(gb_best, feature_order)\n",
    "imp_rf = safe_importances(rf_best, feature_order)\n",
    "imp_et = safe_importances(et_best, feature_order)\n",
    "\n",
    "meta_w = pd.Series(w_nnls, index=[nm for nm,_ in base_models])\n",
    "meta_w = meta_w / (meta_w.abs().sum() + 1e-12)\n",
    "\n",
    "combined = (\n",
    "    meta_w.get(\"gb\",0.0)*imp_gb +\n",
    "    meta_w.get(\"rf\",0.0)*imp_rf +\n",
    "    meta_w.get(\"et\",0.0)*imp_et\n",
    ")\n",
    "\n",
    "impact_df = pd.DataFrame({\n",
    "    \"Impact_MetaWeighted\": combined,\n",
    "    \"GB_Importance\": imp_gb,\n",
    "    \"RF_Importance\": imp_rf,\n",
    "    \"ET_Importance\": imp_et\n",
    "}).sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "\n",
    "impact_df.to_csv(IMPACT_CSV, index=False)\n",
    "print(f\"\\nSaved feature impact table → {IMPACT_CSV}\")\n",
    "print(\"\\nTop 15 features by impact:\")\n",
    "print(impact_df.head(15).round(4).to_string())\n",
    "\n",
    "# Quick check: show Draft Age impact after scaling\n",
    "if \"Draft Age\" in impact_df.index:\n",
    "    print(\"\\nDraft Age impact (after scaling):\")\n",
    "    print(impact_df.loc[\"Draft Age\"].round(5).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adddcc3-5117-4eae-bd82-49e08c59cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 80/20 Stack (NNLS + Isotonic) with targeted feature influences\n",
    "# - Push Break% impact up (~0.06) and Draft Age down (~0.02)\n",
    "# - Bases: GB, RF, ExtraTrees, HistGB\n",
    "# - Meta: NNLS (non-negative) + Isotonic calibration\n",
    "# - Draft Age & test preds treated normally (preds clipped to [0, 15])\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# -----------------------------\n",
    "# Config (tune these two first)\n",
    "# -----------------------------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "TEST_SIZE   = 0.20\n",
    "SEEDS       = [42, 1337, 7]      # try a few seeds; best R2 is reported\n",
    "N_JOBS      = -1\n",
    "N_FOLDS     = 5\n",
    "N_ITER      = 20                 # random-search iterations per base (capped by grid size)\n",
    "\n",
    "# Targeted influence nudges\n",
    "BREAK_SCALE       = 7.0          # ↑ to boost Break% impact (~5–9 is typical)\n",
    "DRAFT_AGE_SCALE   = 0.12         # ↓ to reduce Draft Age impact (~0.08–0.15)\n",
    "WINSOR = {\n",
    "    \"Draft Age\": (0.10, 0.90),   # tighter caps to avoid outlier dominance\n",
    "    \"Break%\":    (0.01, 0.99),   # keep tails so it can matter\n",
    "}\n",
    "\n",
    "# Canonical column aliases (Draft Age included; no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def winsorize_col(x: pd.Series, lo: float, hi: float) -> pd.Series:\n",
    "    lo_q = x.quantile(lo)\n",
    "    hi_q = x.quantile(hi)\n",
    "    return x.clip(lo_q, hi_q)\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "\n",
    "    # 1) Winsorize BEFORE scaling so the scale applies to trimmed ranges\n",
    "    for col, (lo, hi) in WINSOR.items():\n",
    "        if col in X.columns:\n",
    "            X[col] = winsorize_col(X[col], lo, hi)\n",
    "\n",
    "    # 2) Feature-specific scaling (nudges)\n",
    "    if \"Break%\" in X.columns:\n",
    "        X[\"Break%\"] = X[\"Break%\"] * BREAK_SCALE\n",
    "    if \"Draft Age\" in X.columns:\n",
    "        X[\"Draft Age\"] = X[\"Draft Age\"] * DRAFT_AGE_SCALE\n",
    "\n",
    "    # 3) Invert “lower is better” timing/age/round metrics\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = -X[c]\n",
    "\n",
    "    # 4) A few strong interactions\n",
    "    if \"BMI\" in X and \"40 Time\" in X:         X[\"BMIx40\"]    = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X:           X[\"ELUxYCOA\"]  = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X:             X[\"YPCxELU\"]   = X[\"YPC\"] * X[\"ELU\"]\n",
    "\n",
    "    return X\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def space_size(param_grid: dict) -> int:\n",
    "    n = 1\n",
    "    for v in param_grid.values():\n",
    "        n *= len(v)\n",
    "    return n\n",
    "\n",
    "# -----------------------------\n",
    "# Load / prepare data\n",
    "# -----------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target not found. Available: {list(df.columns)}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found from ALIASES.\")\n",
    "\n",
    "X_all = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all, y_all, names_all = X_all.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# Build engineered features (winsorize + scale + invert + interactions)\n",
    "X_all = basic_interactions(X_all)\n",
    "\n",
    "# -----------------------------\n",
    "# Run one seed (train/test)\n",
    "# -----------------------------\n",
    "def run_one(seed: int):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    pred_path = OUT_DIR / f\"stack_nnls_iso_test_predictions_seed{seed}.csv\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test, n_train, n_test = train_test_split(\n",
    "        X_all, y_all, names_all, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    Xtr = imp.fit_transform(X_train)\n",
    "    Xte = imp.transform(X_test)\n",
    "\n",
    "    # ---- tune bases on TRAIN only ----\n",
    "    gb = GradientBoostingRegressor(random_state=seed)\n",
    "    gb_param = {\n",
    "        \"n_estimators\": [600, 900, 1200],\n",
    "        \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "        \"max_depth\": [3, 4],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "    }\n",
    "    gb_best = RandomizedSearchCV(\n",
    "        gb, gb_param, n_iter=min(N_ITER, space_size(gb_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=seed, n_jobs=N_JOBS)\n",
    "    rf_param = {\n",
    "        \"n_estimators\": [700, 1000, 1300],\n",
    "        \"max_depth\": [None, 12, 16],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "    }\n",
    "    rf_best = RandomizedSearchCV(\n",
    "        rf, rf_param, n_iter=min(N_ITER, space_size(rf_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    et = ExtraTreesRegressor(random_state=seed, n_jobs=N_JOBS)\n",
    "    et_param = {\n",
    "        \"n_estimators\": [700, 1000, 1300],\n",
    "        \"max_depth\": [None, 12, 16],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "    }\n",
    "    et_best = RandomizedSearchCV(\n",
    "        et, et_param, n_iter=min(N_ITER, space_size(et_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    hgb = HistGradientBoostingRegressor(random_state=seed)\n",
    "    hgb_param = {\n",
    "        \"learning_rate\": [0.03, 0.05, 0.08],\n",
    "        \"max_depth\": [3, 6, 9],\n",
    "        \"l2_regularization\": [0.0, 0.1, 0.5],\n",
    "        \"max_bins\": [128, 255],\n",
    "    }\n",
    "    hgb_best = RandomizedSearchCV(\n",
    "        hgb, hgb_param, n_iter=min(N_ITER, space_size(hgb_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    base_models = [(\"gb\", gb_best), (\"rf\", rf_best), (\"et\", et_best), (\"hgb\", hgb_best)]\n",
    "\n",
    "    # ---- OOF for meta on TRAIN ----\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(Xtr), len(base_models)))\n",
    "    for j,(nm, mdl) in enumerate(base_models):\n",
    "        fold_pred = np.zeros(len(Xtr))\n",
    "        for tr_idx, val_idx in kf.split(Xtr):\n",
    "            mdl.fit(Xtr[tr_idx], y_train.iloc[tr_idx])\n",
    "            fold_pred[val_idx] = mdl.predict(Xtr[val_idx])\n",
    "        oof[:, j] = fold_pred\n",
    "\n",
    "    # ---- NNLS meta + isotonic calibration ----\n",
    "    w_nnls, _ = nnls(oof, y_train.values)\n",
    "    stack_train_raw = oof @ w_nnls\n",
    "\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    _ = iso.fit_transform(stack_train_raw, y_train.values)  # fit only\n",
    "\n",
    "    # ---- Predict TEST ----\n",
    "    base_preds_test = []\n",
    "    for nm, mdl in base_models:\n",
    "        mdl.fit(Xtr, y_train)\n",
    "        base_preds_test.append(mdl.predict(Xte))\n",
    "    base_preds_test = np.vstack(base_preds_test).T\n",
    "\n",
    "    y_pred_raw = base_preds_test @ w_nnls\n",
    "    y_pred_test = iso.transform(y_pred_raw)\n",
    "    y_pred_test = np.clip(y_pred_test, 0.0, 15.0)\n",
    "\n",
    "    # ---- Metrics ----\n",
    "    r2   = r2_score(y_test, y_pred_test)\n",
    "    mae  = mean_absolute_error(y_test, y_pred_test)\n",
    "    rmse_val = rmse(y_test, y_pred_test)\n",
    "\n",
    "    # ---- Save Test Predictions ----\n",
    "    out = pd.DataFrame({\n",
    "        \"Player\": n_test.values,\n",
    "        \"Actual_RB_Grade\": y_test.values,\n",
    "        \"Predicted_RB_Grade\": y_pred_test,\n",
    "        \"Error\": y_pred_test - y_test.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(pred_path, index=False)\n",
    "\n",
    "    # ---- Meta-weighted feature impact (for inspection) ----\n",
    "    # Fit bases on all TRAIN to get importances aligned to feature names\n",
    "    feature_order = list(X_train.columns)\n",
    "    fi = {}\n",
    "    for nm, mdl in base_models:\n",
    "        # refit already done above; ensure .feature_importances_ exists\n",
    "        if hasattr(mdl, \"feature_importances_\"):\n",
    "            fi[nm] = pd.Series(mdl.feature_importances_, index=feature_order)\n",
    "        else:\n",
    "            # Fallback: uniform tiny vector (rare with chosen models)\n",
    "            fi[nm] = pd.Series(np.full(len(feature_order), 1.0 / len(feature_order)), index=feature_order)\n",
    "\n",
    "    # Normalize each base FI then aggregate with NNLS weights\n",
    "    fi_norm = {k: v / (v.sum() + 1e-12) for k, v in fi.items()}\n",
    "    model_weights = pd.Series(w_nnls, index=[nm for nm,_ in base_models])\n",
    "    model_weights = model_weights / (model_weights.sum() + 1e-12)\n",
    "\n",
    "    combined = sum(model_weights.get(nm, 0.0) * fi_norm[nm] for nm,_ in base_models)\n",
    "    impact_df = pd.DataFrame({\"Impact_MetaWeighted\": combined})\n",
    "    for nm,_ in base_models:\n",
    "        impact_df[f\"{nm.upper()}_Importance\"] = fi_norm[nm]\n",
    "    impact_df = impact_df.sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "\n",
    "    impact_path = OUT_DIR / f\"stack_nnls_iso_feature_impact_seed{seed}.csv\"\n",
    "    impact_df.to_csv(impact_path, index=False)\n",
    "\n",
    "    print(f\"\\n=== Seed {seed} — TEST ===\")\n",
    "    print(f\"R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse_val:.4f} | MaxPred: {out['Predicted_RB_Grade'].max():.3f}\")\n",
    "    print(\"Top 15 on TEST (by Actual):\")\n",
    "    print(out.head(15).round(3).to_string(index=False))\n",
    "    print(f\"Saved predictions → {pred_path}\")\n",
    "    print(f\"Saved feature impact → {impact_path}\")\n",
    "    return {\"seed\": seed, \"R2\": r2, \"MAE\": mae, \"RMSE\": rmse_val, \"csv\": str(pred_path), \"impact_csv\": str(impact_path)}\n",
    "\n",
    "# -----------------------------\n",
    "# Run across seeds\n",
    "# -----------------------------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "results = [run_one(s) for s in SEEDS]\n",
    "summary = pd.DataFrame(results).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Summary across seeds (80/20) ===\")\n",
    "print(summary.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "best = summary.iloc[0]\n",
    "print(f\"\\nBest seed: {int(best['seed'])}  |  R²={best['R2']:.4f}  MAE={best['MAE']:.4f}  RMSE={best['RMSE']:.4f}\")\n",
    "print(f\"Test predictions CSV: {best['csv']}\")\n",
    "print(f\"Feature impact CSV:   {best['impact_csv']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98f917-fd4a-42ee-a39e-628a4e4d74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 80/20 Stack (NNLS) + Linear (Ridge) calibration  — no plateaus\n",
    "# Bases: GB, RF, ExtraTrees, HistGB\n",
    "# Draft Age treated like any other feature (with down-weight)\n",
    "# Break% up-weight\n",
    "# Predictions clipped to [0, 15]\n",
    "# ===========================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# -----------------------------\n",
    "# Config (same as before)\n",
    "# -----------------------------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "TEST_SIZE   = 0.20\n",
    "SEEDS       = [42, 1337, 7]\n",
    "N_JOBS      = -1\n",
    "N_FOLDS     = 5\n",
    "N_ITER      = 20\n",
    "\n",
    "# Targeted influence nudges\n",
    "BREAK_SCALE       = 7.0    # increase Break% influence\n",
    "DRAFT_AGE_SCALE   = 0.12   # reduce Draft Age influence\n",
    "WINSOR = {\n",
    "    \"Draft Age\": (0.10, 0.90),\n",
    "    \"Break%\":    (0.01, 0.99),\n",
    "}\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def winsorize_col(x: pd.Series, lo: float, hi: float) -> pd.Series:\n",
    "    lo_q = x.quantile(lo)\n",
    "    hi_q = x.quantile(hi)\n",
    "    return x.clip(lo_q, hi_q)\n",
    "\n",
    "def basic_interactions(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "\n",
    "    # Winsorize first\n",
    "    for col, (lo, hi) in WINSOR.items():\n",
    "        if col in X.columns:\n",
    "            X[col] = winsorize_col(X[col], lo, hi)\n",
    "\n",
    "    # Influence nudges\n",
    "    if \"Break%\" in X.columns:\n",
    "        X[\"Break%\"] = X[\"Break%\"] * BREAK_SCALE\n",
    "    if \"Draft Age\" in X.columns:\n",
    "        X[\"Draft Age\"] = X[\"Draft Age\"] * DRAFT_AGE_SCALE\n",
    "\n",
    "    # Invert “lower is better”\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = -X[c]\n",
    "\n",
    "    # Interactions\n",
    "    if \"BMI\" in X and \"40 Time\" in X:         X[\"BMIx40\"]    = X[\"BMI\"] * X[\"40 Time\"]\n",
    "    if \"ELU\" in X and \"YCO/A\" in X:           X[\"ELUxYCOA\"]  = X[\"ELU\"] * X[\"YCO/A\"]\n",
    "    if \"DOM++\" in X and \"Draft Capital\" in X: X[\"DOMxDraft\"] = X[\"DOM++\"] * X[\"Draft Capital\"]\n",
    "    if \"YPC\" in X and \"ELU\" in X:             X[\"YPCxELU\"]   = X[\"YPC\"] * X[\"ELU\"]\n",
    "\n",
    "    return X\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def space_size(param_grid: dict) -> int:\n",
    "    n = 1\n",
    "    for v in param_grid.values():\n",
    "        n *= len(v)\n",
    "    return n\n",
    "\n",
    "# -----------------------------\n",
    "# Load & feature build\n",
    "# -----------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target not found. Available: {list(df.columns)}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found from ALIASES.\")\n",
    "\n",
    "X_all = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all, y_all, names_all = X_all.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "X_all = basic_interactions(X_all)\n",
    "\n",
    "# -----------------------------\n",
    "# Train/test for a seed\n",
    "# -----------------------------\n",
    "def run_one(seed: int):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    pred_path   = OUT_DIR / f\"stack_nnls_lin_test_predictions_seed{seed}.csv\"\n",
    "    impact_path = OUT_DIR / f\"stack_nnls_lin_feature_impact_seed{seed}.csv\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test, n_train, n_test = train_test_split(\n",
    "        X_all, y_all, names_all, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    Xtr = imp.fit_transform(X_train)\n",
    "    Xte = imp.transform(X_test)\n",
    "\n",
    "    # --- tune bases ---\n",
    "    gb = GradientBoostingRegressor(random_state=seed)\n",
    "    gb_param = {\n",
    "        \"n_estimators\": [600, 900, 1200],\n",
    "        \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "        \"max_depth\": [3, 4],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "    }\n",
    "    gb_best = RandomizedSearchCV(\n",
    "        gb, gb_param, n_iter=min(N_ITER, space_size(gb_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=seed, n_jobs=N_JOBS)\n",
    "    rf_param = {\n",
    "        \"n_estimators\": [700, 1000, 1300],\n",
    "        \"max_depth\": [None, 12, 16],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "    }\n",
    "    rf_best = RandomizedSearchCV(\n",
    "        rf, rf_param, n_iter=min(N_ITER, space_size(rf_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    et = ExtraTreesRegressor(random_state=seed, n_jobs=N_JOBS)\n",
    "    et_param = {\n",
    "        \"n_estimators\": [700, 1000, 1300],\n",
    "        \"max_depth\": [None, 12, 16],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "    }\n",
    "    et_best = RandomizedSearchCV(\n",
    "        et, et_param, n_iter=min(N_ITER, space_size(et_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    hgb = HistGradientBoostingRegressor(random_state=seed)\n",
    "    hgb_param = {\n",
    "        \"learning_rate\": [0.03, 0.05, 0.08],\n",
    "        \"max_depth\": [3, 6, 9],\n",
    "        \"l2_regularization\": [0.0, 0.1, 0.5],\n",
    "        \"max_bins\": [128, 255],\n",
    "    }\n",
    "    hgb_best = RandomizedSearchCV(\n",
    "        hgb, hgb_param, n_iter=min(N_ITER, space_size(hgb_param)),\n",
    "        scoring=\"r2\", cv=3, n_jobs=N_JOBS, random_state=seed\n",
    "    ).fit(Xtr, y_train).best_estimator_\n",
    "\n",
    "    base_models = [(\"gb\", gb_best), (\"rf\", rf_best), (\"et\", et_best), (\"hgb\", hgb_best)]\n",
    "\n",
    "    # --- OOF for NNLS meta ---\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(Xtr), len(base_models)))\n",
    "    for j,(nm, mdl) in enumerate(base_models):\n",
    "        fold_pred = np.zeros(len(Xtr))\n",
    "        for tr_idx, val_idx in kf.split(Xtr):\n",
    "            mdl.fit(Xtr[tr_idx], y_train.iloc[tr_idx])\n",
    "            fold_pred[val_idx] = mdl.predict(Xtr[val_idx])\n",
    "        oof[:, j] = fold_pred\n",
    "\n",
    "    # --- NNLS stack weights ---\n",
    "    w_nnls, _ = nnls(oof, y_train.values)\n",
    "    stack_train_raw = oof @ w_nnls\n",
    "\n",
    "    # --- Linear calibration (Ridge) on TRAIN ---\n",
    "    calib = Ridge(alpha=1.0, random_state=seed)\n",
    "    calib.fit(stack_train_raw.reshape(-1,1), y_train.values)\n",
    "\n",
    "    # --- Predict TEST ---\n",
    "    base_preds_test = []\n",
    "    for nm, mdl in base_models:\n",
    "        mdl.fit(Xtr, y_train)\n",
    "        base_preds_test.append(mdl.predict(Xte))\n",
    "    base_preds_test = np.vstack(base_preds_test).T\n",
    "\n",
    "    y_pred_raw = base_preds_test @ w_nnls\n",
    "    y_pred_test = calib.predict(y_pred_raw.reshape(-1,1))\n",
    "\n",
    "    # clip to [0, 15]\n",
    "    y_pred_test = np.clip(y_pred_test, 0.0, 15.0)\n",
    "\n",
    "    # metrics\n",
    "    r2   = r2_score(y_test, y_pred_test)\n",
    "    mae  = mean_absolute_error(y_test, y_pred_test)\n",
    "    rmse_val = rmse(y_test, y_pred_test)\n",
    "\n",
    "    # save predictions\n",
    "    out = pd.DataFrame({\n",
    "        \"Player\": n_test.values,\n",
    "        \"Actual_RB_Grade\": y_test.values,\n",
    "        \"Predicted_RB_Grade\": y_pred_test,\n",
    "        \"Error\": y_pred_test - y_test.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(pred_path, index=False)\n",
    "\n",
    "    # meta-weighted feature impact\n",
    "    feature_order = list(X_train.columns)\n",
    "    fi = {}\n",
    "    for nm, mdl in base_models:\n",
    "        if hasattr(mdl, \"feature_importances_\"):\n",
    "            fi[nm] = pd.Series(mdl.feature_importances_, index=feature_order)\n",
    "        else:\n",
    "            fi[nm] = pd.Series(np.full(len(feature_order), 1.0 / len(feature_order)), index=feature_order)\n",
    "\n",
    "    fi_norm = {k: v / (v.sum() + 1e-12) for k, v in fi.items()}\n",
    "    model_weights = pd.Series(w_nnls, index=[nm for nm,_ in base_models])\n",
    "    model_weights = model_weights / (model_weights.sum() + 1e-12)\n",
    "\n",
    "    combined = sum(model_weights.get(nm, 0.0) * fi_norm[nm] for nm,_ in base_models)\n",
    "    impact_df = pd.DataFrame({\"Impact_MetaWeighted\": combined})\n",
    "    for nm,_ in base_models:\n",
    "        impact_df[f\"{nm.upper()}_Importance\"] = fi_norm[nm]\n",
    "    impact_df = impact_df.sort_values(\"Impact_MetaWeighted\", ascending=False)\n",
    "    impact_df.to_csv(impact_path, index=False)\n",
    "\n",
    "    print(f\"\\n=== Seed {seed} — TEST ===\")\n",
    "    print(f\"R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse_val:.4f} | MaxPred: {out['Predicted_RB_Grade'].max():.3f}\")\n",
    "    print(\"Top 15 on TEST (by Actual):\")\n",
    "    print(out.head(15).round(3).to_string(index=False))\n",
    "    print(f\"Saved predictions → {pred_path}\")\n",
    "    print(f\"Saved feature impact → {impact_path}\")\n",
    "\n",
    "    return {\"seed\": seed, \"R2\": r2, \"MAE\": mae, \"RMSE\": rmse_val,\n",
    "            \"csv\": str(pred_path), \"impact_csv\": str(impact_path)}\n",
    "\n",
    "# -----------------------------\n",
    "# Run across seeds\n",
    "# -----------------------------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "results = [run_one(s) for s in SEEDS]\n",
    "summary = pd.DataFrame(results).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Summary across seeds (80/20) ===\")\n",
    "print(summary.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "best = summary.iloc[0]\n",
    "print(f\"\\nBest seed: {int(best['seed'])}  |  R²={best['R2']:.4f}  MAE={best['MAE']:.4f}  RMSE={best['RMSE']:.4f}\")\n",
    "print(f\"Test predictions CSV: {best['csv']}\")\n",
    "print(f\"Feature impact CSV:   {best['impact_csv']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220da64-9164-4f6c-951b-936dcc4d3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RB Grade — Non-negative linear \"line of best fit\" (NNLS)\n",
    "#  - 80/20 train/test by seeds\n",
    "#  - Interaction-only polynomial features (degree=2)\n",
    "#  - Impute (median) + Standardize\n",
    "#  - Non-negative least squares (weights >= 0)\n",
    "#  - Intercept = y_train.mean()\n",
    "#  - Predictions clipped to [0, 15]\n",
    "#  - Saves predictions and coefficient tables per seed\n",
    "# ============================================================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CSV_PATH     = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR      = Path(\"./data/Bakery/_derived\")\n",
    "TEST_SIZE    = 0.20\n",
    "SEEDS        = [3, 7, 11, 19, 23, 29, 31, 37, 41, 42, 1337]\n",
    "CLIP_RANGE   = (0.0, 15.0)\n",
    "\n",
    "# polynomial features\n",
    "POLY_DEGREE           = 2\n",
    "POLY_INTERACTION_ONLY = True  # only pairwise interactions, no squares\n",
    "\n",
    "# columns (Draft Age included; still no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# -------------- helpers --------------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# -------------- load --------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target column not found. Available: {list(df.columns)}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found from ALIASES.\")\n",
    "\n",
    "X_all = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all, y_all, names_all = X_all.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "base_feature_names = list(X_all.columns)\n",
    "\n",
    "# -------------- runner --------------\n",
    "def run_one(seed: int):\n",
    "    pred_path = OUT_DIR / f\"rb_nnls_test_predictions_seed{seed}.csv\"\n",
    "    coef_path = OUT_DIR / f\"rb_nnls_coeffs_seed{seed}.csv\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test, n_train, n_test = train_test_split(\n",
    "        X_all, y_all, names_all, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    # impute\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    Xtr_base = imputer.fit_transform(X_train)\n",
    "    Xte_base = imputer.transform(X_test)\n",
    "\n",
    "    # polynomial interactions\n",
    "    poly = PolynomialFeatures(\n",
    "        degree=POLY_DEGREE,\n",
    "        interaction_only=POLY_INTERACTION_ONLY,\n",
    "        include_bias=False\n",
    "    )\n",
    "    Xtr_poly = poly.fit_transform(Xtr_base)\n",
    "    Xte_poly = poly.transform(Xte_base)\n",
    "    feature_names = poly.get_feature_names_out(base_feature_names)\n",
    "\n",
    "    # standardize\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(Xtr_poly)\n",
    "    Xte = scaler.transform(Xte_poly)\n",
    "\n",
    "    # ---------- Non-negative least squares ----------\n",
    "    # center y so the intercept = y_mean (keeps weights non-negative)\n",
    "    y_mean = float(y_train.mean())\n",
    "    y_center = y_train - y_mean\n",
    "\n",
    "    # NNLS solve: min ||X w - y_center||  s.t. w >= 0\n",
    "    w, _ = nnls(Xtr, y_center.to_numpy())\n",
    "\n",
    "    # predictions: add back mean, clip to [0, 15]\n",
    "    y_pred = y_mean + Xte @ w\n",
    "    y_pred = np.clip(y_pred, CLIP_RANGE[0], CLIP_RANGE[1])\n",
    "\n",
    "    # metrics\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    rmse_val = rmse(y_test, y_pred)\n",
    "\n",
    "    # outputs\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"Player\": n_test.values,\n",
    "        \"Actual_RB_Grade\": y_test.values,\n",
    "        \"Predicted_RB_Grade\": y_pred,\n",
    "        \"Error\": y_pred - y_test.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    pred_df.to_csv(pred_path, index=False)\n",
    "\n",
    "    coef = pd.Series(w, index=feature_names, name=\"weight\").sort_values(ascending=False)\n",
    "    # all weights must be >= 0 by construction\n",
    "    assert (coef >= -1e-12).all(), \"Found a negative weight — should not happen with NNLS.\"\n",
    "    coef.to_csv(coef_path, header=True)\n",
    "\n",
    "    print(f\"\\n=== Seed {seed} — TEST (NNLS, non-negative weights) ===\")\n",
    "    print(f\"R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse_val:.4f} | MaxPred: {pred_df['Predicted_RB_Grade'].max():.3f}\")\n",
    "    print(\"Top 15 (by Actual):\")\n",
    "    print(pred_df.head(15).round(3).to_string(index=False))\n",
    "    print(f\"Saved predictions → {pred_path}\")\n",
    "    print(f\"Saved coefficients → {coef_path}\")\n",
    "\n",
    "    return {\"seed\": seed, \"R2\": r2, \"MAE\": mae, \"RMSE\": rmse_val, \"pred_csv\": str(pred_path), \"coef_csv\": str(coef_path), \"coefs\": coef}\n",
    "\n",
    "# -------------- run all seeds --------------\n",
    "results = [run_one(s) for s in SEEDS]\n",
    "summary = pd.DataFrame([{k:v for k,v in r.items() if k!='coefs'} for r in results]).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Summary across seeds (NNLS, non-negative weights) ===\")\n",
    "print(summary.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "# Aggregate coefficient stability across seeds\n",
    "coef_df = pd.DataFrame(results[0][\"coefs\"])\n",
    "coef_df.columns = [f\"seed_{results[0]['seed']}\"]\n",
    "for r in results[1:]:\n",
    "    coef_df = coef_df.join(r[\"coefs\"].rename(f\"seed_{r['seed']}\"), how=\"outer\")\n",
    "coef_mean = coef_df.mean(axis=1).fillna(0.0)\n",
    "coef_std  = coef_df.std(axis=1).fillna(0.0)\n",
    "\n",
    "coef_agg = pd.DataFrame({\"weight_mean\": coef_mean, \"weight_std\": coef_std})\n",
    "coef_agg = coef_agg.reindex(coef_agg[\"weight_mean\"].sort_values(ascending=False).index)\n",
    "\n",
    "agg_path = OUT_DIR / \"rb_nnls_coeffs_aggregate.csv\"\n",
    "coef_agg.to_csv(agg_path)\n",
    "\n",
    "print(f\"\\nSaved aggregate coefficient table → {agg_path}\")\n",
    "print(\"Top 20 features by mean weight (non-negative):\")\n",
    "print(coef_agg.head(20).round(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a3409-667d-4fa0-a812-f929add2644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RB Grade — Lean model with minimal interactions (Fixed)\n",
    "#   • Greedy forward selection now seeds with best single feature\n",
    "#   • Won't evaluate CV with 0 columns\n",
    "#   • Interactions only considered when both parents are selected\n",
    "#   • 80/20 train–test; GB tuned; predictions clipped to [0, 15]\n",
    "# ============================================================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "TEST_SIZE   = 0.20\n",
    "SEEDS       = [42, 1337, 7]\n",
    "CV_FOLDS    = 5\n",
    "CLIP_RANGE  = (0.0, 15.0)\n",
    "IMPROVE_MIN = 0.002     # minimum CV R² gain to accept a new feature\n",
    "\n",
    "BASE_FEATURES = [\n",
    "    \"DOM++\",\"40 Time\",\"BMI\",\"YPC\",\"ELU\",\"YCO/A\",\"Break%\",\"Draft Capital\",\"Bama\",\"Draft Age\",\n",
    "    # \"Shuttle\",\"Three Cone\",\"Rec Yards\",  # optional if present\n",
    "]\n",
    "\n",
    "INTERACTIONS = {\n",
    "    \"DOMxDraft\": (\"DOM++\", \"Draft Capital\"),\n",
    "    \"YPCxELU\":   (\"YPC\",   \"ELU\"),\n",
    "    # \"ELUxYCOA\":  (\"ELU\",   \"YCO/A\"),  # optional if you want to allow it\n",
    "}\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def add_interactions(X, use_interactions):\n",
    "    X = X.copy()\n",
    "    for name, (a, b) in INTERACTIONS.items():\n",
    "        if name in use_interactions and a in X.columns and b in X.columns:\n",
    "            X[name] = X[a] * X[b]\n",
    "    return X\n",
    "\n",
    "def build_X(df, mapped_cols, use_interactions=None, invert_lower_better=True):\n",
    "    X = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped_cols.items()})\n",
    "    if invert_lower_better:\n",
    "        for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "            if c in X.columns: X[c] = -X[c]\n",
    "    if use_interactions:\n",
    "        X = add_interactions(X, use_interactions)\n",
    "    return X\n",
    "\n",
    "# ---------- load ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target column not found. Available: {list(df.columns)}\")\n",
    "\n",
    "mapped = {}\n",
    "for feat in BASE_FEATURES:\n",
    "    col = find_col(df, ALIASES.get(feat, [feat]))\n",
    "    if col is not None:\n",
    "        mapped[feat] = col\n",
    "\n",
    "available_features = list(mapped.keys())\n",
    "if not available_features:\n",
    "    raise ValueError(\"None of the base features were found in the CSV.\")\n",
    "\n",
    "X0 = build_X(df, mapped, use_interactions=None)\n",
    "y  = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y.notna()\n",
    "X0, y, names = X0.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True), names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "allowed_interactions = []\n",
    "for name, (a,b) in INTERACTIONS.items():\n",
    "    if a in X0.columns and b in X0.columns:\n",
    "        allowed_interactions.append(name)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------- greedy forward selection (fixed) ---------\n",
    "def greedy_select(X_train_df, y_train, base_feats, interactions, random_state=42):\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    def cv_score(cols, inters):\n",
    "        # guard: no columns -> invalid\n",
    "        if len(cols) == 0:\n",
    "            return -1e12\n",
    "        X_use = X_train_df[cols].copy()\n",
    "        if inters:\n",
    "            for iname, (a,b) in INTERACTIONS.items():\n",
    "                if iname in inters and a in X_use.columns and b in X_use.columns:\n",
    "                    X_use[iname] = X_use[a]*X_use[b]\n",
    "        Xt = imp.fit_transform(X_use)\n",
    "        gb = GradientBoostingRegressor(\n",
    "            random_state=random_state,\n",
    "            n_estimators=600, learning_rate=0.05, max_depth=3, subsample=0.9,\n",
    "            min_samples_leaf=2\n",
    "        )\n",
    "        kf = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=random_state)\n",
    "        cv = cross_val_score(gb, Xt, y_train, scoring=\"r2\", cv=kf)\n",
    "        return float(np.mean(cv))\n",
    "\n",
    "    # --- Step 1: pick the best single feature to seed ---\n",
    "    best_first_feat = None\n",
    "    best_first_score = -1e12\n",
    "    for f in base_feats:\n",
    "        sc = cv_score([f], [])\n",
    "        if sc > best_first_score:\n",
    "            best_first_score = sc\n",
    "            best_first_feat = f\n",
    "    if best_first_feat is None:\n",
    "        # fallback: at least take one feature arbitrarily to avoid empty set\n",
    "        best_first_feat = base_feats[0]\n",
    "        best_first_score = cv_score([best_first_feat], [])\n",
    "\n",
    "    selected_feats = [best_first_feat]\n",
    "    selected_inters = []\n",
    "    best = best_first_score\n",
    "\n",
    "    # remaining candidates\n",
    "    feat_cands = [f for f in base_feats if f != best_first_feat]\n",
    "    inter_cands = interactions.copy()\n",
    "\n",
    "    while True:\n",
    "        improved = False\n",
    "        best_try = None\n",
    "        best_score = best\n",
    "\n",
    "        # try adding one more base feature\n",
    "        for f in feat_cands:\n",
    "            score = cv_score(selected_feats + [f], selected_inters)\n",
    "            if score > best_score + 1e-9:\n",
    "                best_try = (\"feat\", f)\n",
    "                best_score = score\n",
    "\n",
    "        # try adding an interaction ONLY if its parents are already selected\n",
    "        if len(selected_inters) < 2:\n",
    "            for inter in inter_cands:\n",
    "                a, b = INTERACTIONS[inter]\n",
    "                if a in selected_feats and b in selected_feats:\n",
    "                    score = cv_score(selected_feats, selected_inters + [inter])\n",
    "                    if score > best_score + 1e-9:\n",
    "                        best_try = (\"inter\", inter)\n",
    "                        best_score = score\n",
    "\n",
    "        if best_try and (best_score - best) >= IMPROVE_MIN:\n",
    "            if best_try[0] == \"feat\":\n",
    "                f = best_try[1]\n",
    "                selected_feats.append(f)\n",
    "                feat_cands.remove(f)\n",
    "            else:\n",
    "                i = best_try[1]\n",
    "                selected_inters.append(i)\n",
    "                inter_cands.remove(i)\n",
    "            best = best_score\n",
    "            improved = True\n",
    "\n",
    "        if not improved:\n",
    "            break\n",
    "\n",
    "    return selected_feats, selected_inters, best\n",
    "\n",
    "# --------- one full train/test run ----------\n",
    "def run_one(seed: int):\n",
    "    pred_path = OUT_DIR / f\"lean_gb_predictions_seed{seed}.csv\"\n",
    "\n",
    "    X_train_df, X_test_df, y_train, y_test, n_train, n_test = train_test_split(\n",
    "        X0, y, names, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    feats_sel, inters_sel, cv_r2 = greedy_select(\n",
    "        X_train_df, y_train,\n",
    "        base_feats=[f for f in available_features if f in X_train_df.columns],\n",
    "        interactions=allowed_interactions,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # build final train/test matrices with the selected set\n",
    "    X_train = X_train_df[feats_sel].copy()\n",
    "    X_test  = X_test_df[feats_sel].copy()\n",
    "    for iname in inters_sel:\n",
    "        a,b = INTERACTIONS[iname]\n",
    "        X_train[iname] = X_train[a]*X_train[b]\n",
    "        X_test[iname]  = X_test[a]*X_test[b]\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    Xtr = imp.fit_transform(X_train)\n",
    "    Xte = imp.transform(X_test)\n",
    "\n",
    "    # final tuning\n",
    "    gb = GradientBoostingRegressor(random_state=seed)\n",
    "    param = {\n",
    "        \"n_estimators\": [700, 900, 1200],\n",
    "        \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "        \"max_depth\": [3, 4],\n",
    "        \"subsample\": [0.85, 1.0],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "    }\n",
    "    search = RandomizedSearchCV(\n",
    "        gb, param, n_iter=min(12, np.prod([len(v) for v in param.values()])),\n",
    "        scoring=\"r2\", cv=CV_FOLDS, random_state=seed, n_jobs=-1\n",
    "    ).fit(Xtr, y_train)\n",
    "    best_gb = search.best_estimator_\n",
    "\n",
    "    best_gb.fit(Xtr, y_train)\n",
    "    y_pred = best_gb.predict(Xte)\n",
    "    y_pred = np.clip(y_pred, CLIP_RANGE[0], CLIP_RANGE[1])\n",
    "\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    rse  = rmse(y_test, y_pred)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"Player\": n_test.values,\n",
    "        \"Actual_RB_Grade\": y_test.values,\n",
    "        \"Predicted_RB_Grade\": y_pred,\n",
    "        \"Error\": y_pred - y_test.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    out.to_csv(pred_path, index=False)\n",
    "\n",
    "    print(f\"\\n=== Seed {seed} — TEST (lean GB) ===\")\n",
    "    print(f\"Selected features ({len(feats_sel)}): {feats_sel}\")\n",
    "    print(f\"Selected interactions ({len(inters_sel)}): {inters_sel}\")\n",
    "    print(f\"Greedy CV R² (train only): {cv_r2:.4f}\")\n",
    "    print(f\"TEST  R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rse:.4f} | MaxPred: {out['Predicted_RB_Grade'].max():.3f}\")\n",
    "    print(\"\\nTop 15 by Actual (TEST):\")\n",
    "    print(out.head(15).round(3).to_string(index=False))\n",
    "    print(f\"\\nSaved predictions → {pred_path}\")\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed, \"R2\": r2, \"MAE\": mae, \"RMSE\": rse,\n",
    "        \"cvR2\": cv_r2, \"pred_csv\": str(pred_path),\n",
    "        \"features\": feats_sel, \"interactions\": inters_sel\n",
    "    }\n",
    "\n",
    "# --------- run across seeds ----------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "results = [run_one(s) for s in SEEDS]\n",
    "summary = pd.DataFrame(results).sort_values(\"R2\", ascending=False)\n",
    "print(\"\\n=== Summary across seeds (lean features) ===\")\n",
    "print(summary[[\"seed\",\"R2\",\"MAE\",\"RMSE\",\"cvR2\",\"features\",\"interactions\"]]\n",
    "      .to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db6cea-d6d2-41ad-9b15-a5d64e40046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# RB Grade — Wide search over feature subsets & hyperparameters\n",
    "# - Randomly sample feature combos (+ limited interactions)\n",
    "# - Tune multiple models (GB, RF, ET, HGB) with RandomizedSearchCV\n",
    "# - 80/20 train-test; CV on TRAIN only; metrics on TEST\n",
    "# - Predictions clipped to [0, 15]\n",
    "# - Logs leaderboard + meta for reproducibility\n",
    "# ===============================================================\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor, RandomForestRegressor,\n",
    "    ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    ")\n",
    "\n",
    "# ---------------- Search controls ----------------\n",
    "CSV_PATH            = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR             = Path(\"./data/Bakery/_derived\")\n",
    "TEST_SIZE           = 0.20\n",
    "\n",
    "SEEDS               = [42, 1337, 7]   # run search for multiple seeds\n",
    "N_SUBSETS           = 60              # random feature subsets per seed\n",
    "MAX_BASE_FEATS      = 7               # cap base features per subset\n",
    "MAX_INTERACTIONS    = 2               # cap interactions per subset\n",
    "N_ITER_PER_MODEL    = 20              # RandomizedSearchCV iterations per model\n",
    "CV_FOLDS            = 5\n",
    "\n",
    "CLIP_MIN, CLIP_MAX  = 0.0, 15.0       # clip predictions\n",
    "\n",
    "# ---------------- Feature space ----------------\n",
    "BASE_FEATURES = [\n",
    "    \"DOM++\",\"40 Time\",\"BMI\",\"YPC\",\"ELU\",\"YCO/A\",\"Break%\",\"Draft Capital\",\"Bama\",\"Draft Age\"\n",
    "]\n",
    "\n",
    "INTERACTIONS = {\n",
    "    \"DOMxDraft\": (\"DOM++\", \"Draft Capital\"),\n",
    "    \"YPCxELU\":   (\"YPC\",   \"ELU\"),\n",
    "    \"ELUxYCOA\":  (\"ELU\",   \"YCO/A\"),\n",
    "}\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------------- Utilities ----------------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def invert_cols(X):\n",
    "    # invert \"lower is better\"\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    return X\n",
    "\n",
    "def add_interactions(X, inter_names):\n",
    "    X = X.copy()\n",
    "    for name in inter_names:\n",
    "        a,b = INTERACTIONS[name]\n",
    "        if a in X.columns and b in X.columns:\n",
    "            X[name] = X[a]*X[b]\n",
    "    return X\n",
    "\n",
    "# ---------------- Load & prep ----------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Could not find target column among {TARGET_CANDS}\")\n",
    "\n",
    "mapped = {}\n",
    "for feat in BASE_FEATURES:\n",
    "    col = find_col(df, ALIASES.get(feat, [feat]))\n",
    "    if col is not None:\n",
    "        mapped[feat] = col\n",
    "\n",
    "ALLOWED_INTERS = {k:v for k,v in INTERACTIONS.items() if v[0] in mapped and v[1] in mapped}\n",
    "\n",
    "X_all_raw = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all     = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all_raw, y_all, names_all = X_all_raw.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- Model spaces ----------------\n",
    "def model_spaces(random_state):\n",
    "    return [\n",
    "        (\"GB\", GradientBoostingRegressor(random_state=random_state), {\n",
    "            \"n_estimators\": [400, 600, 800, 1000, 1200],\n",
    "            \"learning_rate\": [0.03, 0.05, 0.07, 0.1],\n",
    "            \"max_depth\": [2, 3, 4],\n",
    "            \"subsample\": [0.8, 0.9, 1.0],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "        }),\n",
    "        (\"RF\", RandomForestRegressor(random_state=random_state, n_jobs=-1), {\n",
    "            \"n_estimators\": [600, 900, 1200, 1500],\n",
    "            \"max_depth\": [None, 12, 16, 20],\n",
    "            \"min_samples_split\": [2, 4, 6],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "            \"max_features\": [\"sqrt\", 0.7, 0.9, 1.0],\n",
    "        }),\n",
    "        (\"ET\", ExtraTreesRegressor(random_state=random_state, n_jobs=-1), {\n",
    "            \"n_estimators\": [600, 900, 1200, 1500],\n",
    "            \"max_depth\": [None, 12, 16, 20],\n",
    "            \"min_samples_split\": [2, 4, 6],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "            \"max_features\": [\"sqrt\", 0.7, 0.9, 1.0],\n",
    "        }),\n",
    "        (\"HGB\", HistGradientBoostingRegressor(random_state=random_state), {\n",
    "            \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "            \"max_depth\": [3, 6, 9],\n",
    "            \"l2_regularization\": [0.0, 0.1, 0.3, 0.5],\n",
    "            \"max_bins\": [128, 255],\n",
    "        })\n",
    "    ]\n",
    "\n",
    "# ---------------- Random subset generator ----------------\n",
    "def sample_subset(rng, base_pool, max_bases, allowed_inters, max_inters):\n",
    "    n_bases = rng.integers(low=min(3, len(base_pool)), high=min(max_bases, len(base_pool)) + 1)\n",
    "    bases = rng.choice(base_pool, size=int(n_bases), replace=False).tolist()\n",
    "\n",
    "    inter_names = []\n",
    "    if allowed_inters and max_inters > 0:\n",
    "        eligible = [name for name,(a,b) in allowed_inters.items() if a in bases and b in bases]\n",
    "        if eligible:\n",
    "            k = rng.integers(low=0, high=min(max_inters, len(eligible)) + 1)\n",
    "            if k > 0:\n",
    "                inter_names = rng.choice(eligible, size=int(k), replace=False).tolist()\n",
    "    return bases, inter_names\n",
    "\n",
    "# ---------------- One full search (per seed) ----------------\n",
    "def run_seed(seed: int):\n",
    "    pred_path  = OUT_DIR / f\"rb_wide_best_preds_seed{seed}.csv\"\n",
    "    meta_path  = OUT_DIR / f\"rb_wide_best_meta_seed{seed}.json\"\n",
    "    board_path = OUT_DIR / f\"rb_wide_leaderboard_seed{seed}.csv\"\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    X_tr_raw, X_te_raw, y_tr, y_te, n_tr, n_te = train_test_split(\n",
    "        X_all_raw, y_all, names_all, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    leaderboard = []\n",
    "    baseline_done = False\n",
    "\n",
    "    for subset_idx in range(N_SUBSETS):\n",
    "        if not baseline_done:\n",
    "            # best single feature baseline\n",
    "            best_feat, best_cv = None, -1e9\n",
    "            for f in X_tr_raw.columns:\n",
    "                imp = SimpleImputer(strategy=\"median\")\n",
    "                X_single = imp.fit_transform(X_tr_raw[[f]])\n",
    "                kf = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=seed)\n",
    "                cv = cross_val_score(GradientBoostingRegressor(random_state=seed), X_single, y_tr, scoring=\"r2\", cv=kf).mean()\n",
    "                if cv > best_cv: best_cv, best_feat = cv, f\n",
    "            bases, inters = [best_feat], []\n",
    "            baseline_done = True\n",
    "        else:\n",
    "            bases, inters = sample_subset(rng, list(X_tr_raw.columns), MAX_BASE_FEATS, ALLOWED_INTERS, MAX_INTERACTIONS)\n",
    "\n",
    "        # build matrices\n",
    "        Xtr_df, Xte_df = X_tr_raw[bases].copy(), X_te_raw[bases].copy()\n",
    "        for iname in inters:\n",
    "            a,b = INTERACTIONS[iname]\n",
    "            Xtr_df[iname], Xte_df[iname] = Xtr_df[a]*Xtr_df[b], Xte_df[a]*Xte_df[b]\n",
    "        Xtr_df, Xte_df = invert_cols(Xtr_df), invert_cols(Xte_df)\n",
    "\n",
    "        imp = SimpleImputer(strategy=\"median\")\n",
    "        Xtr, Xte = imp.fit_transform(Xtr_df), imp.transform(Xte_df)\n",
    "\n",
    "        # tune models\n",
    "        best_cv, best_tag, best_est = -1e9, None, None\n",
    "        for tag, est, grid in model_spaces(seed):\n",
    "            n_iter = min(N_ITER_PER_MODEL, int(np.prod([len(v) for v in grid.values()])))\n",
    "            search = RandomizedSearchCV(est, grid, n_iter=n_iter, scoring=\"r2\", cv=CV_FOLDS, random_state=seed, n_jobs=-1)\n",
    "            search.fit(Xtr, y_tr)\n",
    "            if search.best_score_ > best_cv:\n",
    "                best_cv, best_tag, best_est = search.best_score_, tag, search.best_estimator_\n",
    "\n",
    "        best_est.fit(Xtr, y_tr)\n",
    "        y_pred = np.clip(best_est.predict(Xte), CLIP_MIN, CLIP_MAX)\n",
    "\n",
    "        leaderboard.append({\n",
    "            \"seed\": seed, \"subset_idx\": subset_idx, \"model\": best_tag,\n",
    "            \"cvR2_mean\": best_cv, \"TEST_R2\": r2_score(y_te, y_pred),\n",
    "            \"TEST_MAE\": mean_absolute_error(y_te, y_pred),\n",
    "            \"TEST_RMSE\": rmse(y_te, y_pred),\n",
    "            \"n_features\": len(bases) + len(inters),\n",
    "            \"bases\": \"|\".join(bases), \"interactions\": \"|\".join(inters),\n",
    "            \"max_pred\": float(np.max(y_pred)),\n",
    "        })\n",
    "\n",
    "    # leaderboard\n",
    "    board = pd.DataFrame(leaderboard).sort_values([\"TEST_R2\",\"cvR2_mean\"], ascending=False).head(15)\n",
    "    board.to_csv(board_path, index=False)\n",
    "\n",
    "    best_row = board.iloc[0]\n",
    "    print(f\"\\n=== Seed {seed} — top of board ===\")\n",
    "    print(board[[\"subset_idx\",\"model\",\"n_features\",\"cvR2_mean\",\"TEST_R2\",\"TEST_MAE\",\"TEST_RMSE\",\"bases\",\"interactions\"]]\n",
    "          .to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "    # save meta\n",
    "    meta = {\n",
    "        \"seed\": seed, \"leaderboard_csv\": str(board_path), \"best_predictions_csv\": str(pred_path),\n",
    "        \"best_bases\": best_row[\"bases\"].split(\"|\"), \"best_interactions\": best_row[\"interactions\"].split(\"|\") if best_row[\"interactions\"] else [],\n",
    "        \"best_model_tag\": best_row[\"model\"], \"best_cvR2\": float(best_row[\"cvR2_mean\"]),\n",
    "        \"best_test_R2\": float(best_row[\"TEST_R2\"]), \"best_test_MAE\": float(best_row[\"TEST_MAE\"]),\n",
    "        \"best_test_RMSE\": float(best_row[\"TEST_RMSE\"]), \"max_pred_test\": float(best_row[\"max_pred\"])\n",
    "    }\n",
    "    with open(meta_path, \"w\") as f: json.dump(meta, f, indent=2)\n",
    "\n",
    "    return meta\n",
    "\n",
    "# ---------------- Run across seeds & summarize ----------------\n",
    "all_meta = [run_seed(s) for s in SEEDS]\n",
    "summary = pd.DataFrame(all_meta).sort_values(\"best_test_R2\", ascending=False)\n",
    "print(\"\\n=== Overall summary across seeds ===\")\n",
    "print(summary[[\"seed\",\"best_cvR2\",\"best_test_R2\",\"best_test_MAE\",\"best_test_RMSE\",\"best_model_tag\",\"best_bases\",\"best_interactions\"]]\n",
    "      .to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec10172-ee91-446d-8e52-f41117a76645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Subset 28 reproducer (seed=42, HGB model, 5 features)\n",
    "# Features: ELU, Draft Capital, Draft Age, DOM++, BMI\n",
    "# Output: CSV with Player, Actual_RB_Grade, Predicted_RB_Grade, Error\n",
    "# ===============================================================\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CSV_PATH    = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR     = Path(\"./data/Bakery/_derived\")\n",
    "SEED        = 42           # Subset 28 was from seed 42\n",
    "TEST_SIZE   = 0.20\n",
    "CV_FOLDS    = 5\n",
    "CLIP_MIN, CLIP_MAX = 0.0, 15.0\n",
    "\n",
    "# Five features from the winning subset\n",
    "WIN_FEATURES = [\"ELU\", \"Draft Capital\", \"Draft Age\", \"DOM++\", \"BMI\"]\n",
    "\n",
    "# Aliases (so it works even if your headers vary a bit)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# HGB search space (kept tight; we’ll let CV pick the best)\n",
    "HGB_PARAM = {\n",
    "    \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "    \"max_depth\":     [3, 6, 9],\n",
    "    \"l2_regularization\": [0.0, 0.1, 0.3, 0.5],\n",
    "    \"max_bins\": [128, 255],\n",
    "}\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# ---------------- Load ----------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target column not found among {TARGET_CANDS}. Available: {list(df.columns)}\")\n",
    "\n",
    "# map the 5 winning columns\n",
    "mapped = {}\n",
    "for feat in WIN_FEATURES:\n",
    "    col = find_col(df, ALIASES.get(feat, [feat]))\n",
    "    if col is None:\n",
    "        raise ValueError(f\"Could not find required feature '{feat}' in the CSV headers.\")\n",
    "    mapped[feat] = col\n",
    "\n",
    "X_raw = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "# keep only rows with target\n",
    "mask = y_all.notna()\n",
    "X_raw, y_all, names = X_raw.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# Invert 'lower is better' features (earlier round/younger age should increase grade)\n",
    "for c in [\"Draft Capital\", \"Draft Age\"]:\n",
    "    if c in X_raw.columns:\n",
    "        X_raw[c] = -X_raw[c]\n",
    "\n",
    "# ---------------- Split, Impute, Tune, Predict ----------------\n",
    "X_train_df, X_test_df, y_train, y_test, n_train, n_test = train_test_split(\n",
    "    X_raw, y_all, names, test_size=TEST_SIZE, random_state=SEED\n",
    ")\n",
    "\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "Xtr = imp.fit_transform(X_train_df)\n",
    "Xte = imp.transform(X_test_df)\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(random_state=SEED)\n",
    "n_iter = min(20, int(np.prod([len(v) for v in HGB_PARAM.values()])))\n",
    "search = RandomizedSearchCV(\n",
    "    hgb, HGB_PARAM, n_iter=n_iter, scoring=\"r2\",\n",
    "    cv=KFold(n_splits=CV_FOLDS, shuffle=True, random_state=SEED),\n",
    "    random_state=SEED, n_jobs=-1\n",
    ")\n",
    "search.fit(Xtr, y_train)\n",
    "best_hgb = search.best_estimator_\n",
    "\n",
    "y_pred = np.clip(best_hgb.predict(Xte), CLIP_MIN, CLIP_MAX)\n",
    "\n",
    "# ---------------- Report & Save ----------------\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rse  = rmse(y_test, y_pred)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pred_csv = OUT_DIR / \"subset28_seed42_hgb_predictions.csv\"\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"Player\": n_test.values,\n",
    "    \"Actual_RB_Grade\": y_test.values,\n",
    "    \"Predicted_RB_Grade\": y_pred,\n",
    "    \"Error\": y_pred - y_test.values\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "out.to_csv(pred_csv, index=False)\n",
    "\n",
    "print(\"\\n=== Subset 28 replica (HGB, 5 features) — TEST ===\")\n",
    "print(f\"Features used: {WIN_FEATURES}\")\n",
    "print(f\"Best HGB params: {search.best_params_}\")\n",
    "print(f\"R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rse:.4f} | MaxPred: {out['Predicted_RB_Grade'].max():.3f}\")\n",
    "\n",
    "print(\"\\nTop 20 by Actual (TEST):\")\n",
    "print(out.head(20).round(3).to_string(index=False))\n",
    "\n",
    "print(f\"\\nSaved predictions → {pred_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e1aa4-dca8-4909-a75b-de1f2f0ca309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Subset 28 full-dataset predictions + PDF\n",
    "# Features: ELU, Draft Capital, Draft Age, DOM++, BMI\n",
    "# Outputs:\n",
    "#   data/Bakery/_derived/subset28_seed42_hgb_full_predictions.csv\n",
    "#   data/Bakery/_derived/subset28_seed42_hgb_full_report.pdf\n",
    "# ===============================================================\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Use matplotlib for a simple PDF (no seaborn, no custom colors)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "PROJECT_ROOT = Path(\".\")  # adjust if you run from elsewhere\n",
    "CANDIDATE_CSV_PATHS = [\n",
    "    PROJECT_ROOT / \"data/Bakery/RB/Bakery_RB_Overall.csv\",\n",
    "    PROJECT_ROOT / \"../data/Bakery/RB/Bakery_RB_Overall.csv\",\n",
    "]\n",
    "OUT_DIR     = PROJECT_ROOT / \"data/Bakery/_derived\"\n",
    "SEED        = 42\n",
    "CV_FOLDS    = 5\n",
    "CLIP_MIN, CLIP_MAX = 0.0, 15.0\n",
    "\n",
    "WIN_FEATURES = [\"ELU\", \"Draft Capital\", \"Draft Age\", \"DOM++\", \"BMI\"]\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# HGB search space\n",
    "HGB_PARAM = {\n",
    "    \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "    \"max_depth\":     [3, 6, 9],\n",
    "    \"l2_regularization\": [0.0, 0.1, 0.3, 0.5],\n",
    "    \"max_bins\": [128, 255],\n",
    "}\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def find_existing_path(paths):\n",
    "    for p in paths:\n",
    "        if Path(p).exists():\n",
    "            return Path(p)\n",
    "    return None\n",
    "\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# ---------------- Load ----------------\n",
    "CSV_PATH = find_existing_path(CANDIDATE_CSV_PATHS)\n",
    "if CSV_PATH is None:\n",
    "    raise FileNotFoundError(f\"Could not find Bakery_RB_Overall.csv in any of: {CANDIDATE_CSV_PATHS}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target column not found among {TARGET_CANDS}. Available: {list(df.columns)}\")\n",
    "\n",
    "# map the 5 subset-28 columns\n",
    "mapped = {}\n",
    "for feat in WIN_FEATURES:\n",
    "    col = find_col(df, ALIASES.get(feat, [feat]))\n",
    "    if col is None:\n",
    "        raise ValueError(f\"Could not find required feature '{feat}' in the CSV headers.\")\n",
    "    mapped[feat] = col\n",
    "\n",
    "X_raw = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "# keep only rows with target\n",
    "mask = y_all.notna()\n",
    "X_raw, y_all, names = X_raw.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# Invert 'lower is better' (earlier round / younger age -> higher grade)\n",
    "for c in [\"Draft Capital\", \"Draft Age\"]:\n",
    "    if c in X_raw.columns:\n",
    "        X_raw[c] = -X_raw[c]\n",
    "\n",
    "# ---------------- Impute, tune (CV), fit on ALL rows ----------------\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_all = imp.fit_transform(X_raw)\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(random_state=SEED)\n",
    "n_iter = min(20, int(np.prod([len(v) for v in HGB_PARAM.values()])))\n",
    "cv = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    hgb, HGB_PARAM, n_iter=n_iter, scoring=\"r2\",\n",
    "    cv=cv, random_state=SEED, n_jobs=-1\n",
    ")\n",
    "search.fit(X_all, y_all)\n",
    "best_hgb = search.best_estimator_\n",
    "\n",
    "y_pred = np.clip(best_hgb.predict(X_all), CLIP_MIN, CLIP_MAX)\n",
    "\n",
    "# ---------------- Report & Save ----------------\n",
    "R2   = r2_score(y_all, y_pred)\n",
    "MAE  = mean_absolute_error(y_all, y_pred)\n",
    "RMSE = rmse(y_all, y_pred)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pred_csv = OUT_DIR / \"subset28_seed42_hgb_full_predictions.csv\"\n",
    "\n",
    "out_df = pd.DataFrame({\n",
    "    \"Player\": names.values,\n",
    "    \"Actual_RB_Grade\": y_all.values,\n",
    "    \"Predicted_RB_Grade\": y_pred,\n",
    "    \"Error\": y_pred - y_all.values\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "out_df.to_csv(pred_csv, index=False)\n",
    "\n",
    "# ----- Build PDF (matplotlib table) -----\n",
    "report_pdf = OUT_DIR / \"subset28_seed42_hgb_full_report.pdf\"\n",
    "\n",
    "top_k = 25\n",
    "tbl = out_df.head(top_k).copy().round(3)\n",
    "tbl.insert(0, \"#\", range(1, len(tbl) + 1))\n",
    "\n",
    "title_lines = [\n",
    "    \"Bakery RB — Subset 28 Full-Dataset Report\",\n",
    "    f\"Model: HistGradientBoosting (seed={SEED})\",\n",
    "    f\"Features: {', '.join(WIN_FEATURES)}\",\n",
    "    f\"Best Params: {search.best_params_}\",\n",
    "    f\"Rows: {len(out_df)} | R²={R2:.4f} | MAE={MAE:.3f} | RMSE={RMSE:.3f}\",\n",
    "    f\"CSV: {pred_csv}\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10.5, 13.5))\n",
    "plt.axis('off')\n",
    "ypos = 0.98\n",
    "for line in title_lines:\n",
    "    plt.text(0.02, ypos, line, fontsize=11, ha='left', va='top')\n",
    "    ypos -= 0.035\n",
    "\n",
    "col_labels = list(tbl.columns)\n",
    "cell_text  = tbl.values.tolist()\n",
    "the_table = plt.table(cellText=cell_text, colLabels=col_labels, cellLoc='left', loc='center')\n",
    "the_table.auto_set_font_size(False)\n",
    "the_table.set_fontsize(9)\n",
    "the_table.scale(1, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(report_pdf, format=\"pdf\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n=== Subset 28 (HGB, 5 features) — FULL DATASET ===\")\n",
    "print(f\"Best HGB params: {search.best_params_}\")\n",
    "print(f\"R²: {R2:.4f} | MAE: {MAE:.4f} | RMSE: {RMSE:.4f}\")\n",
    "print(f\"\\nSaved predictions CSV → {pred_csv}\")\n",
    "print(f\"Saved PDF report      → {report_pdf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0c6cb-2a2b-43bb-be80-af1a5e56c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RB Grade — Non-negative linear \"line of best fit\" (NNLS)\n",
    "#  - 80/20 train/test by seeds\n",
    "#  - Interaction-only polynomial features (degree=2)\n",
    "#  - Impute (median) + Standardize\n",
    "#  - Non-negative least squares (weights >= 0)\n",
    "#  - Intercept = y_train.mean()\n",
    "#  - Predictions clipped to [0, 15]\n",
    "#  - Saves predictions and coefficient tables per seed\n",
    "# ============================================================\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CSV_PATH     = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR      = Path(\"./data/Bakery/_derived\")\n",
    "TEST_SIZE    = 0.20\n",
    "SEEDS        = [3, 7, 11, 19, 23, 29, 31, 37, 41, 42, 1337]\n",
    "CLIP_RANGE   = (0.0, 15.0)\n",
    "\n",
    "# polynomial features\n",
    "POLY_DEGREE           = 2\n",
    "POLY_INTERACTION_ONLY = True  # only pairwise interactions, no squares\n",
    "\n",
    "# columns (Draft Age included; still no Breakout Age)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# -------------- helpers --------------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# -------------- load --------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Target column not found. Available: {list(df.columns)}\")\n",
    "\n",
    "mapped = {feat: find_col(df, alts) for feat, alts in ALIASES.items()}\n",
    "mapped = {k:v for k,v in mapped.items() if v is not None}\n",
    "if not mapped:\n",
    "    raise ValueError(\"No usable feature columns found from ALIASES.\")\n",
    "\n",
    "X_all = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all, y_all, names_all = X_all.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "base_feature_names = list(X_all.columns)\n",
    "\n",
    "# -------------- runner --------------\n",
    "def run_one(seed: int):\n",
    "    pred_path = OUT_DIR / f\"rb_nnls_test_predictions_seed{seed}.csv\"\n",
    "    coef_path = OUT_DIR / f\"rb_nnls_coeffs_seed{seed}.csv\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test, n_train, n_test = train_test_split(\n",
    "        X_all, y_all, names_all, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    # impute\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    Xtr_base = imputer.fit_transform(X_train)\n",
    "    Xte_base = imputer.transform(X_test)\n",
    "\n",
    "    # polynomial interactions\n",
    "    poly = PolynomialFeatures(\n",
    "        degree=POLY_DEGREE,\n",
    "        interaction_only=POLY_INTERACTION_ONLY,\n",
    "        include_bias=False\n",
    "    )\n",
    "    Xtr_poly = poly.fit_transform(Xtr_base)\n",
    "    Xte_poly = poly.transform(Xte_base)\n",
    "    feature_names = poly.get_feature_names_out(base_feature_names)\n",
    "\n",
    "    # standardize\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(Xtr_poly)\n",
    "    Xte = scaler.transform(Xte_poly)\n",
    "\n",
    "    # ---------- Non-negative least squares ----------\n",
    "    # center y so the intercept = y_mean (keeps weights non-negative)\n",
    "    y_mean = float(y_train.mean())\n",
    "    y_center = y_train - y_mean\n",
    "\n",
    "    # NNLS solve: min ||X w - y_center||  s.t. w >= 0\n",
    "    w, _ = nnls(Xtr, y_center.to_numpy())\n",
    "\n",
    "    # predictions: add back mean, clip to [0, 15]\n",
    "    y_pred = y_mean + Xte @ w\n",
    "    y_pred = np.clip(y_pred, CLIP_RANGE[0], CLIP_RANGE[1])\n",
    "\n",
    "    # metrics\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    rmse_val = rmse(y_test, y_pred)\n",
    "\n",
    "    # outputs\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"Player\": n_test.values,\n",
    "        \"Actual_RB_Grade\": y_test.values,\n",
    "        \"Predicted_RB_Grade\": y_pred,\n",
    "        \"Error\": y_pred - y_test.values\n",
    "    }).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "    pred_df.to_csv(pred_path, index=False)\n",
    "\n",
    "    coef = pd.Series(w, index=feature_names, name=\"weight\").sort_values(ascending=False)\n",
    "    # all weights must be >= 0 by construction\n",
    "    assert (coef >= -1e-12).all(), \"Found a negative weight — should not happen with NNLS.\"\n",
    "    coef.to_csv(coef_path, header=True)\n",
    "\n",
    "    print(f\"\\n=== Seed {seed} — TEST (NNLS, non-negative weights) ===\")\n",
    "    print(f\"R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse_val:.4f} | MaxPred: {pred_df['Predicted_RB_Grade'].max():.3f}\")\n",
    "    print(\"Top 15 (by Actual):\")\n",
    "    print(pred_df.head(15).round(3).to_string(index=False))\n",
    "    print(f\"Saved predictions → {pred_path}\")\n",
    "    print(f\"Saved coefficients → {coef_path}\")\n",
    "\n",
    "    return {\"seed\": seed, \"R2\": r2, \"MAE\": mae, \"RMSE\": rmse_val, \"pred_csv\": str(pred_path), \"coef_csv\": str(coef_path), \"coefs\": coef}\n",
    "\n",
    "# -------------- run all seeds --------------\n",
    "results = [run_one(s) for s in SEEDS]\n",
    "summary = pd.DataFrame([{k:v for k,v in r.items() if k!='coefs'} for r in results]).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Summary across seeds (NNLS, non-negative weights) ===\")\n",
    "print(summary.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "# Aggregate coefficient stability across seeds\n",
    "coef_df = pd.DataFrame(results[0][\"coefs\"])\n",
    "coef_df.columns = [f\"seed_{results[0]['seed']}\"]\n",
    "for r in results[1:]:\n",
    "    coef_df = coef_df.join(r[\"coefs\"].rename(f\"seed_{r['seed']}\"), how=\"outer\")\n",
    "coef_mean = coef_df.mean(axis=1).fillna(0.0)\n",
    "coef_std  = coef_df.std(axis=1).fillna(0.0)\n",
    "\n",
    "coef_agg = pd.DataFrame({\"weight_mean\": coef_mean, \"weight_std\": coef_std})\n",
    "coef_agg = coef_agg.reindex(coef_agg[\"weight_mean\"].sort_values(ascending=False).index)\n",
    "\n",
    "agg_path = OUT_DIR / \"rb_nnls_coeffs_aggregate.csv\"\n",
    "coef_agg.to_csv(agg_path)\n",
    "\n",
    "print(f\"\\nSaved aggregate coefficient table → {agg_path}\")\n",
    "print(\"Top 20 features by mean weight (non-negative):\")\n",
    "print(coef_agg.head(20).round(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b0df5-9cb9-46e5-9c7b-d288d0c89b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ===============================================================\n",
    "# RB Grade — Wide search over feature subsets & hyperparameters\n",
    "# - Randomly sample feature combos (+ limited interactions)\n",
    "# - Tune multiple models (GB, RF, ET, HGB) with RandomizedSearchCV\n",
    "# - 80/20 train-test; CV on TRAIN only; metrics on TEST\n",
    "# - Predictions clipped to [0, 15]\n",
    "# - Logs leaderboard + meta for reproducibility\n",
    "# ===============================================================\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor, RandomForestRegressor,\n",
    "    ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    ")\n",
    "\n",
    "# ---------------- Search controls ----------------\n",
    "CSV_PATH            = Path(\"./data/Bakery/RB/Bakery_RB_Overall.csv\")\n",
    "OUT_DIR             = Path(\"./data/Bakery/_derived\")\n",
    "TEST_SIZE           = 0.20\n",
    "\n",
    "SEEDS               = [456, 123, 789] # run search for multiple seeds\n",
    "N_SUBSETS           = 60              # random feature subsets per seed\n",
    "MAX_BASE_FEATS      = 8               # cap base features per subset\n",
    "MAX_INTERACTIONS    = 3               # cap interactions per subset\n",
    "N_ITER_PER_MODEL    = 25              # RandomizedSearchCV iterations per model\n",
    "CV_FOLDS            = 5\n",
    "\n",
    "CLIP_MIN, CLIP_MAX  = 0.0, 15.0       # clip predictions\n",
    "\n",
    "# ---------------- Feature space ----------------\n",
    "BASE_FEATURES = [\n",
    "    \"DOM++\",\"40 Time\",\"BMI\",\"YPC\",\"ELU\",\"YCO/A\",\"Break%\",\"Draft Capital\",\"Bama\",\"Draft Age\"\n",
    "]\n",
    "\n",
    "INTERACTIONS = {\n",
    "    \"DOMxDraft\": (\"DOM++\", \"Draft Capital\"),\n",
    "    \"YPCxELU\":   (\"YPC\",   \"ELU\"),\n",
    "    \"ELUxYCOA\":  (\"ELU\",   \"YCO/A\"),\n",
    "}\n",
    "\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# ---------------- Utilities ----------------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def invert_cols(X):\n",
    "    # invert \"lower is better\"\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in X.columns: X[c] = -X[c]\n",
    "    return X\n",
    "\n",
    "def add_interactions(X, inter_names):\n",
    "    X = X.copy()\n",
    "    for name in inter_names:\n",
    "        a,b = INTERACTIONS[name]\n",
    "        if a in X.columns and b in X.columns:\n",
    "            X[name] = X[a]*X[b]\n",
    "    return X\n",
    "\n",
    "# ---------------- Load & prep ----------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = find_col(df, TARGET_CANDS)\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Could not find target column among {TARGET_CANDS}\")\n",
    "\n",
    "mapped = {}\n",
    "for feat in BASE_FEATURES:\n",
    "    col = find_col(df, ALIASES.get(feat, [feat]))\n",
    "    if col is not None:\n",
    "        mapped[feat] = col\n",
    "\n",
    "ALLOWED_INTERS = {k:v for k,v in INTERACTIONS.items() if v[0] in mapped and v[1] in mapped}\n",
    "\n",
    "X_all_raw = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "y_all     = to_num(df[y_col])\n",
    "names_all = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "mask = y_all.notna()\n",
    "X_all_raw, y_all, names_all = X_all_raw.loc[mask].reset_index(drop=True), y_all.loc[mask].reset_index(drop=True), names_all.loc[mask].reset_index(drop=True)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- Model spaces ----------------\n",
    "def model_spaces(random_state):\n",
    "    return [\n",
    "        (\"GB\", GradientBoostingRegressor(random_state=random_state), {\n",
    "            \"n_estimators\": [400, 600, 800, 1000, 1200],\n",
    "            \"learning_rate\": [0.03, 0.05, 0.07, 0.1],\n",
    "            \"max_depth\": [2, 3, 4],\n",
    "            \"subsample\": [0.8, 0.9, 1.0],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "        }),\n",
    "        (\"RF\", RandomForestRegressor(random_state=random_state, n_jobs=-1), {\n",
    "            \"n_estimators\": [600, 900, 1200, 1500],\n",
    "            \"max_depth\": [None, 12, 16, 20],\n",
    "            \"min_samples_split\": [2, 4, 6],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "            \"max_features\": [\"sqrt\", 0.7, 0.9, 1.0],\n",
    "        }),\n",
    "        (\"ET\", ExtraTreesRegressor(random_state=random_state, n_jobs=-1), {\n",
    "            \"n_estimators\": [600, 900, 1200, 1500],\n",
    "            \"max_depth\": [None, 12, 16, 20],\n",
    "            \"min_samples_split\": [2, 4, 6],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "            \"max_features\": [\"sqrt\", 0.7, 0.9, 1.0],\n",
    "        }),\n",
    "        (\"HGB\", HistGradientBoostingRegressor(random_state=random_state), {\n",
    "            \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "            \"max_depth\": [3, 6, 9],\n",
    "            \"l2_regularization\": [0.0, 0.1, 0.3, 0.5],\n",
    "            \"max_bins\": [128, 255],\n",
    "        })\n",
    "    ]\n",
    "\n",
    "# ---------------- Random subset generator ----------------\n",
    "def sample_subset(rng, base_pool, max_bases, allowed_inters, max_inters):\n",
    "    n_bases = rng.integers(low=min(3, len(base_pool)), high=min(max_bases, len(base_pool)) + 1)\n",
    "    bases = rng.choice(base_pool, size=int(n_bases), replace=False).tolist()\n",
    "\n",
    "    inter_names = []\n",
    "    if allowed_inters and max_inters > 0:\n",
    "        eligible = [name for name,(a,b) in allowed_inters.items() if a in bases and b in bases]\n",
    "        if eligible:\n",
    "            k = rng.integers(low=0, high=min(max_inters, len(eligible)) + 1)\n",
    "            if k > 0:\n",
    "                inter_names = rng.choice(eligible, size=int(k), replace=False).tolist()\n",
    "    return bases, inter_names\n",
    "\n",
    "# ---------------- One full search (per seed) ----------------\n",
    "def run_seed(seed: int):\n",
    "    pred_path  = OUT_DIR / f\"rb_wide_best_preds_seed{seed}.csv\"\n",
    "    meta_path  = OUT_DIR / f\"rb_wide_best_meta_seed{seed}.json\"\n",
    "    board_path = OUT_DIR / f\"rb_wide_leaderboard_seed{seed}.csv\"\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    X_tr_raw, X_te_raw, y_tr, y_te, n_tr, n_te = train_test_split(\n",
    "        X_all_raw, y_all, names_all, test_size=TEST_SIZE, random_state=seed\n",
    "    )\n",
    "\n",
    "    leaderboard = []\n",
    "    baseline_done = False\n",
    "\n",
    "    for subset_idx in range(N_SUBSETS):\n",
    "        if not baseline_done:\n",
    "            # best single feature baseline\n",
    "            best_feat, best_cv = None, -1e9\n",
    "            for f in X_tr_raw.columns:\n",
    "                imp = SimpleImputer(strategy=\"median\")\n",
    "                X_single = imp.fit_transform(X_tr_raw[[f]])\n",
    "                kf = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=seed)\n",
    "                cv = cross_val_score(GradientBoostingRegressor(random_state=seed), X_single, y_tr, scoring=\"r2\", cv=kf).mean()\n",
    "                if cv > best_cv: best_cv, best_feat = cv, f\n",
    "            bases, inters = [best_feat], []\n",
    "            baseline_done = True\n",
    "        else:\n",
    "            bases, inters = sample_subset(rng, list(X_tr_raw.columns), MAX_BASE_FEATS, ALLOWED_INTERS, MAX_INTERACTIONS)\n",
    "\n",
    "        # build matrices\n",
    "        Xtr_df, Xte_df = X_tr_raw[bases].copy(), X_te_raw[bases].copy()\n",
    "        for iname in inters:\n",
    "            a,b = INTERACTIONS[iname]\n",
    "            Xtr_df[iname], Xte_df[iname] = Xtr_df[a]*Xtr_df[b], Xte_df[a]*Xte_df[b]\n",
    "        Xtr_df, Xte_df = invert_cols(Xtr_df), invert_cols(Xte_df)\n",
    "\n",
    "        imp = SimpleImputer(strategy=\"median\")\n",
    "        Xtr, Xte = imp.fit_transform(Xtr_df), imp.transform(Xte_df)\n",
    "\n",
    "        # tune models\n",
    "        best_cv, best_tag, best_est = -1e9, None, None\n",
    "        for tag, est, grid in model_spaces(seed):\n",
    "            n_iter = min(N_ITER_PER_MODEL, int(np.prod([len(v) for v in grid.values()])))\n",
    "            search = RandomizedSearchCV(est, grid, n_iter=n_iter, scoring=\"r2\", cv=CV_FOLDS, random_state=seed, n_jobs=-1)\n",
    "            search.fit(Xtr, y_tr)\n",
    "            if search.best_score_ > best_cv:\n",
    "                best_cv, best_tag, best_est = search.best_score_, tag, search.best_estimator_\n",
    "\n",
    "        best_est.fit(Xtr, y_tr)\n",
    "        y_pred = np.clip(best_est.predict(Xte), CLIP_MIN, CLIP_MAX)\n",
    "\n",
    "        leaderboard.append({\n",
    "            \"seed\": seed, \"subset_idx\": subset_idx, \"model\": best_tag,\n",
    "            \"cvR2_mean\": best_cv, \"TEST_R2\": r2_score(y_te, y_pred),\n",
    "            \"TEST_MAE\": mean_absolute_error(y_te, y_pred),\n",
    "            \"TEST_RMSE\": rmse(y_te, y_pred),\n",
    "            \"n_features\": len(bases) + len(inters),\n",
    "            \"bases\": \"|\".join(bases), \"interactions\": \"|\".join(inters),\n",
    "            \"max_pred\": float(np.max(y_pred)),\n",
    "        })\n",
    "\n",
    "    # leaderboard\n",
    "    board = pd.DataFrame(leaderboard).sort_values([\"TEST_R2\",\"cvR2_mean\"], ascending=False).head(15)\n",
    "    board.to_csv(board_path, index=False)\n",
    "\n",
    "    best_row = board.iloc[0]\n",
    "    print(f\"\\n=== Seed {seed} — top of board ===\")\n",
    "    print(board[[\"subset_idx\",\"model\",\"n_features\",\"cvR2_mean\",\"TEST_R2\",\"TEST_MAE\",\"TEST_RMSE\",\"bases\",\"interactions\"]]\n",
    "          .to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "    # save meta\n",
    "    meta = {\n",
    "        \"seed\": seed, \"leaderboard_csv\": str(board_path), \"best_predictions_csv\": str(pred_path),\n",
    "        \"best_bases\": best_row[\"bases\"].split(\"|\"), \"best_interactions\": best_row[\"interactions\"].split(\"|\") if best_row[\"interactions\"] else [],\n",
    "        \"best_model_tag\": best_row[\"model\"], \"best_cvR2\": float(best_row[\"cvR2_mean\"]),\n",
    "        \"best_test_R2\": float(best_row[\"TEST_R2\"]), \"best_test_MAE\": float(best_row[\"TEST_MAE\"]),\n",
    "        \"best_test_RMSE\": float(best_row[\"TEST_RMSE\"]), \"max_pred_test\": float(best_row[\"max_pred\"])\n",
    "    }\n",
    "    with open(meta_path, \"w\") as f: json.dump(meta, f, indent=2)\n",
    "\n",
    "    return meta\n",
    "\n",
    "# ---------------- Run across seeds & summarize ----------------\n",
    "all_meta = [run_seed(s) for s in SEEDS]\n",
    "summary = pd.DataFrame(all_meta).sort_values(\"best_test_R2\", ascending=False)\n",
    "print(\"\\n=== Overall summary across seeds ===\")\n",
    "print(summary[[\"seed\",\"best_cvR2\",\"best_test_R2\",\"best_test_MAE\",\"best_test_RMSE\",\"best_model_tag\",\"best_bases\",\"best_interactions\"]]\n",
    "      .to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cfe67-898f-467c-90e1-c841f2dd26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Use TOP result from wide-search leaderboard to score ALL players\n",
    "# - Reads leaderboard -> gets best model, bases, interactions\n",
    "# - Rebuilds those features from Bakery_RB_Overall.csv\n",
    "# - Tunes the chosen model (light CV), fits on ALL rows\n",
    "# - Saves Player | Actual_RB_Grade | Predicted_RB_Grade | Error\n",
    "# ===============================================================\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "PROJECT_ROOT   = Path(\".\")  # adjust if you run elsewhere\n",
    "# Point this to your leaderboard (works with either local project path or an absolute path like /mnt/data/...):\n",
    "LEADERBOARD_CSV = PROJECT_ROOT / \"data/Bakery/_derived/rb_wide_leaderboard_seed789.csv\"\n",
    "# Example if you stored it in /mnt/data:\n",
    "# LEADERBOARD_CSV = Path(\"/mnt/data/rb_wide_leaderboard_seed789.csv\")\n",
    "\n",
    "DATA_CSV      = PROJECT_ROOT / \"data/Bakery/RB/Bakery_RB_Overall.csv\"\n",
    "OUT_DIR       = PROJECT_ROOT / \"data/Bakery/_derived\"\n",
    "OUT_CSV       = OUT_DIR / \"rb_top_model_full_predictions.csv\"\n",
    "\n",
    "SEED          = 789   # this can be any; only affects tuning randomness\n",
    "CV_FOLDS      = 5\n",
    "CLIP_MIN, CLIP_MAX = 0.0, 15.0\n",
    "N_JOBS        = -1\n",
    "\n",
    "# Aliases (so we can map headers reliably)\n",
    "ALIASES = {\n",
    "    \"DOM++\":         [\"DOM++\",\"DOMpp\",\"DOM_plus_plus\",\"DOMpp_Weighted\",\"DOM\"],\n",
    "    \"40 Time\":       [\"40 Time\",\"Forty\",\"40\"],\n",
    "    \"BMI\":           [\"BMI\"],\n",
    "    \"YPC\":           [\"YPC\",\"Yards per Carry\",\"Yards/Carry\",\"Rushing YPC\"],\n",
    "    \"ELU\":           [\"ELU\",\"Elusiveness\",\"Elusiveness Rating\"],\n",
    "    \"YCO/A\":         [\"YCO/A\",\"YAC/A\",\"Yards After Contact / Att\",\"Yards After Contact per Attempt\"],\n",
    "    \"Break%\":        [\"Break%\",\"Break %\",\"Breakaway %\",\"Breakaway Percentage\",\"Breakaway%\"],\n",
    "    \"Draft Capital\": [\"Draft Capital\",\"Draft Cap\",\"Draft Round\",\"Round\",\"Rnd\"],\n",
    "    \"Bama\":          [\"Bama\",\"Bama Rating\",\"BamaAdj\",\"BAMA\"],\n",
    "    \"Shuttle\":       [\"Shuttle\",\"Short Shuttle\",\"20 Shuttle\",\"20 Yard Shuttle\"],\n",
    "    \"Three Cone\":    [\"3 Cone\",\"Three Cone\",\"3-Cone\"],\n",
    "    \"Rec Yards\":     [\"Receiving Yards\",\"Rec Yds\",\"RecYds\"],\n",
    "    \"Draft Age\":     [\"Draft Age\",\"Age at Draft\",\"DraftAge\",\"Age (Draft)\",\"AgeDraft\",\"Age_at_Draft\"],\n",
    "}\n",
    "TARGET_CANDS = [\"RB Grade\",\"RBGrade\",\"RB_Grade\"]\n",
    "NAME_CANDS   = [\"Player\",\"Player Name\",\"Name\"]\n",
    "\n",
    "# Interaction definitions used in the wide search\n",
    "INTERACTIONS = {\n",
    "    \"DOMxDraft\": (\"DOM++\", \"Draft Capital\"),\n",
    "    \"YPCxELU\":   (\"YPC\",   \"ELU\"),\n",
    "    \"ELUxYCOA\":  (\"ELU\",   \"YCO/A\"),\n",
    "}\n",
    "\n",
    "# Light grids to (re)fit the chosen model family\n",
    "def model_space(tag, seed):\n",
    "    if tag == \"GB\":\n",
    "        est = GradientBoostingRegressor(random_state=seed)\n",
    "        grid = {\n",
    "            \"n_estimators\": [600, 800, 1000, 1200],\n",
    "            \"learning_rate\": [0.03, 0.05, 0.07, 0.1],\n",
    "            \"max_depth\": [2, 3, 4],\n",
    "            \"subsample\": [0.85, 1.0],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "        }\n",
    "    elif tag == \"RF\":\n",
    "        est = RandomForestRegressor(random_state=seed, n_jobs=N_JOBS)\n",
    "        grid = {\n",
    "            \"n_estimators\": [800, 1100, 1400],\n",
    "            \"max_depth\": [None, 12, 16, 20],\n",
    "            \"min_samples_split\": [2, 4],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "            \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "        }\n",
    "    elif tag == \"ET\":\n",
    "        est = ExtraTreesRegressor(random_state=seed, n_jobs=N_JOBS)\n",
    "        grid = {\n",
    "            \"n_estimators\": [800, 1100, 1400],\n",
    "            \"max_depth\": [None, 12, 16, 20],\n",
    "            \"min_samples_split\": [2, 4],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "            \"max_features\": [\"sqrt\", 0.8, 1.0],\n",
    "        }\n",
    "    elif tag == \"HGB\":\n",
    "        est = HistGradientBoostingRegressor(random_state=seed)\n",
    "        grid = {\n",
    "            \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "            \"max_depth\": [3, 6, 9],\n",
    "            \"l2_regularization\": [0.0, 0.1, 0.3, 0.5],\n",
    "            \"max_bins\": [128, 255],\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model tag in leaderboard: {tag}\")\n",
    "    return est, grid\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def find_col(frame, candidates):\n",
    "    norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in frame.columns}\n",
    "    for cand in candidates:\n",
    "        key = re.sub(r\"\\s+\",\"\",cand).lower()\n",
    "        if key in norm: return norm[key]\n",
    "    return None\n",
    "\n",
    "def to_num(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = (s.str.replace('%','',regex=False)\n",
    "           .str.replace(r'(?i)round\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)^r\\s*','',regex=True)\n",
    "           .str.replace(r'(?i)(st|nd|rd|th)$','',regex=True)\n",
    "           .str.replace(',','',regex=False)\n",
    "           .str.replace(r'[^0-9\\.\\-]','',regex=True))\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def invert_lower_better(df):\n",
    "    # Earlier round / faster time / younger age / faster shuttles → higher grade\n",
    "    for c in [\"40 Time\",\"Draft Capital\",\"Shuttle\",\"Three Cone\",\"Draft Age\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = -df[c]\n",
    "    return df\n",
    "\n",
    "def add_interactions(X_df, inter_names):\n",
    "    X = X_df.copy()\n",
    "    for name in inter_names:\n",
    "        if not name:\n",
    "            continue\n",
    "        a, b = INTERACTIONS[name]\n",
    "        if a in X.columns and b in X.columns:\n",
    "            X[name] = X[a] * X[b]\n",
    "    return X\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# ---------------- Load leaderboard & pick the top row ----------------\n",
    "board = pd.read_csv(LEADERBOARD_CSV)\n",
    "if board.empty:\n",
    "    raise ValueError(f\"Leaderboard is empty: {LEADERBOARD_CSV}\")\n",
    "\n",
    "# Sort by TEST_R2 desc (tie-breaker by cvR2_mean if present)\n",
    "sort_cols = [c for c in [\"TEST_R2\",\"cvR2_mean\"] if c in board.columns]\n",
    "board_sorted = board.sort_values(sort_cols, ascending=[False]*len(sort_cols)).reset_index(drop=True)\n",
    "best = board_sorted.iloc[0]\n",
    "\n",
    "best_tag  = best[\"model\"]\n",
    "bases     = (str(best[\"bases\"]).split(\"|\") if pd.notna(best[\"bases\"]) and best[\"bases\"] != \"\" else [])\n",
    "inters    = (str(best[\"interactions\"]).split(\"|\") if pd.notna(best[\"interactions\"]) and best[\"interactions\"] != \"\" else [])\n",
    "\n",
    "print(\"== Using TOP leaderboard row ==\")\n",
    "print(f\"Model: {best_tag}\")\n",
    "print(f\"Bases: {bases}\")\n",
    "print(f\"Interactions: {inters}\")\n",
    "if \"TEST_R2\" in best:\n",
    "    print(f\"Leaderboard TEST R²: {best['TEST_R2']:.4f}\")\n",
    "\n",
    "# ---------------- Load data & build features ----------------\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "y_col    = None\n",
    "name_col = None\n",
    "for cands in [TARGET_CANDS]:\n",
    "    y_col = find_col(df, cands)\n",
    "    if y_col: break\n",
    "name_col = find_col(df, NAME_CANDS) or \"Player\"\n",
    "if not y_col:\n",
    "    raise ValueError(f\"Could not find target column in {TARGET_CANDS}. Available: {list(df.columns)}\")\n",
    "\n",
    "# map bases\n",
    "mapped = {}\n",
    "for feat in bases:\n",
    "    col = find_col(df, ALIASES.get(feat, [feat]))\n",
    "    if col is None:\n",
    "        raise ValueError(f\"Could not find required feature '{feat}' in the CSV headers.\")\n",
    "    mapped[feat] = col\n",
    "\n",
    "# restrict interactions to those whose parents exist\n",
    "valid_inters = []\n",
    "for name in inters:\n",
    "    if not name: \n",
    "        continue\n",
    "    if name not in INTERACTIONS:\n",
    "        print(f\"Warning: interaction '{name}' not recognized; skipping.\")\n",
    "        continue\n",
    "    a,b = INTERACTIONS[name]\n",
    "    if a in mapped and b in mapped:\n",
    "        valid_inters.append(name)\n",
    "\n",
    "X_base = pd.DataFrame({feat: to_num(df[col]) for feat, col in mapped.items()})\n",
    "X_base = invert_lower_better(X_base)\n",
    "X_all  = add_interactions(X_base, valid_inters)\n",
    "\n",
    "y_all   = to_num(df[y_col])\n",
    "names   = df[name_col].astype(str).fillna(\"\")\n",
    "\n",
    "# keep rows with target\n",
    "mask    = y_all.notna()\n",
    "X_all   = X_all.loc[mask].reset_index(drop=True)\n",
    "y_all   = y_all.loc[mask].reset_index(drop=True)\n",
    "names   = names.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# impute\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X_all)\n",
    "\n",
    "# ---------------- Train chosen model family on ALL rows ----------------\n",
    "est, grid = model_space(best_tag, SEED)\n",
    "n_iter = min(20, int(np.prod([len(v) for v in grid.values()])))\n",
    "cv = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "search = RandomizedSearchCV(est, grid, n_iter=n_iter, scoring=\"r2\",\n",
    "                            cv=cv, random_state=SEED, n_jobs=N_JOBS)\n",
    "search.fit(X_imp, y_all)\n",
    "best_est = search.best_estimator_\n",
    "\n",
    "y_pred = best_est.predict(X_imp)\n",
    "y_pred = np.clip(y_pred, CLIP_MIN, CLIP_MAX)\n",
    "\n",
    "# ---------------- Save results (ALL players) ----------------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out = pd.DataFrame({\n",
    "    \"Player\": names.values,\n",
    "    \"Actual_RB_Grade\": y_all.values,\n",
    "    \"Predicted_RB_Grade\": y_pred,\n",
    "    \"Error\": y_pred - y_all.values\n",
    "}).sort_values(\"Actual_RB_Grade\", ascending=False)\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "# quick in-sample metrics (just for sense check)\n",
    "R2   = r2_score(y_all, y_pred)\n",
    "MAE  = mean_absolute_error(y_all, y_pred)\n",
    "RMSE = rmse(y_all, y_pred)\n",
    "\n",
    "print(\"\\n=== Top model applied to FULL dataset ===\")\n",
    "print(f\"Chosen model: {best_tag}  | tuned params: {search.best_params_}\")\n",
    "print(f\"In-sample R²: {R2:.4f} | MAE: {MAE:.4f} | RMSE: {RMSE:.4f}\")\n",
    "print(f\"Saved → {OUT_CSV}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(out.head(15).round(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08d4e9-dcd4-4742-b5c4-367308db513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visuals.plot_feature_scatter_batch import main\n",
    "\n",
    "# Generate max plots, 6 per PDF page, for RB\n",
    "main(position=\"RB\", max_plots=900, cols=3, rows=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
